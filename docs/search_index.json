[
["index.html", "Logit Models Chapter 1 Basics 1.1 Logit 1.2 Probit", " Logit Models Bill 2019-12-23 Chapter 1 Basics 1.1 Logit \\[f(x)=log(\\frac{p(y=1)}{1-p(y=1)})\\] The basic idea of logistic regression: \\[p(y=1)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)}}=\\frac{e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}{1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}\\] Thus, \\(e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}\\) can be from \\(-\\infty\\) to \\(+\\infty\\), and \\(p(y=1)\\) will be always within the range of \\((0,1)\\). f&lt;-function(x){exp(x)/(1+exp(x))} data&lt;-seq(-10,10,1) plot(data,f(data),type = &quot;b&quot;) We can also write the function into another format as follows: \\[log \\frac{p(y=1)}{1-p(y=1)}= \\beta_0+\\beta_1x_1+...+\\beta_nx_n\\] 1.2 Probit \\[\\beta_0+\\beta_1x_1+...+\\beta_nx_n =\\Phi^{-1}(p)\\] Thus, \\[\\Phi(\\beta_0+\\beta_1x_1+...+\\beta_nx_n )= p(y=1)\\] "],
["intro.html", "Chapter 2 MLE", " Chapter 2 MLE The probablity of \\(y=1\\) is as follows: \\[p=p(y=1)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)}}=\\frac{e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}{1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}\\] Thus, the likelihood function is as follows: \\[L=\\prod p^{y_i}(1-p)^{1-y_i}=\\prod (\\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)}})^{y_i}(\\frac{1}{1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}})^{1-y_i}\\] \\[=\\prod (1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)})^{-y_i}(1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n})^{-(1-y_i)}\\] Thus, the log-likelihood is as follows: \\[logL=\\sum (-y_i \\cdot log(1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)})-(1-y_i)\\cdot log(1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}))\\] Typically, optimisers minimize a function, so we use negative log-likelihood as minimising that is equivalent to maximising the log-likelihood or the likelihood itself. #Source of R code: https://www.r-bloggers.com/logistic-regression/ mle.logreg = function(fmla, data) { # Define the negative log likelihood function logl &lt;- function(theta,x,y){ y &lt;- y x &lt;- as.matrix(x) beta &lt;- theta[1:ncol(x)] # Use the log-likelihood of the Bernouilli distribution, where p is # defined as the logistic transformation of a linear combination # of predictors, according to logit(p)=(x%*%beta) loglik &lt;- sum(-y*log(1 + exp(-(x%*%beta))) - (1-y)*log(1 + exp(x%*%beta))) return(-loglik) } # Prepare the data outcome = rownames(attr(terms(fmla),&quot;factors&quot;))[1] dfrTmp = model.frame(data) x = as.matrix(model.matrix(fmla, data=dfrTmp)) y = as.numeric(as.matrix(data[,match(outcome,colnames(data))])) # Define initial values for the parameters theta.start = rep(0,(dim(x)[2])) names(theta.start) = colnames(x) # Calculate the maximum likelihood mle = optim(theta.start,logl,x=x,y=y, method = &#39;BFGS&#39;, hessian=T) out = list(beta=mle$par,vcov=solve(mle$hessian),ll=2*mle$value) } mydata = read.csv(url(&#39;https://stats.idre.ucla.edu/stat/data/binary.csv&#39;)) mylogit1 = glm(admit~gre+gpa+as.factor(rank), family=binomial, data=mydata) mydata$rank = factor(mydata$rank) #Treat rank as a categorical variable fmla = as.formula(&quot;admit~gre+gpa+rank&quot;) #Create model formula mylogit2 = mle.logreg(fmla, mydata) #Estimate coefficients print(cbind(coef(mylogit1), mylogit2$beta)) ## [,1] [,2] ## (Intercept) -3.989979073 -3.772676422 ## gre 0.002264426 0.001375522 ## gpa 0.804037549 0.898201239 ## as.factor(rank)2 -0.675442928 -0.675543009 ## as.factor(rank)3 -1.340203916 -1.356554831 ## as.factor(rank)4 -1.551463677 -1.563396035 "],
["twitter-example.html", "Chapter 3 Twitter Example 3.1 Model 3.2 Simulating Data of Senators on Twitter 3.3 Simulating Data of Conservative Users on Twitter and Model Testing 3.4 Simulating Data of Liberal Users on Twitter and Model Testing", " Chapter 3 Twitter Example The following is part of my course project for Stat 536. It aims to replicate part of the findings from Barbera (2015) Birds of the Same Feather Tweet Together: Bayesian Ideal Point Estimation Using Twitter Data. Political Analysis 23 (1). Note that, the following model is much simpler than that in the original paper. 3.1 Model Suppose that a Twitter user is presented with a choice between following or not following another target \\(j \\in \\{ 1, ..., m\\}\\). Let \\(y_{j}=1\\) if the user decides to follow \\(j\\), and \\(y_{j}=0\\) otherwise. \\[y_{j}=\\begin{cases} 1 &amp; Following \\\\ 0 &amp; Not Following \\end{cases}\\] \\[p(y_{j}=1|\\theta) = \\frac{exp(- \\theta_0|\\theta_1 - x_j|^2)}{1+exp(- \\theta_0|\\theta_1 - x_j|^2)}\\] We additionally know the priors of \\(\\theta\\). \\[\\theta_i \\sim N(0,10^2) (i = 0, 1)\\] The likelihood function is as follows. \\[L(Y|\\theta)=\\prod_{j=1}^{m} (\\frac{exp(- \\theta_0|\\theta_1 - x_j|^2)}{1+exp(- \\theta_0|\\theta_1 - x_j|^2)})^{y_j}(1-\\frac{exp(- \\theta_0|\\theta_1 - x_j|^2)}{1+exp(- \\theta_0|\\theta_1 - x_j|^2)})^{(1-y_j)}\\] Thus, the posterior is as follows. \\[L(Y|\\theta) \\cdot N(\\theta_0|0,10) \\cdot N(\\theta_1|0,10)\\] \\[\\propto \\prod_{j=1}^{m} (\\frac{exp(- \\theta_0|\\theta_1 - x_j|^2)}{1+exp(- \\theta_0|\\theta_1 - x_j|^2)})^{y_j}(1-\\frac{exp(- \\theta_0|\\theta_1 - x_j|^2)}{1+exp(- \\theta_0|\\theta_1 - x_j|^2)})^{(1-y_j)}\\cdot exp(-\\frac{1}{2}(\\frac{\\theta_0}{10})^2)\\cdot exp(-\\frac{1}{2}(\\frac{\\theta_1}{10})^2)\\] 3.2 Simulating Data of Senators on Twitter Assume that we have 100 senators, 50 Democrats and 50 Republicans, who we know their ideology. Assume that Democrats have negative ideology scores to indicate that they are more liberal, whereas Republicans have positive scores to indicate that they are more conservative. The following is data simulation for senators. # Republicans are more conservative, and they have positive numbers. Republicans&lt;-c() Republicans&lt;-rnorm(50,1,0.5) No_Republicans&lt;-rep(1:50,1) Part_1&lt;-cbind(No_Republicans,Republicans) # Democrats are more liberal, and they have negative numbers. Democrats&lt;-c() Democrats&lt;-rnorm(50,-1,0.5) No_Democrats&lt;-rep(51:100,1) Part_2&lt;-cbind(No_Democrats,Democrats) Data_Elites&lt;-rbind(Part_1,Part_2) Data_Elites&lt;-as.data.frame(Data_Elites) colnames(Data_Elites) &lt;- c(&quot;Elite_No&quot;,&quot;Elite_ideology&quot;) head(Data_Elites) ## Elite_No Elite_ideology ## 1 1 0.0212499 ## 2 2 0.3228297 ## 3 3 1.1409273 ## 4 4 1.5264003 ## 5 5 1.6489864 ## 6 6 1.2043241 3.3 Simulating Data of Conservative Users on Twitter and Model Testing Assume that we observe one Twitter user, who is more conservative. To simulate Twitter following data for this user, I assign this user to follow more Republican senators. Thus, if the Metropolis Hastings algorithm works as intended, we would expect to see a positive estimated value for their ideology. Importantly, as we can see in the histogram below, the estimated value indeed is positive, providing preliminary evidence for the statistical model and the algorithm. In addition, for the acceptance rate, we can see that the constant has a lower number than ideology, since we only accept a constant when it is positive. #This user approximately follows 45 Republican Senators and 10 Democrat Senators. Data_user&lt;-as.data.frame(matrix(c(ifelse(runif(50)&lt;.1,0,1),ifelse(runif(50)&lt;.8,0,1))), 100, 1) colnames(Data_user)&lt;-c(&quot;R_User&quot;) Data_combined&lt;-cbind(Data_Elites,Data_user) X_data&lt;-Data_combined$Elite_ideology Y_data&lt;-Data_combined$R_User fit_C&lt;-Bayes_logit(Y_data,X_data) fit_C$acceptance_rate ## [1] 0.1820910 0.5347674 plot(fit_C$theta[,1],main=&quot;Constant (Conservative Users)&quot;, xlab=&quot;Iteration Process&quot;,ylab=&quot;Estimated Scores&quot;,type=&quot;l&quot;) plot(fit_C$theta[,2],main=&quot;Estimated Ideology Scores (Conservative Users)&quot;, xlab=&quot;Iteration Process&quot;,ylab=&quot;Ideology Scores&quot;,type=&quot;l&quot;) hist(fit_C$theta[,2],main=&quot;Estimated Ideology Scores (Conservative Users)&quot;, xlab=&quot;Ideology Scores&quot;,breaks = 100) 3.4 Simulating Data of Liberal Users on Twitter and Model Testing To further verify the Metropolis Hastings algorithm, I plan to test the opposite estimate. Specifically, assume that we observe another user, who is more liberal. To simulate Twitter following data for this user, I assign this user to follow more Democrat senators. In this case, we would expect to see a negative value for their estimated ideology. As we can see in the histogram shown below, as expected, the estimated value is negative, providing convergent evidence for the model and the algorithm. #This user approximately follows 10 Republican Senators and 45 Democrat Senators. Data_user&lt;-as.data.frame(matrix(c(ifelse(runif(50)&lt;.8,0,1),ifelse(runif(50)&lt;.1,0,1))), 100, 1) colnames(Data_user)&lt;-c(&quot;L_User&quot;) Data_combined&lt;-cbind(Data_Elites,Data_user) X_data&lt;-Data_combined$Elite_ideology Y_data&lt;-Data_combined$L_User fit_L&lt;-Bayes_logit(Y_data,X_data) fit_L$acceptance_rate ## [1] 0.1890945 0.5297649 plot(fit_L$theta[,1],main=&quot;Constant (Liberal Users)&quot;, xlab=&quot;Iteration Process&quot;,ylab=&quot;Estimated Scores&quot;,type=&quot;l&quot;) plot(fit_L$theta[,2],main=&quot;Estimated Ideology Scores (Liberal Users)&quot;, xlab=&quot;Iteration Process&quot;,ylab=&quot;Ideology Scores&quot;,type=&quot;l&quot;) hist(fit_L$theta[,2],main=&quot;Estimated Ideology Scores (Liberal Users)&quot;, xlab=&quot;Ideology Scores&quot;,breaks = 100) "]
]
