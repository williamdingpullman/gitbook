<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 MLE | Generalized Linear Mixed Models &amp; R</title>
  <meta name="description" content="The webpages are mainly about logit models." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 MLE | Generalized Linear Mixed Models &amp; R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The webpages are mainly about logit models." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 MLE | Generalized Linear Mixed Models &amp; R" />
  
  <meta name="twitter:description" content="The webpages are mainly about logit models." />
  

<meta name="author" content="Bill" />


<meta name="date" content="2019-12-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="linear-mixed-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bill's Stat Project</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Basics</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#logit"><i class="fa fa-check"></i><b>1.1</b> Logit</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#probit"><i class="fa fa-check"></i><b>1.2</b> Probit</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> MLE</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-idea-of-mle"><i class="fa fa-check"></i><b>2.1</b> Basic idea of MLE</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#coin-flip-example-probit-and-logit"><i class="fa fa-check"></i><b>2.2</b> Coin flip example, probit, and logit</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#probit-1"><i class="fa fa-check"></i><b>2.2.1</b> Probit</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#logit-1"><i class="fa fa-check"></i><b>2.2.2</b> Logit</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#further-on-logit"><i class="fa fa-check"></i><b>2.3</b> Further on logit</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#fisher-information"><i class="fa fa-check"></i><b>2.4</b> Fisher information</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#references"><i class="fa fa-check"></i><b>2.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html"><i class="fa fa-check"></i><b>3</b> Linear Mixed Models</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#calculate-mean"><i class="fa fa-check"></i><b>3.1</b> Calculate mean</a></li>
<li class="chapter" data-level="3.2" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#test-the-treatment-effect"><i class="fa fa-check"></i><b>3.2</b> Test the treatment effect</a></li>
<li class="chapter" data-level="3.3" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#another-example"><i class="fa fa-check"></i><b>3.3</b> Another example</a></li>
<li class="chapter" data-level="3.4" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#full-lmm-model"><i class="fa fa-check"></i><b>3.4</b> Full LMM model</a></li>
<li class="chapter" data-level="3.5" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#serial-correlations-in-time-and-space"><i class="fa fa-check"></i><b>3.5</b> Serial correlations in time and space</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-concepts.html"><a href="basic-concepts.html"><i class="fa fa-check"></i><b>4</b> Basic Concepts</a><ul>
<li class="chapter" data-level="4.1" data-path="basic-concepts.html"><a href="basic-concepts.html#score"><i class="fa fa-check"></i><b>4.1</b> Score</a></li>
<li class="chapter" data-level="4.2" data-path="basic-concepts.html"><a href="basic-concepts.html#gradient-and-jacobian"><i class="fa fa-check"></i><b>4.2</b> Gradient and Jacobian</a></li>
<li class="chapter" data-level="4.3" data-path="basic-concepts.html"><a href="basic-concepts.html#hessian-and-fisher-scoring"><i class="fa fa-check"></i><b>4.3</b> Hessian and Fisher Scoring</a></li>
<li class="chapter" data-level="4.4" data-path="basic-concepts.html"><a href="basic-concepts.html#canonical-link-function"><i class="fa fa-check"></i><b>4.4</b> Canonical link function</a></li>
<li class="chapter" data-level="4.5" data-path="basic-concepts.html"><a href="basic-concepts.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>4.5</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="4.6" data-path="basic-concepts.html"><a href="basic-concepts.html#taylor-series"><i class="fa fa-check"></i><b>4.6</b> Taylor series</a></li>
<li class="chapter" data-level="4.7" data-path="basic-concepts.html"><a href="basic-concepts.html#references-1"><i class="fa fa-check"></i><b>4.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="computing-techniques.html"><a href="computing-techniques.html"><i class="fa fa-check"></i><b>5</b> Computing techniques</a><ul>
<li class="chapter" data-level="5.0.1" data-path="computing-techniques.html"><a href="computing-techniques.html#monte-carlo-approximation"><i class="fa fa-check"></i><b>5.0.1</b> Monte carlo approximation</a></li>
<li class="chapter" data-level="5.0.2" data-path="computing-techniques.html"><a href="computing-techniques.html#importance-sampling"><i class="fa fa-check"></i><b>5.0.2</b> Importance sampling</a></li>
<li class="chapter" data-level="5.0.3" data-path="computing-techniques.html"><a href="computing-techniques.html#newton-raphson-algorithm"><i class="fa fa-check"></i><b>5.0.3</b> Newton Raphson algorithm</a></li>
<li class="chapter" data-level="5.1" data-path="computing-techniques.html"><a href="computing-techniques.html#newton-raphson"><i class="fa fa-check"></i><b>5.1</b> Newton Raphson</a></li>
<li class="chapter" data-level="5.2" data-path="computing-techniques.html"><a href="computing-techniques.html#references-2"><i class="fa fa-check"></i><b>5.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html"><i class="fa fa-check"></i><b>6</b> Generalized Linear Mixed Models</a><ul>
<li class="chapter" data-level="6.1" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#basics-of-glmm"><i class="fa fa-check"></i><b>6.1</b> Basics of GLMM</a></li>
<li class="chapter" data-level="6.2" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#some-references"><i class="fa fa-check"></i><b>6.2</b> Some References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="twitter-example.html"><a href="twitter-example.html"><i class="fa fa-check"></i><b>7</b> Twitter Example</a><ul>
<li class="chapter" data-level="7.1" data-path="twitter-example.html"><a href="twitter-example.html#model"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="twitter-example.html"><a href="twitter-example.html#simulating-data-of-senators-on-twitter"><i class="fa fa-check"></i><b>7.2</b> Simulating Data of Senators on Twitter</a></li>
<li class="chapter" data-level="7.3" data-path="twitter-example.html"><a href="twitter-example.html#simulating-data-of-conservative-users-on-twitter-and-model-testing"><i class="fa fa-check"></i><b>7.3</b> Simulating Data of Conservative Users on Twitter and Model Testing</a></li>
<li class="chapter" data-level="7.4" data-path="twitter-example.html"><a href="twitter-example.html#simulating-data-of-liberal-users-on-twitter-and-model-testing"><i class="fa fa-check"></i><b>7.4</b> Simulating Data of Liberal Users on Twitter and Model Testing</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://www.williamsding.com/" target="blank">Bill's website</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Generalized Linear Mixed Models &amp; R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> MLE</h1>
<div id="basic-idea-of-mle" class="section level2">
<h2><span class="header-section-number">2.1</span> Basic idea of MLE</h2>
<p>Suppose that we flip a coin, <span class="math inline">\(y_i=0\)</span> for tails and <span class="math inline">\(y_i=1\)</span> for heads. If we get <span class="math inline">\(p\)</span> heads from <span class="math inline">\(n\)</span> trials, we can get the proportion of heads is <span class="math inline">\(p/n\)</span>, which is the sample mean. If we do not do any further calculation, this is our best guess.</p>
<p>Suppose that the true proablity is <span class="math inline">\(\rho\)</span>, then we can get:</p>
<p><span class="math display">\[
\mathbf{L}(y_i)=\begin{cases} \rho \;\;\:   y_i = 1 \\ 1-\rho \;\;\:  y_i = 0 \end{cases}
\]</span> Thus, we can also write it as follows. <span class="math display">\[\mathbf{L}(y_i) = \rho^{y_i}(1-\rho)^{1-y_i}\]</span></p>
<p>Thus, we can get:</p>
<p><span class="math display">\[\prod \mathbf{L}(y_i|\rho)=\rho^{\sum y_i}(1-\rho)^{\sum(1-y_i)}\]</span> Further, we can get a log-transformed format.</p>
<p><span class="math display">\[log (\prod \mathbf{L}(y_i|\rho))=\sum y_i log \rho + \sum(1-y_i) log(1-\rho)\]</span></p>
<p>To maximize the log-function above, we can calculate the derivative with respect to <span class="math inline">\(\rho\)</span>. <span class="math display">\[\frac{\partial log (\prod \mathbf{L}(y_i|\rho)) }{\partial \rho}=\sum y_i \frac{1}{\rho}-\sum(1-y_i) \frac{1}{1-\rho}\]</span> Set the derivative to zero and solve for <span class="math inline">\(\rho\)</span>, we can get</p>
<p><span class="math display">\[\sum y_i \frac{1}{\rho}-\sum(1-y_i) \frac{1}{1-\rho}=0\]</span> <span class="math display">\[\Rightarrow (1-\rho)\sum y_i - \rho \sum(1-y_i) =0\]</span> <span class="math display">\[\Rightarrow \sum y_i-\rho\sum y_i - n\rho +\rho\sum y_i =0\]</span> <span class="math display">\[\Rightarrow \sum y_i - n\rho  =0\]</span> <span class="math display">\[\Rightarrow \rho  = \frac{\sum y_i}{n}=\frac{p}{n}\]</span> Thus, we can see that the <span class="math inline">\(\rho\)</span> maximizing the likelihood function is equal to the sample mean.</p>
</div>
<div id="coin-flip-example-probit-and-logit" class="section level2">
<h2><span class="header-section-number">2.2</span> Coin flip example, probit, and logit</h2>
<p>In the example above, we are not really trying to estimate a lot of regression coefficients. What we are doing actually is to calculate the sample mean, or intercept in the regresion sense. What does it mean? Let’s use some data to explain it.</p>
<p>Suppose that we flip a coin 20 times and observe 8 heads. We can use the R’s glm function to esimate the <span class="math inline">\(\rho\)</span>. If the result is consistent with what we did above, we should observe that the <span class="math inline">\(cdf\)</span> of the esimate of <span class="math inline">\(\beta_0\)</span> (i.e., intercept) should be equal to <span class="math inline">\(8/20=0.4\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coins&lt;-<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dt">times=</span><span class="dv">8</span>),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dt">times=</span><span class="dv">12</span>))
<span class="kw">table</span>(coins)</code></pre></div>
<pre><code>## coins
##  0  1 
## 12  8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coins&lt;-<span class="kw">as.data.frame</span>(coins)</code></pre></div>
<div id="probit-1" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Probit</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">probitresults &lt;-<span class="st"> </span><span class="kw">glm</span>(coins <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), <span class="dt">data =</span> coins)
probitresults</code></pre></div>
<pre><code>## 
## Call:  glm(formula = coins ~ 1, family = binomial(link = &quot;probit&quot;), 
##     data = coins)
## 
## Coefficients:
## (Intercept)  
##     -0.2533  
## 
## Degrees of Freedom: 19 Total (i.e. Null);  19 Residual
## Null Deviance:       26.92 
## Residual Deviance: 26.92     AIC: 28.92</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(probitresults<span class="op">$</span>coefficients)</code></pre></div>
<pre><code>## (Intercept) 
##         0.4</code></pre>
<p>As we can see the intercept is <span class="math inline">\(-0.2533\)</span>, and thus <span class="math inline">\(\Phi(-0.2533471)=0.4\)</span></p>
</div>
<div id="logit-1" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Logit</h3>
<p>We can also use logit link to calculate the intercept as well. Recall that</p>
<p><span class="math display">\[p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span> Thus,</p>
<p><span class="math display">\[p(y=1)=\frac{e^{\beta_0}}{1+e^{\beta_0}}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logitresults &lt;-<span class="st"> </span><span class="kw">glm</span>(coins <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>), <span class="dt">data =</span> coins)
logitresults<span class="op">$</span>coefficients</code></pre></div>
<pre><code>## (Intercept) 
##  -0.4054651</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(logitresults<span class="op">$</span>coefficients)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(logitresults<span class="op">$</span>coefficients))</code></pre></div>
<pre><code>## (Intercept) 
##         0.4</code></pre>
<p>Note that, the defaul link for the binomial in the glm function in logit.</p>
</div>
</div>
<div id="further-on-logit" class="section level2">
<h2><span class="header-section-number">2.3</span> Further on logit</h2>
<p>The probablity of <span class="math inline">\(y=1\)</span> is as follows:</p>
<p><span class="math display">\[p=p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span></p>
<p>Thus, the likelihood function is as follows:</p>
<p><span class="math display">\[L=\prod p^{y_i}(1-p)^{1-y_i}=\prod (\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}})^{y_i}(\frac{1}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}})^{1-y_i}\]</span></p>
<p><span class="math display">\[=\prod (1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)})^{-y_i}(1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n})^{-(1-y_i)}\]</span></p>
<p>Thus, the log-likelihood is as follows: <span class="math display">\[logL=\sum (-y_i \cdot log(1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)})-(1-y_i)\cdot log(1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}))\]</span></p>
<p>Typically, optimisers minimize a function, so we use negative log-likelihood as minimising that is equivalent to maximising the log-likelihood or the likelihood itself.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Source of R code: https://www.r-bloggers.com/logistic-regression/</span>

mle.logreg =<span class="st"> </span><span class="cf">function</span>(fmla, data)
{
  <span class="co"># Define the negative log likelihood function</span>
  logl &lt;-<span class="st"> </span><span class="cf">function</span>(theta,x,y){
    y &lt;-<span class="st"> </span>y
    x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(x)
    beta &lt;-<span class="st"> </span>theta[<span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x)]
    
    <span class="co"># Use the log-likelihood of the Bernouilli distribution, where p is</span>
    <span class="co"># defined as the logistic transformation of a linear combination</span>
    <span class="co"># of predictors, according to logit(p)=(x%*%beta)</span>
    loglik &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">-</span>y<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>(x<span class="op">%*%</span>beta))) <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x<span class="op">%*%</span>beta)))
    <span class="kw">return</span>(<span class="op">-</span>loglik)
  }
  
  <span class="co"># Prepare the data</span>
  outcome =<span class="st"> </span><span class="kw">rownames</span>(<span class="kw">attr</span>(<span class="kw">terms</span>(fmla),<span class="st">&quot;factors&quot;</span>))[<span class="dv">1</span>]
  dfrTmp =<span class="st"> </span><span class="kw">model.frame</span>(data)
  x =<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">model.matrix</span>(fmla, <span class="dt">data=</span>dfrTmp))
  y =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.matrix</span>(data[,<span class="kw">match</span>(outcome,<span class="kw">colnames</span>(data))]))
  
  <span class="co"># Define initial values for the parameters</span>
  theta.start =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,(<span class="kw">dim</span>(x)[<span class="dv">2</span>]))
  <span class="kw">names</span>(theta.start) =<span class="st"> </span><span class="kw">colnames</span>(x)
  
  <span class="co"># Calculate the maximum likelihood</span>
  mle =<span class="st"> </span><span class="kw">optim</span>(theta.start,logl,<span class="dt">x=</span>x,<span class="dt">y=</span>y, <span class="dt">method =</span> <span class="st">&#39;BFGS&#39;</span>, <span class="dt">hessian=</span>T)
  out =<span class="st"> </span><span class="kw">list</span>(<span class="dt">beta=</span>mle<span class="op">$</span>par,<span class="dt">vcov=</span><span class="kw">solve</span>(mle<span class="op">$</span>hessian),<span class="dt">ll=</span><span class="dv">2</span><span class="op">*</span>mle<span class="op">$</span>value)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mydata =<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&#39;https://stats.idre.ucla.edu/stat/data/binary.csv&#39;</span>))
mylogit1 =<span class="st"> </span><span class="kw">glm</span>(admit<span class="op">~</span>gre<span class="op">+</span>gpa<span class="op">+</span><span class="kw">as.factor</span>(rank), <span class="dt">family=</span>binomial, <span class="dt">data=</span>mydata)

mydata<span class="op">$</span>rank =<span class="st"> </span><span class="kw">factor</span>(mydata<span class="op">$</span>rank) <span class="co">#Treat rank as a categorical variable</span>
fmla =<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;admit~gre+gpa+rank&quot;</span>) <span class="co">#Create model formula</span>
mylogit2 =<span class="st"> </span><span class="kw">mle.logreg</span>(fmla, mydata) <span class="co">#Estimate coefficients</span>


 <span class="kw">print</span>(<span class="kw">cbind</span>(<span class="kw">coef</span>(mylogit1), mylogit2<span class="op">$</span>beta))</code></pre></div>
<pre><code>##                          [,1]         [,2]
## (Intercept)      -3.989979073 -3.772676422
## gre               0.002264426  0.001375522
## gpa               0.804037549  0.898201239
## as.factor(rank)2 -0.675442928 -0.675543009
## as.factor(rank)3 -1.340203916 -1.356554831
## as.factor(rank)4 -1.551463677 -1.563396035</code></pre>
</div>
<div id="fisher-information" class="section level2">
<h2><span class="header-section-number">2.4</span> Fisher information</h2>
<p><a href="https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf">https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf</a></p>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">2.5</span> References</h2>
<p><a href="http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf" class="uri">http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf</a></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-mixed-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-MLE.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
