# Bayesian - 2

## Components of Bayesian models

$$y_i=\mu+\epsilon_i$$

Where,

$$\epsilon_i \sim N(0, \sigma^2)$$

$$y_i \sim N(\mu, \sigma^2)$$
(Thus, $y_i$ is $=$ to a fixed $\mu$ plus with a $\epsilon_i$, whereas $y_i \sim N(\mu, \sigma^2)$. These two expressions are not exactly the same, but they are connected.)


__Likelihood:__ $P(y|\theta)$   ($P(y,\theta)=P(\theta)P(y|\theta)$)


__Prior:__ $P(\theta)$


__Posterior:__ 

$$P(\theta|y)=\frac{P(\theta, y)}{P(y)}=\frac{P(\theta, y)}{\int P(\theta, y)d\theta}=\frac{P(\theta)P(y|\theta)}{\int P(\theta, y)d\theta}=\frac{P(\theta)P(y|\theta)}{\int P(\theta)P(y|\theta)d\theta}$$
__Markts__

(1) The only random variables in frequentist models are the data. In contrast, Bayesian paradigm also uses probability to describe one's uncertainty about unknown model parameters.


(2) Consider the following model for binary outcome $y$:

$y_i|\theta_i \sim Bern (\theta_i), i=1,2,3...6$

$\theta_i |\alpha \sim Beta(a, b_0), i=1,2,3...6$

$\alpha \sim Exp(r_0)$

Thus, the joint distribution of all variable:

$$\prod_{i=1}^{6}[\theta_i^{y_i}(1-\theta_i)^{1-y_i}\frac{\Gamma(a+b_0)}{\Gamma(a)\Gamma(b_0)}\theta_i^{a-1}(1-\theta_i)^{b_0-1}]r_0e^{-r_0 \alpha}$$
(Question: Why not write it as $a_i$?)


## Monte Carlo Estimation

### Mean and Variance: Application of Central Limit Theorem

If we simulate 100 samples from a Gamma(2,1), what is the approximate distribution of the sample average $\bar{x^*}=\frac{1}{100} \sum_{i=1}^{100} x_i^*$?


As we know, based on the central limit theorem, the approximating distribution is normal with the mean equal to the sample mean, and the variance equal to the variance of the orignal variable divided by the sample size. We know that the mean for Gamma(2,1) is $\frac{2}{1}=2$ and variance is $\frac{2}{1^2}=2$. Thus, we know that the mean for the distribution for $\bar{x^*}$ is 2, whereas the variance is $\frac{2}{100}$. Thus, it is $N(2,0.02)$.

Thus, we can get the generalized formula as follows:

$$\bar{\theta^*} \sim N(E(\theta), \frac{Var(\theta)}{m})$$
Note that, it is the variance for $\bar{\theta^*}$, not $\theta$. The approximate for the variance for $\theta$ is $\frac{1}{m}\sum(\theta_i^*-\bar{\theta^*})^2$.


The following R code is for the $\theta$: 
```{R}
sample_data_gamma<-rgamma(10000,4,2)
mean(sample_data_gamma)
var(sample_data_gamma)
```

The following R code is for the $\bar{\theta^*}$. As we can see, the variance is 0.0016, which is close the theoretical value of $\frac{1}{1000}=0.001$. 
```{R}
set.seed(123)
mean_c<-c()
for(i in 1:10)
{mean_c[i]<-mean(rgamma(1000,4,2))}
var(mean_c)
```

__Side Note__

Note that the definition of variance is:

$$V(x)=E[(x-\mu)^2]$$

Thus, we can calculate the variance using integral:

$$V(x)=\int (x-\mu)^2 f(x)dx$$
When we using samples from the simulation, we will get the following:

$$V(x)=\int (x-\mu)^2 f(x)dx= \frac{1}{m} \sum (x_i^*-\bar{x^*})^2$$
Thus, we can use $Var$ function in $R$ with simulated sample to calculate the variance. 

### Expected value and probability

If you know that $\theta \sim Beta(5,3)$, what is the approximate for the $E(\frac{\theta}{1-\theta})$? You can use the following R code to calculate it.

```{R}
sample_data<-rbeta(1000,5,3)
mean(sample_data/(1-sample_data))
```

If you want to calculate the approximate for the probability that $\frac{\theta}{1-\theta}$ is greater 1.

```{R}
mean((sample_data/(1-sample_data))>1)
```

### Quantile

Use Monte Carlo to approximate the 0.3 quantile of N(0,1). Note that, the idea of quantile is to quantify the probability. Thus, the number we will get for 0.3 quantile if the value for the random variable's cdf $\int_{-\infty}^{quantile-number}dx$. As we can see below, $quantile(sample_data2,0.3)$ gets the result of -0.52, which is the same from the $qnorm(0.3)$. Note that, the cdf of $pnorm(-0.52)$ will get the probablity of $0.3$.

```{R}
sample_data2<-rnorm(10000,0,1)
quantile(sample_data2,0.3)
qnorm(0.3)
pnorm(-0.52)
```


##  Prior predictive distributions

$$y|\theta \sim Bin(10, \theta)$$
$$\theta \sim Beta(2,2)$$

Simulate:

(1) $\theta^*$ from beta

(2) Given $\theta$, draw $y_i^* \sim Bin(10, \theta_i^*)$

(3) Get pairs ($y_i^*$,$\theta_i^*$)

```{R}
m=1000

y=numeric(m)
phi=numeric(m)

for (i in 1:m)
{
  phi[i]=rbeta(1,shape1=2,shape2 = 2)
  y[i]=rbinom(1,size=10,prob = phi[i])
}

# we can use vector method

phi=rbeta(m,shape1 = 2,shape2 = 2)
y=rbinom(m,size=10,prob = phi)
table(y)/m

# The marginal distribution of y
plot(table(y)/m)

# the marginal expected value of y
mean(y)

```
