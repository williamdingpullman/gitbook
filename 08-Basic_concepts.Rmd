# Basic Concepts

## Score

The score is the gradient (the vector of partial derivatives) of $log L(\theta)$, with respect to an m-dimensional parameter vector $\theta$.

$$S(\theta) = \frac{\partial\ell}{\partial \theta}$$

Such differentiation will generate a $m\times 1$ row vector, which indicates the sensitivity of the likelihood.

Quote from Steffen Lauritzen's slides: "Generally the solution to this equation must be calculated by iterative methods. One of the most common methods is the Newton–Raphson
method and this is based on successive approximations to the solution, using Taylor’s theorem to approximate the equation."


For instance, using logit link, we can get the first derivative of log likelihood logistic regression as follows. We can not really find $\beta$ easily to make the equation to be 0. 

$$\begin{aligned}
\frac{\partial \ell} {\partial \beta} 
&= \sum_{i=1}^{n}x_i^T[y_i-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}] \\
&=\sum_{i=1}^{n} x_i^T[y_i-\hat{y_i}]
\end{aligned}$$

## Canonical link function

For $n$ time Bernoulli (i.e., Binomial), its canonical link function is logit.


$$log \frac{p(y_i=1)}{1-p(y_i=1)}=\beta^Tx_i$$
Where,

$$p(y_i=1)= \frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}$$


We can rewrite it as follows.

$$\beta^Tx_i=log\frac{\mu}{1-\mu}=g(\mu)$$
(Side note: logit = log odds = log $\frac{Event - Happened }{Event - Not - Happened}$)

Where,

$$\mu=p(y=1)=\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}$$
Thus, we know that $\mu$ or $p(y=1)$ is the mean function. Recall that, $n$ trails of coin flips, and get $p$ heads. Thus $\mu = \frac{p}{n}$.



References:
Steffen Lauritzen's slides:
http://www.stats.ox.ac.uk/~steffen/teaching/bs2HT9/scoring.pdf


