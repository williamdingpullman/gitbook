# MLE {#intro}

## Basic idea of MLE

Suppose that we flip a coin, $y_i=0$ for tails and $y_i=1$ for heads. If we get $p$ heads from $n$ trials, we can get the proportion of heads is $p/n$, which is the sample mean. If we do not do any further calculation, this is our best guess. 

Suppose that the true proablity is $\rho$, then we can get:

$$
\mathbf{L}(y_i)=\begin{cases} \rho \;\;\:   y_i = 1 \\ 1-\rho \;\;\:  y_i = 0 \end{cases}
$$
Thus, we can also write it as follows.
$$\mathbf{L}(y_i) = \rho^{y_i}(1-\rho)^{1-y_i}$$

Thus, we can get:

$$\prod \mathbf{L}(y_i|\rho)=\rho^{\sum y_i}(1-\rho)^{\sum(1-y_i)}$$
Further, we can get a log-transformed format.

$$log (\prod \mathbf{L}(y_i|\rho))=\sum y_i log \rho + \sum(1-y_i) log(1-\rho)$$

To maximize the log-function above, we can calculate the derivative with respect to $\rho$.
$$\frac{\partial log (\prod \mathbf{L}(y_i|\rho)) }{\partial \rho}=\sum y_i \frac{1}{\rho}-\sum(1-y_i) \frac{1}{1-\rho}$$
Set the derivative to zero and solve for $\rho$, we can get

$$\sum y_i \frac{1}{\rho}-\sum(1-y_i) \frac{1}{1-\rho}=0$$
$$\Rightarrow (1-\rho)\sum y_i - \rho \sum(1-y_i) =0$$
$$\Rightarrow \sum y_i-\rho\sum y_i - n\rho +\rho\sum y_i =0$$
$$\Rightarrow \sum y_i - n\rho  =0$$
$$\Rightarrow \rho  = \frac{\sum y_i}{n}=\frac{p}{n}$$
Thus, we can see that the $\rho$ maximizing the likelihood function is equal to the sample mean.

## Coin flip example, probit, and logit 

In the example above, we are not really trying to estimate a lot of regression coefficients. What we are doing actually is to calculate the sample mean, or intercept in the regresion sense. What does it mean? Let's use some data to explain it. 

Suppose that we flip a coin 20 times and observe 8 heads. We can use the R's glm function to esimate the $\rho$. If the result is consistent with what we did above, we should observe that the $cdf$ of the esimate of $\beta_0$ (i.e., intercept) should be equal to $8/20=0.4$.

```{R}
coins<-c(rep(1,times=8),rep(0,times=12))
table(coins)
coins<-as.data.frame(coins)
```

### Probit


```{R}
probitresults <- glm(coins ~ 1, family = binomial(link = "probit"), data = coins)
probitresults
pnorm(probitresults$coefficients)
```

As we can see the intercept is $-0.2533$, and thus $\Phi(-0.2533471)=0.4$


### Logit

We can also use logit link to calculate the intercept as well. Recall that

$$p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}$$
Thus,

$$p(y=1)=\frac{e^{\beta_0}}{1+e^{\beta_0}}$$

```{R}
logitresults <- glm(coins ~ 1, family = binomial(link = "logit"), data = coins)
logitresults$coefficients
exp(logitresults$coefficients)/(1+exp(logitresults$coefficients))

```

Note that, the defaul link for the binomial in the glm function in logit.

## Further on logit 

The probablity of $y=1$ is as follows:

$$p=p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}$$ 

Thus, the likelihood function is as follows:

$$L=\prod p^{y_i}(1-p)^{1-y_i}=\prod (\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}})^{y_i}(\frac{1}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}})^{1-y_i}$$

$$=\prod (1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)})^{-y_i}(1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n})^{-(1-y_i)}$$


Thus, the log-likelihood is as follows:
$$logL=\sum (-y_i \cdot log(1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)})-(1-y_i)\cdot log(1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}))$$  


Typically, optimisers minimize a function, so we use negative log-likelihood as minimising that is equivalent to maximising the log-likelihood or the likelihood itself.


```{r}

#Source of R code: https://www.r-bloggers.com/logistic-regression/

mle.logreg = function(fmla, data)
{
  # Define the negative log likelihood function
  logl <- function(theta,x,y){
    y <- y
    x <- as.matrix(x)
    beta <- theta[1:ncol(x)]
    
    # Use the log-likelihood of the Bernouilli distribution, where p is
    # defined as the logistic transformation of a linear combination
    # of predictors, according to logit(p)=(x%*%beta)
    loglik <- sum(-y*log(1 + exp(-(x%*%beta))) - (1-y)*log(1 + exp(x%*%beta)))
    return(-loglik)
  }
  
  # Prepare the data
  outcome = rownames(attr(terms(fmla),"factors"))[1]
  dfrTmp = model.frame(data)
  x = as.matrix(model.matrix(fmla, data=dfrTmp))
  y = as.numeric(as.matrix(data[,match(outcome,colnames(data))]))
  
  # Define initial values for the parameters
  theta.start = rep(0,(dim(x)[2]))
  names(theta.start) = colnames(x)
  
  # Calculate the maximum likelihood
  mle = optim(theta.start,logl,x=x,y=y, method = 'BFGS', hessian=T)
  out = list(beta=mle$par,vcov=solve(mle$hessian),ll=2*mle$value)
}
```


```{R}
mydata = read.csv(url('https://stats.idre.ucla.edu/stat/data/binary.csv'))
mylogit1 = glm(admit~gre+gpa+as.factor(rank), family=binomial, data=mydata)

mydata$rank = factor(mydata$rank) #Treat rank as a categorical variable
fmla = as.formula("admit~gre+gpa+rank") #Create model formula
mylogit2 = mle.logreg(fmla, mydata) #Estimate coefficients


 print(cbind(coef(mylogit1), mylogit2$beta))
```

## References

http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf

