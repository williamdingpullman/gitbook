# Generalized Linear Mixed Models

## Computing techniques

Since GLMM can use EM algorithm in its maximum likelihood calculation (see McCulloch, 1994), it is practically useful to rehearse EM and other computing techniques.

### Monte carlo approximation

Example: calculate the integral of $p(z>2)$ when $z \sim N(0,1)$. To use Monte Carlo approximation, we can have an indicator function, which will determine whether the sample from $N(0,1)$ will be included into the calculation of the integral.

```{R}
Nsim=10^4

indicator=function(x){
y=ifelse((x>2),1,0)
return(y)}

newdata<-rnorm(Nsim, 0,1 )

mc=c(); v=c(); upper=c(); lower=c()

for (j in 1:Nsim)
{
mc[j]=mean(indicator(newdata[1:j]))
v[j]=(j^{-1})*var(indicator(newdata[1:j]))
upper[j]=mc[j]+1.96*sqrt(v[j])
lower[j]=mc[j]-1.96*sqrt(v[j])
}

library(ggplot2)
values=c(mc,upper,lower)
type=c(rep("mc",Nsim),rep("upper",Nsim),rep("lower",Nsim))
iter=rep(seq(1:Nsim),3)
data=data.frame(val=values, tp=type, itr=iter)
Rcode<-ggplot(data,aes(itr,val,col=tp))+geom_line(size=0.5)
Rcode+geom_hline(yintercept=1-pnorm(2),color="green",size=0.5)
```

### Importance sampling

Importance sampling has samples generated from a different distribution than the distribution of interest. Specifically, assume that we want to calculate the expectation value of $h(x)$, and $x \sim f(x)$.

$$E(h(x))=\int h(x) f(x) dx = \int h(x) \frac{f(x)}{g(x)} g(x) dx $$
We can sample $x_i$ from $g(x)$ and then calculate the mean of $h(x_i) \frac{f(x_i)}{g(x_i)}$.



Using the same explane above, we can use a shifted exponential distribution to help calculate the intergral for normal distribution. Specifically,

$$\int_2^{\infty} \frac{1}{2 \pi} e^{-\frac{1}{2}x^2}dx = \int_2^{\infty} \frac{\frac{1}{2 \pi} e^{-\frac{1}{2}x^2}}{e^{-(x-2)}} e^{-(x-2)}dx $$
The idea is that, we can generate $x_i$ from exponential distribution of $e{-(x-2)}$, and then insert them into the density function of $\frac{\frac{1}{2 \pi} e^{-\frac{1}{2}x^2}}{e^{-(x-2)}}$. Thus, as you can see, importance sampling is based on the law of large numbers (i.e., If the same experiment or study is repeated independently a large number of times, the average of the results of the trials must be close to the expected value). We can use it to calculate integral based on link of the definition of expected value. 

```{R}
Nsim=10^4
normal_density=function(x)
{y=(1/sqrt(2*pi))*exp(-0.5*(x^2))
return(y)}
x=2-log(runif(Nsim))
ImpS=c(); v=c(); upper=c(); lower=c()
for (j in 1:Nsim)
{
ImpS[j]=mean(normal_density(x[1:j])/exp(-(x[1:j]-2)))
v[j]=(j^{-1})*var(normal_density(x[1:j])/exp(-(x[1:j]-2)))
upper[j]=ImpS[j]+1.96*sqrt(v[j])
lower[j]=ImpS[j]-1.96*sqrt(v[j])
}

library(ggplot2)
values=c(ImpS,upper,lower)
type=c(rep("mc",Nsim),rep("upper",Nsim),rep("lower",Nsim))
iter=rep(seq(1:Nsim),3)
data=data.frame(val=values, tp=type, itr=iter)
ggplot(data,aes(itr,val,col=tp))+geom_line(size=0.5)+
geom_hline(yintercept=1-pnorm(2),color="green",size=0.5)

```

###Newton Raphson algorithm

For nonlinear functions, it is sometimes difficult to calculate MLEs. Newton Raphson algorithm is an iterative procedure to calculate MlEs. 

The basic idea of Newton Raphson is to find a approximate function that can be easily maximized analytically. We need some theoretical background from Taylor's Theorem. 

If $f$ has $k+1$ times differentiable on an open interval $I$. For any $x$ and $x+h$ in $I$, there is a point of $w$ between $x$ and $x+h$ where we can get the following:

$$f(x+h)=f(x)+f^{'}h+\frac{1}{2}f^{''}h^2+...+\frac{1}{k!}f^{[k]}(x)h^k+\frac{1}{(k+1)!}f^{[k+1]}(w)h^{k+1}$$
If $h$ goes to be close to $0$, the higher order terms will go to $0$ as well. Thus, we can get:

$$f(x+h) \approx f(x)+f^{'}(x)h $$
This is the first order Taylor approximation of $f$ at $x$. In a similar vein, we also have the second order Taylor approximation of $f$ at $x$ as follows. 

$$f(x+h)=f(x)+f^{'}h+\frac{1}{2}f^{''}h^2$$
For the first order, we can rewrite it as follows. 

$$f(x+h) \approx f(x)+f^{'}(x)h = a+bh$$
where, 

$$ a = f(x), b=f^{'}(x)$$
Similarly,

$$f(x+h)\approx f(x)+f^{'}(x)h+\frac{1}{2}f^{''}(x)h^2=a+bh+\frac{1}{2}ch^2$$
We can calculate the derivative with respect to $h$, we can get:

$$f^{'}(x+h) \approx b+ch$$
We can then set it to zero, and get:

$$0=b+c \hat{h}$$
Thus, we can get,

$$\hat{h} = -\frac{b}{c}=-\frac{f^{'}(x)}{f^{''}(x)}$$
Thus, we can get that the following can maximize $f$ at $x$:

$$x+\hat{h}=x-\frac{f^{'}(x)}{f^{''}(x)}$$
Thus, the basic idea of Newton Raphson algorithm is as follows. 
- set a tolerance (typically a very small number)
- Check if $|f^{'}(x)|< the tolerance$. If not, $i \leftarrow i+1; x_i\leftarrow x_{i-1}-\frac{f^{'}(x_{i-1})}{f^{''}(x_{i-1})}$


Practice:

(1) Solve the $x^3-5=0$
Note that, this is obviously not a maximization problem. In contrast, it involves a function with zero. As we can see, we can think it as the first order of Taylor approximation. That is, itself is a $f^{'}(x)=x^3-5=0$. As we can see the following plot, it converts pretty quick.

```{R}
f_firstorder=function(x){x^3-5}
f_secondorder=function(x){3*x}
x_old=1;tolerance=1e-3
max_its=2000;iteration=1;difference=2
c_iteration<-c() ## to collect numbers generated in the iteration process 
while(difference>tolerance & iteration<max_its){
  x_updated=x_old-(f_firstorder(x_old)/f_secondorder(x_old))
  difference=abs(x_updated-x_old);
  iteration=iteration+1;
  x_old=x_updated
  c_iteration<-c(c_iteration,x_updated)}

plot(c_iteration,type="b")
``` 

## Basics of GLMM

Recall the formula in the probit model:

$$Y^*=X\beta+\epsilon, \epsilon \sim N(0,\sigma^2)=N(0,I)$$
Similar to LMM, binary model with random effect can be written as follows.

$$Y^*=X\beta+ Z u+\epsilon$$
where, 

$$\epsilon \sim N(0,I)$$
$$u \sim N(0, D)$$

We also assume $\epsilon$ and $u$ are independent.Thus, we know that $D$ represents the virances of the random effects. If we make $u =1$, the model becomes the usual probit model. McCulloch (1994) states that there are a few advantages to use probit, rather than logit models. 



The following is the note from Charle E. McCulloch's "Maximum likelihood algorithems for Generalized Linear Mixed Models"

## Some References

http://www.biostat.umn.edu/~baolin/teaching/linmods/glmm.html

http://www.biostat.umn.edu/~baolin/teaching/probmods/GLMM_mcmc.html

https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
