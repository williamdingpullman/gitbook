<div id="my-section" class="section level1 unnumbered">
<h1>Preface: Motivation</h1>
<p>All the notes I have done here are the preparation for my stat master project, which will be about Generalized Linear Mixed Models. While I have tried my best, probably there are still some typos and erros. Please feel free to let me know in case you find one. Thank you!</p>
</div>
<div id="basics" class="section level1">
<h1><span class="header-section-number">1</span> Basics</h1>
<div id="logit" class="section level2">
<h2><span class="header-section-number">1.1</span> Logit</h2>
<p><span class="math display">\[f(x)=log(\frac{p(y=1)}{1-p(y=1)})\]</span> The basic idea of logistic regression: <span class="math display">\[p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span> Thus, <span class="math inline">\(e^{\beta_0+\beta_1x_1+...+\beta_nx_n}\)</span> can be from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>, and <span class="math inline">\(p(y=1)\)</span> will be always within the range of <span class="math inline">\((0,1)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f&lt;-<span class="cf">function</span>(x){<span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x))}
data&lt;-<span class="kw">seq</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(data,<span class="kw">f</span>(data),<span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-1-1.png" /><!-- --></p>
<p>We can also write the function into another format as follows: <span class="math display">\[log \frac{p(y=1)}{1-p(y=1)}= \beta_0+\beta_1x_1+...+\beta_nx_n\]</span> Thus, we know that the regression coeficients of <span class="math inline">\(\beta_i\)</span> actually change the &quot;log-odds&quot; of the event. Of course, note that the magnitude of <span class="math inline">\(\beta_i\)</span> is dependent upon the units of <span class="math inline">\(x_i\)</span>.</p>
<p>The following is an example testing whether that home teams are more likely to win in NFL games. The results show that the odd of winning is the same for both home and away teams.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mydata =<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&#39;https://raw.githubusercontent.com/nfl-football-ops/Big-Data-Bowl/master/Data/games.csv&#39;</span>))
mydata<span class="op">$</span>result_new&lt;-<span class="kw">ifelse</span>(mydata<span class="op">$</span>HomeScore<span class="op">&gt;</span>mydata<span class="op">$</span>VisitorScore,<span class="dv">1</span>,<span class="dv">0</span>)
<span class="kw">summary</span>(mydata<span class="op">$</span>result_new)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.4945  1.0000  1.0000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mylogit1 =<span class="st"> </span><span class="kw">glm</span>(result_new<span class="op">~</span><span class="dv">1</span>, <span class="dt">family=</span>binomial, <span class="dt">data=</span>mydata)
<span class="kw">summary</span>(mylogit1)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = result_new ~ 1, family = binomial, data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.168  -1.168  -1.168   1.187   1.187  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -0.02198    0.20967  -0.105    0.917
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 126.14  on 90  degrees of freedom
## Residual deviance: 126.14  on 90  degrees of freedom
## AIC: 128.14
## 
## Number of Fisher Scoring iterations: 3</code></pre>
</div>
<div id="probit" class="section level2">
<h2><span class="header-section-number">1.2</span> Probit</h2>
<p>As noted above, logit <span class="math inline">\(f(x)=log(\frac{p(y=1)}{1-p(y=1)})\)</span> provides the resulting range of <span class="math inline">\((0,1)\)</span>. Another way to provide the same rage is through the cdf of normal distribution.The following R code is used to illusrate this process.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data2&lt;-<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(data2,<span class="kw">pnorm</span>(data2),<span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-3-1.png" /><!-- --> Thus, the cdf of normal distribution can be used to indicate the probability of <span class="math inline">\(p(y=1)\)</span>.</p>
<p><span class="math display">\[\Phi(\beta_0+\beta_1x_1+...+\beta_nx_n )= p(y=1)\]</span></p>
<p>Similar to logit model, we can also write the inverse function of the cdf to get the function that can be from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>.</p>
<p><span class="math display">\[\beta_0+\beta_1x_1+...+\beta_nx_n =\Phi^{-1}(p(y=1))\]</span></p>
<p>Thus, for example, if <span class="math inline">\(X\beta\)</span> = -2, based on <span class="math inline">\(\Phi(\beta_0+\beta_1x_1+...+\beta_nx_n )= p(y=1)\)</span> we can get that the <span class="math inline">\(p(y=1)=0.023\)</span>.</p>
<p>In contrast, if <span class="math inline">\(X\beta\)</span> = 3, the <span class="math inline">\(p(y=1)=0.999\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="op">-</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.02275013</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>## [1] 0.9986501</code></pre>
<p>Let's assume that there is a latent variable called <span class="math inline">\(Y^*\)</span> such that</p>
<p><span class="math display">\[Y^*=X\beta+\epsilon, \epsilon \sim N(0,\sigma^2)\]</span> You could think of <span class="math inline">\(Y^*\)</span> as a kind of &quot;proxy&quot; between <span class="math inline">\(X\beta+\epsilon\)</span> and the observed <span class="math inline">\(Y (1 or 0)\)</span>. Thus, we can get the following. Note that, it does not have to be zero, and can be any constant.</p>
<p><span class="math display">\[
Y^*=\begin{cases} 0 \;\;\: if \;  y_i^* \leq 0 \\ 1 \;\;\: if \;  y_i^* &gt; 0 \end{cases}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[y_i^* &gt; 0 \Rightarrow \beta^{&#39;}X_i + \epsilon_i &gt;0 \Rightarrow \epsilon_i &gt; -\beta^{&#39;}X_i\]</span></p>
<p>Thus, we can write it as follows. Note that <span class="math inline">\(\frac{ \epsilon_i}{\sigma} \sim N(0,1)\)</span></p>
<p><span class="math display">\[p(y=1|x_i)= p(y_i^* &gt;0|x_i)=p(\epsilon_i &gt; -\beta^{&#39;}X_i)= p(\frac{ \epsilon_i}{\sigma}&gt;\frac{-\beta^{&#39;}X_i}{\sigma})=\Phi(\frac{\beta^{&#39;}X_i}{\sigma}) \]</span> We thus can get:</p>
<p><span class="math display">\[p(y=0|x_i)=1-\Phi(\frac{\beta^{&#39;}X_i}{\sigma})\]</span></p>
<p>For <span class="math inline">\(p(y=1|x_i)=\Phi(\frac{\beta^{&#39;}X_i}{\sigma})\)</span>, we can not really estimate both <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> as they are in a ratio. We can assume <span class="math inline">\(\sigma =1\)</span>, then <span class="math inline">\(\epsilon \sim N(0,1)\)</span>. We know <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> since we observe them. Thus, we can write it as follows.</p>
<p><span class="math display">\[p(y=1|x_i)=\Phi(\beta^{&#39;}X_i)\]</span></p>
<!--chapter:end:index.Rmd-->
</div>
</div>
<div id="intro" class="section level1">
<h1><span class="header-section-number">2</span> MLE</h1>
<div id="basic-idea-of-mle" class="section level2">
<h2><span class="header-section-number">2.1</span> Basic idea of MLE</h2>
<p>Suppose that we flip a coin, <span class="math inline">\(y_i=0\)</span> for tails and <span class="math inline">\(y_i=1\)</span> for heads. If we get <span class="math inline">\(p\)</span> heads from <span class="math inline">\(n\)</span> trials, we can get the proportion of heads is <span class="math inline">\(p/n\)</span>, which is the sample mean. If we do not do any further calculation, this is our best guess.</p>
<p>Suppose that the true proablity is <span class="math inline">\(\rho\)</span>, then we can get:</p>
<p><span class="math display">\[
\mathbf{L}(y_i)=\begin{cases} \rho \;\;\:   y_i = 1 \\ 1-\rho \;\;\:  y_i = 0 \end{cases}
\]</span> Thus, we can also write it as follows. <span class="math display">\[\mathbf{L}(y_i) = \rho^{y_i}(1-\rho)^{1-y_i}\]</span></p>
<p>Thus, we can get:</p>
<p><span class="math display">\[\prod \mathbf{L}(y_i|\rho)=\rho^{\sum y_i}(1-\rho)^{\sum(1-y_i)}\]</span> Further, we can get a log-transformed format.</p>
<p><span class="math display">\[log (\prod \mathbf{L}(y_i|\rho))=\sum y_i log \rho + \sum(1-y_i) log(1-\rho)\]</span></p>
<p>To maximize the log-function above, we can calculate the derivative with respect to <span class="math inline">\(\rho\)</span>. <span class="math display">\[\frac{\partial log (\prod \mathbf{L}(y_i|\rho)) }{\partial \rho}=\sum y_i \frac{1}{\rho}-\sum(1-y_i) \frac{1}{1-\rho}\]</span> Set the derivative to zero and solve for <span class="math inline">\(\rho\)</span>, we can get</p>
<p><span class="math display">\[\sum y_i \frac{1}{\rho}-\sum(1-y_i) \frac{1}{1-\rho}=0\]</span> <span class="math display">\[\Rightarrow (1-\rho)\sum y_i - \rho \sum(1-y_i) =0\]</span> <span class="math display">\[\Rightarrow \sum y_i-\rho\sum y_i - n\rho +\rho\sum y_i =0\]</span> <span class="math display">\[\Rightarrow \sum y_i - n\rho  =0\]</span> <span class="math display">\[\Rightarrow \rho  = \frac{\sum y_i}{n}=\frac{p}{n}\]</span> Thus, we can see that the <span class="math inline">\(\rho\)</span> maximizing the likelihood function is equal to the sample mean.</p>
</div>
<div id="coin-flip-example-probit-and-logit" class="section level2">
<h2><span class="header-section-number">2.2</span> Coin flip example, probit, and logit</h2>
<p>In the example above, we are not really trying to estimate a lot of regression coefficients. What we are doing actually is to calculate the sample mean, or intercept in the regresion sense. What does it mean? Let's use some data to explain it.</p>
<p>Suppose that we flip a coin 20 times and observe 8 heads. We can use the R's glm function to esimate the <span class="math inline">\(\rho\)</span>. If the result is consistent with what we did above, we should observe that the <span class="math inline">\(cdf\)</span> of the esimate of <span class="math inline">\(\beta_0\)</span> (i.e., intercept) should be equal to <span class="math inline">\(8/20=0.4\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coins&lt;-<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dt">times=</span><span class="dv">8</span>),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dt">times=</span><span class="dv">12</span>))
<span class="kw">table</span>(coins)</code></pre></div>
<pre><code>## coins
##  0  1 
## 12  8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coins&lt;-<span class="kw">as.data.frame</span>(coins)</code></pre></div>
<div id="probit-1" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Probit</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">probitresults &lt;-<span class="st"> </span><span class="kw">glm</span>(coins <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), <span class="dt">data =</span> coins)
probitresults</code></pre></div>
<pre><code>## 
## Call:  glm(formula = coins ~ 1, family = binomial(link = &quot;probit&quot;), 
##     data = coins)
## 
## Coefficients:
## (Intercept)  
##     -0.2533  
## 
## Degrees of Freedom: 19 Total (i.e. Null);  19 Residual
## Null Deviance:       26.92 
## Residual Deviance: 26.92     AIC: 28.92</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(probitresults<span class="op">$</span>coefficients)</code></pre></div>
<pre><code>## (Intercept) 
##         0.4</code></pre>
<p>As we can see the intercept is <span class="math inline">\(-0.2533\)</span>, and thus <span class="math inline">\(\Phi(-0.2533471)=0.4\)</span></p>
</div>
<div id="logit-1" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Logit</h3>
<p>We can also use logit link to calculate the intercept as well. Recall that</p>
<p><span class="math display">\[p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span> Thus,</p>
<p><span class="math display">\[p(y=1)=\frac{e^{\beta_0}}{1+e^{\beta_0}}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logitresults &lt;-<span class="st"> </span><span class="kw">glm</span>(coins <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>), <span class="dt">data =</span> coins)
logitresults<span class="op">$</span>coefficients</code></pre></div>
<pre><code>## (Intercept) 
##  -0.4054651</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(logitresults<span class="op">$</span>coefficients)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(logitresults<span class="op">$</span>coefficients))</code></pre></div>
<pre><code>## (Intercept) 
##         0.4</code></pre>
<p>Note that, the defaul link for the binomial in the glm function in logit.</p>
</div>
</div>
<div id="further-on-logit" class="section level2">
<h2><span class="header-section-number">2.3</span> Further on logit</h2>
<p>The probablity of <span class="math inline">\(y=1\)</span> is as follows:</p>
<p><span class="math display">\[p=p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span></p>
<p>Thus, the likelihood function is as follows:</p>
<p><span class="math display">\[L=\prod p^{y_i}(1-p)^{1-y_i}=\prod (\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}})^{y_i}(\frac{1}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}})^{1-y_i}\]</span></p>
<p><span class="math display">\[=\prod (1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)})^{-y_i}(1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n})^{-(1-y_i)}\]</span></p>
<p>Thus, the log-likelihood is as follows: <span class="math display">\[logL=\sum (-y_i \cdot log(1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)})-(1-y_i)\cdot log(1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}))\]</span></p>
<p>Typically, optimisers minimize a function, so we use negative log-likelihood as minimising that is equivalent to maximising the log-likelihood or the likelihood itself.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Source of R code: https://www.r-bloggers.com/logistic-regression/</span>

mle.logreg =<span class="st"> </span><span class="cf">function</span>(fmla, data)
{
  <span class="co"># Define the negative log likelihood function</span>
  logl &lt;-<span class="st"> </span><span class="cf">function</span>(theta,x,y){
    y &lt;-<span class="st"> </span>y
    x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(x)
    beta &lt;-<span class="st"> </span>theta[<span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x)]
    
    <span class="co"># Use the log-likelihood of the Bernouilli distribution, where p is</span>
    <span class="co"># defined as the logistic transformation of a linear combination</span>
    <span class="co"># of predictors, according to logit(p)=(x%*%beta)</span>
    loglik &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">-</span>y<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>(x<span class="op">%*%</span>beta))) <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x<span class="op">%*%</span>beta)))
    <span class="kw">return</span>(<span class="op">-</span>loglik)
  }
  
  <span class="co"># Prepare the data</span>
  outcome =<span class="st"> </span><span class="kw">rownames</span>(<span class="kw">attr</span>(<span class="kw">terms</span>(fmla),<span class="st">&quot;factors&quot;</span>))[<span class="dv">1</span>]
  dfrTmp =<span class="st"> </span><span class="kw">model.frame</span>(data)
  x =<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">model.matrix</span>(fmla, <span class="dt">data=</span>dfrTmp))
  y =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.matrix</span>(data[,<span class="kw">match</span>(outcome,<span class="kw">colnames</span>(data))]))
  
  <span class="co"># Define initial values for the parameters</span>
  theta.start =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,(<span class="kw">dim</span>(x)[<span class="dv">2</span>]))
  <span class="kw">names</span>(theta.start) =<span class="st"> </span><span class="kw">colnames</span>(x)
  
  <span class="co"># Calculate the maximum likelihood</span>
  mle =<span class="st"> </span><span class="kw">optim</span>(theta.start,logl,<span class="dt">x=</span>x,<span class="dt">y=</span>y, <span class="dt">method =</span> <span class="st">&#39;BFGS&#39;</span>, <span class="dt">hessian=</span>T)
  out =<span class="st"> </span><span class="kw">list</span>(<span class="dt">beta=</span>mle<span class="op">$</span>par,<span class="dt">vcov=</span><span class="kw">solve</span>(mle<span class="op">$</span>hessian),<span class="dt">ll=</span><span class="dv">2</span><span class="op">*</span>mle<span class="op">$</span>value)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mydata =<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&#39;https://stats.idre.ucla.edu/stat/data/binary.csv&#39;</span>))
mylogit1 =<span class="st"> </span><span class="kw">glm</span>(admit<span class="op">~</span>gre<span class="op">+</span>gpa<span class="op">+</span><span class="kw">as.factor</span>(rank), <span class="dt">family=</span>binomial, <span class="dt">data=</span>mydata)

mydata<span class="op">$</span>rank =<span class="st"> </span><span class="kw">factor</span>(mydata<span class="op">$</span>rank) <span class="co">#Treat rank as a categorical variable</span>
fmla =<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;admit~gre+gpa+rank&quot;</span>) <span class="co">#Create model formula</span>
mylogit2 =<span class="st"> </span><span class="kw">mle.logreg</span>(fmla, mydata) <span class="co">#Estimate coefficients</span>


 <span class="kw">print</span>(<span class="kw">cbind</span>(<span class="kw">coef</span>(mylogit1), mylogit2<span class="op">$</span>beta))</code></pre></div>
<pre><code>##                          [,1]         [,2]
## (Intercept)      -3.989979073 -3.772676422
## gre               0.002264426  0.001375522
## gpa               0.804037549  0.898201239
## as.factor(rank)2 -0.675442928 -0.675543009
## as.factor(rank)3 -1.340203916 -1.356554831
## as.factor(rank)4 -1.551463677 -1.563396035</code></pre>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">2.4</span> References</h2>
<p><a href="http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf" class="uri">http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf</a></p>
<!--chapter:end:01-MLE.Rmd-->
</div>
</div>
<div id="linear-mixed-models" class="section level1">
<h1><span class="header-section-number">3</span> Linear Mixed Models</h1>
<p>The following is a shortened version of Jonathan Rosenblatt's LMM tutorial. <a href="http://www.john-ros.com/Rcourse/lme.html" class="uri">http://www.john-ros.com/Rcourse/lme.html</a>.</p>
<p>In addition, another reference is from Douglas Bates's R package document. <a href="https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf?fbclid=IwAR1nmmRP9A0BrhKdgBibNjM5acR_spTpXV8QlQGdmTWyQz3ZtV3LYn6kCbQ" class="uri">https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf?fbclid=IwAR1nmmRP9A0BrhKdgBibNjM5acR_spTpXV8QlQGdmTWyQz3ZtV3LYn6kCbQ</a></p>
<p>Assume that <span class="math inline">\(y\)</span> is a function of <span class="math inline">\(x\)</span> and <span class="math inline">\(u\)</span>, where <span class="math inline">\(x\)</span> is the fixed effect and <span class="math inline">\(u\)</span> is the random effect. Thus, we can get,</p>
<p><span class="math display">\[y|x, u = x&#39;\beta+z&#39;u+\epsilon\]</span></p>
<p>For random effect, one example can be that you want to test the treatment effect, and sample 8 observations from 4 groups. You measure before and after the treatment. In this case, <span class="math inline">\(x\)</span> represents the treatment effect, whereas <span class="math inline">\(z\)</span> represents the group effect (i.e., random effect). Note that, in this case, it reminds the paired t-test. Remember in SPSS, why do we do paired t-test? Typically, it is the case when we measure a subject (or, participant) twice. In this case, we can consider each participant as an unit of random effect (rather than as group in the last example.)</p>
<div id="calculate-mean" class="section level2">
<h2><span class="header-section-number">3.1</span> Calculate mean</h2>
<p>The following code generates 4 numbers (<span class="math inline">\(N(0,10)\)</span>) for 4 groups. Then, replicate it within each group.That is, in the end, there are 8 observations.</p>
<p>Note that, in the following code, there are no &quot;independent variables&quot;. Both the linear model and mixed model are actually just trying to calculate the mean. Note that lmer(y~1+1|groups) and lmer(y~1|groups) will generate the same results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
n.groups &lt;-<span class="st"> </span><span class="dv">4</span> <span class="co"># number of groups</span>
n.repeats &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># samples per group</span>
<span class="co">#Generating index for observations belong to the same group</span>
groups &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n.groups, <span class="dt">each=</span>n.repeats))
n &lt;-<span class="st"> </span><span class="kw">length</span>(groups)
<span class="co">#Generating 4 random numbers, assuming normal distribution</span>
z0 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n.groups, <span class="dv">0</span>, <span class="dv">10</span>) 
z &lt;-<span class="st"> </span>z0[<span class="kw">as.numeric</span>(groups)] <span class="co"># generate and inspect random group effects</span>
z</code></pre></div>
<pre><code>## [1] -5.6047565 -5.6047565 -2.3017749 -2.3017749 15.5870831 15.5870831  0.7050839
## [8]  0.7050839</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>) <span class="co"># generate measurement error</span>
beta0 &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># this is the actual parameter of interest! The global mean.</span>
y &lt;-<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>z <span class="op">+</span><span class="st"> </span>epsilon <span class="co"># sample from an LMM</span>

<span class="co"># fit a linear model assuming independence</span>
<span class="co"># i.e., assume that there is no &quot;group things&quot;.</span>
lm.<span class="dv">5</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span><span class="dv">1</span>)

<span class="co"># fit a mixed-model that deals with the group dependence</span>
<span class="co">#install.packages(&quot;lme4&quot;)</span>
<span class="kw">library</span>(lme4)
lme.<span class="fl">5.</span>a &lt;-<span class="st"> </span><span class="kw">lmer</span>(y<span class="op">~</span><span class="dv">1</span><span class="op">+</span><span class="dv">1</span><span class="op">|</span>groups) 
lme.<span class="fl">5.</span>b &lt;-<span class="st"> </span><span class="kw">lmer</span>(y<span class="op">~</span><span class="dv">1</span><span class="op">|</span>groups) 
lm.<span class="dv">5</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ 1)
## 
## Coefficients:
## (Intercept)  
##       4.283</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lme.<span class="fl">5.</span>a </code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: y ~ 1 + 1 | groups
## REML criterion at convergence: 36.1666
## Random effects:
##  Groups   Name        Std.Dev.
##  groups   (Intercept) 8.8521  
##  Residual             0.8873  
## Number of obs: 8, groups:  groups, 4
## Fixed Effects:
## (Intercept)  
##       4.283</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lme.<span class="fl">5.</span>b </code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: y ~ 1 | groups
## REML criterion at convergence: 36.1666
## Random effects:
##  Groups   Name        Std.Dev.
##  groups   (Intercept) 8.8521  
##  Residual             0.8873  
## Number of obs: 8, groups:  groups, 4
## Fixed Effects:
## (Intercept)  
##       4.283</code></pre>
</div>
<div id="test-the-treatment-effect" class="section level2">
<h2><span class="header-section-number">3.2</span> Test the treatment effect</h2>
<p>As we can see that, LLM and paired t-test generate the same t-value.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">times&lt;-<span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="dv">4</span>) <span class="co"># first time and second time</span>
times</code></pre></div>
<pre><code>## [1] 1 2 1 2 1 2 1 2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data_combined&lt;-<span class="kw">cbind</span>(y,groups,times)
data_combined</code></pre></div>
<pre><code>##               y groups times
## [1,] -3.4754687      1     1
## [2,] -1.8896915      1     2
## [3,]  0.1591413      2     1
## [4,] -1.5668361      2     2
## [5,] 16.9002303      3     1
## [6,] 17.1414212      3     2
## [7,]  3.9291657      4     1
## [8,]  3.0648977      4     2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lme_diff_times&lt;-<span class="st"> </span><span class="kw">lmer</span>(y<span class="op">~</span>times<span class="op">+</span>(<span class="dv">1</span><span class="op">|</span>groups)) 


t_results&lt;-<span class="kw">t.test</span>(y<span class="op">~</span>times, <span class="dt">paired=</span><span class="ot">TRUE</span>)

lme_diff_times</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: y ~ times + (1 | groups)
## REML criterion at convergence: 35.0539
## Random effects:
##  Groups   Name        Std.Dev.
##  groups   (Intercept) 8.845   
##  Residual             1.013   
## Number of obs: 8, groups:  groups, 4
## Fixed Effects:
## (Intercept)        times  
##      4.5691      -0.1908</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="st">&quot;The following results are from paired t-test&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;The following results are from paired t-test&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t_results<span class="op">$</span>statistic</code></pre></div>
<pre><code>##         t 
## 0.2664793</code></pre>
</div>
<div id="another-example" class="section level2">
<h2><span class="header-section-number">3.3</span> Another example</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Dyestuff, <span class="dt">package=</span><span class="st">&#39;lme4&#39;</span>)
<span class="kw">attach</span>(Dyestuff)</code></pre></div>
<pre><code>## The following objects are masked from Dyestuff (pos = 4):
## 
##     Batch, Yield</code></pre>
<pre><code>## The following objects are masked from Dyestuff (pos = 8):
## 
##     Batch, Yield</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Dyestuff</code></pre></div>
<pre><code>##    Batch Yield
## 1      A  1545
## 2      A  1440
## 3      A  1440
## 4      A  1520
## 5      A  1580
## 6      B  1540
## 7      B  1555
## 8      B  1490
## 9      B  1560
## 10     B  1495
## 11     C  1595
## 12     C  1550
## 13     C  1605
## 14     C  1510
## 15     C  1560
## 16     D  1445
## 17     D  1440
## 18     D  1595
## 19     D  1465
## 20     D  1545
## 21     E  1595
## 22     E  1630
## 23     E  1515
## 24     E  1635
## 25     E  1625
## 26     F  1520
## 27     F  1455
## 28     F  1450
## 29     F  1480
## 30     F  1445</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lme_batch&lt;-<span class="st"> </span><span class="kw">lmer</span>( Yield <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>Batch)  , Dyestuff )
<span class="kw">summary</span>(lme_batch)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Yield ~ 1 + (1 | Batch)
##    Data: Dyestuff
## 
## REML criterion at convergence: 319.7
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -1.4117 -0.7634  0.1418  0.7792  1.8296 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Batch    (Intercept) 1764     42.00   
##  Residual             2451     49.51   
## Number of obs: 30, groups:  Batch, 6
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  1527.50      19.38    78.8</code></pre>
</div>
<div id="full-lmm-model" class="section level2">
<h2><span class="header-section-number">3.4</span> Full LMM model</h2>
<p>In the following, I used the data from the package of lme4. For Days + (1 | Subject), it only has random intercept; in contrast, Days + ( Days| Subject ) has both random intercept and random slope for Days. Note that, random effects do not generate specific slopes for each level of Days, but rather just a variance of all the slopes.</p>
<p>Therefore, we can see that &quot;Days + ( Days| Subject )&quot; and &quot;Days + ( 1+Days| Subject )&quot; generate the same results. For more discussion, you can refer to the following link: <a href="https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r" class="uri">https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(sleepstudy, <span class="dt">package=</span><span class="st">&#39;lme4&#39;</span>)
<span class="kw">attach</span>(sleepstudy)</code></pre></div>
<pre><code>## The following objects are masked from sleepstudy (pos = 4):
## 
##     Days, Reaction, Subject</code></pre>
<pre><code>## The following objects are masked from sleepstudy (pos = 8):
## 
##     Days, Reaction, Subject</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm1 &lt;-<span class="st"> </span><span class="kw">lmer</span>(Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Subject), sleepstudy)
<span class="kw">summary</span>(fm1)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Reaction ~ Days + (1 | Subject)
##    Data: sleepstudy
## 
## REML criterion at convergence: 1786.5
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.2257 -0.5529  0.0109  0.5188  4.2506 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Subject  (Intercept) 1378.2   37.12   
##  Residual              960.5   30.99   
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 251.4051     9.7467   25.79
## Days         10.4673     0.8042   13.02
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.371</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm2&lt;-<span class="kw">lmer</span> ( Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>( Days<span class="op">|</span><span class="st"> </span>Subject ) , <span class="dt">data=</span> sleepstudy )
<span class="kw">summary</span>(fm2)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Reaction ~ Days + (Days | Subject)
##    Data: sleepstudy
## 
## REML criterion at convergence: 1743.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.9536 -0.4634  0.0231  0.4633  5.1793 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  Subject  (Intercept) 611.90   24.737       
##           Days         35.08    5.923   0.07
##  Residual             654.94   25.592       
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  251.405      6.824  36.843
## Days          10.467      1.546   6.771
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.138</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm3&lt;-<span class="kw">lmer</span> ( Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">+</span>Days<span class="op">|</span><span class="st"> </span>Subject ) , <span class="dt">data=</span> sleepstudy )
<span class="kw">summary</span>(fm3)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Reaction ~ Days + (1 + Days | Subject)
##    Data: sleepstudy
## 
## REML criterion at convergence: 1743.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.9536 -0.4634  0.0231  0.4633  5.1793 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  Subject  (Intercept) 611.90   24.737       
##           Days         35.08    5.923   0.07
##  Residual             654.94   25.592       
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  251.405      6.824  36.843
## Days          10.467      1.546   6.771
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.138</code></pre>
</div>
<div id="serial-correlations-in-time-and-space" class="section level2">
<h2><span class="header-section-number">3.5</span> Serial correlations in time and space</h2>
<p>The hierarchical model of <span class="math inline">\(y|x, u = x&#39;\beta+z&#39;u+\epsilon\)</span> can work well for correlations within blocks, but not for correlations in time as the correlations decay in time. The following uses nlme package to calculate time serial data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nlme)
<span class="kw">head</span>(nlme<span class="op">::</span>Ovary,<span class="dt">n=</span><span class="dv">50</span>)</code></pre></div>
<pre><code>## Grouped Data: follicles ~ Time | Mare
##    Mare        Time follicles
## 1     1 -0.13636360        20
## 2     1 -0.09090910        15
## 3     1 -0.04545455        19
## 4     1  0.00000000        16
## 5     1  0.04545455        13
## 6     1  0.09090910        10
## 7     1  0.13636360        12
## 8     1  0.18181820        14
## 9     1  0.22727270        13
## 10    1  0.27272730        20
## 11    1  0.31818180        22
## 12    1  0.36363640        15
## 13    1  0.40909090        18
## 14    1  0.45454550        17
## 15    1  0.50000000        14
## 16    1  0.54545450        18
## 17    1  0.59090910        14
## 18    1  0.63636360        16
## 19    1  0.68181820        17
## 20    1  0.72727270        18
## 21    1  0.77272730        18
## 22    1  0.81818180        17
## 23    1  0.86363640        14
## 24    1  0.90909090        12
## 25    1  0.95454550        12
## 26    1  1.00000000        14
## 27    1  1.04545500        10
## 28    1  1.09090900        11
## 29    1  1.13636400        16
## 30    2 -0.15000000         6
## 31    2 -0.10000000         6
## 32    2 -0.05000000         8
## 33    2  0.00000000         7
## 34    2  0.05000000        16
## 35    2  0.10000000        10
## 36    2  0.15000000        13
## 37    2  0.20000000         9
## 38    2  0.25000000         7
## 39    2  0.30000000         6
## 40    2  0.35000000         8
## 41    2  0.40000000         8
## 42    2  0.45000000         6
## 43    2  0.50000000         8
## 44    2  0.55000000         7
## 45    2  0.60000000         9
## 46    2  0.65000000         6
## 47    2  0.70000000         4
## 48    2  0.75000000         5
## 49    2  0.80000000         8
## 50    2  0.85000000        11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm1Ovar.lme &lt;-<span class="st"> </span>nlme<span class="op">::</span><span class="kw">lme</span>(<span class="dt">fixed=</span>follicles <span class="op">~</span><span class="st"> </span><span class="kw">sin</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>Time) <span class="op">+</span><span class="st"> </span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>Time), 
                   <span class="dt">data =</span> Ovary, 
                   <span class="dt">random =</span> <span class="kw">pdDiag</span>(<span class="op">~</span><span class="kw">sin</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>Time)), 
                   <span class="dt">correlation=</span><span class="kw">corAR1</span>() )
<span class="kw">summary</span>(fm1Ovar.lme)</code></pre></div>
<pre><code>## Linear mixed-effects model fit by REML
##  Data: Ovary 
##        AIC     BIC   logLik
##   1563.448 1589.49 -774.724
## 
## Random effects:
##  Formula: ~sin(2 * pi * Time) | Mare
##  Structure: Diagonal
##         (Intercept) sin(2 * pi * Time) Residual
## StdDev:    2.858385           1.257977 3.507053
## 
## Correlation Structure: AR(1)
##  Formula: ~1 | Mare 
##  Parameter estimate(s):
##       Phi 
## 0.5721866 
## Fixed effects: follicles ~ sin(2 * pi * Time) + cos(2 * pi * Time) 
##                        Value Std.Error  DF   t-value p-value
## (Intercept)        12.188089 0.9436602 295 12.915760  0.0000
## sin(2 * pi * Time) -2.985297 0.6055968 295 -4.929513  0.0000
## cos(2 * pi * Time) -0.877762 0.4777821 295 -1.837159  0.0672
##  Correlation: 
##                    (Intr) s(*p*T
## sin(2 * pi * Time)  0.000       
## cos(2 * pi * Time) -0.123  0.000
## 
## Standardized Within-Group Residuals:
##         Min          Q1         Med          Q3         Max 
## -2.34910093 -0.58969626 -0.04577893  0.52931186  3.37167486 
## 
## Number of Observations: 308
## Number of Groups: 11</code></pre>
<!--chapter:end:07-lmm.rmd-->
</div>
</div>
<div id="basic-stat-concepts" class="section level1">
<h1><span class="header-section-number">4</span> Basic Stat Concepts</h1>
<div id="score" class="section level2">
<h2><span class="header-section-number">4.1</span> Score</h2>
<p>The score is the gradient (the vector of partial derivatives) of <span class="math inline">\(log L(\theta)\)</span>, with respect to an m-dimensional parameter vector <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[S(\theta) = \frac{\partial\ell}{\partial \theta}\]</span> Typically, they use <span class="math inline">\(\nabla\)</span> to denote the partical derivative.</p>
<p><span class="math display">\[\nabla \ell\]</span></p>
<p>Such differentiation will generate a <span class="math inline">\(m \times 1\)</span> row vector, which indicates the sensitivity of the likelihood.</p>
<p>Quote from Steffen Lauritzen's slides: &quot;Generally the solution to this equation must be calculated by iterative methods. One of the most common methods is the Newton–Raphson method and this is based on successive approximations to the solution, using Taylor’s theorem to approximate the equation.&quot;</p>
<p>For instance, using logit link, we can get the first derivative of log likelihood logistic regression as follows. We can not really find <span class="math inline">\(\beta\)</span> easily to make the equation to be 0.</p>
<p><span class="math display">\[\begin{aligned}
\frac{\partial \ell} {\partial \beta} 
&amp;= \sum_{i=1}^{n}x_i^T[y_i-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}] \\
&amp;=\sum_{i=1}^{n} x_i^T[y_i-\hat{y_i}]
\end{aligned}\]</span></p>
</div>
<div id="gradient-and-jacobian" class="section level2">
<h2><span class="header-section-number">4.2</span> Gradient and Jacobian</h2>
<p><strong>Remarks</strong>: This part discusses gradient in a more general sense.</p>
<p>When <span class="math inline">\(f(x)\)</span> is only in a single dimension space:</p>
<p><span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}\)</span></p>
<p><span class="math display">\[\nabla f(x)=[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n}]\]</span> When <span class="math inline">\(f(x)\)</span> is only in a m-dimension space (i.e., Jacobian): <span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R^m}\)</span></p>
<p><span class="math display">\[Jac(f)=\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_1}{\partial x_3} &amp; ... &amp; \frac{\partial f_1}{\partial x_n}\\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_3} &amp; ... &amp; \frac{\partial f_2}{\partial x_n} \\
...\\
\frac{\partial f_m}{\partial x_1} &amp; \frac{\partial f_m}{\partial x_2} &amp; \frac{\partial f_n}{\partial x_3} &amp; ... &amp; \frac{\partial f_m}{\partial x_n}
\end{bmatrix}\]</span></p>
<p>For instance,</p>
<p><span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}\)</span>:</p>
<p><span class="math display">\[f(x,y)=x^2+2y\]</span> <span class="math display">\[\nabla f(x,y)=[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]=[2x,2]\]</span> <span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R^m}\)</span></p>
<p><span class="math display">\[f(x,y)=(x^2+2y,x^3)\]</span> <span class="math display">\[Jac(f)=\begin{bmatrix}
2x &amp; 2\\
2x^2 &amp; 0 
\end{bmatrix}\]</span></p>
</div>
<div id="hessian-and-fisher-information" class="section level2">
<h2><span class="header-section-number">4.3</span> Hessian and Fisher Information</h2>
<p>Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field.</p>
<p><span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}\)</span></p>
<p><span class="math display">\[Hessian=\nabla ^2(f) =\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_3} &amp; ... &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \frac{\partial^2 f}{\partial x_2 \partial x_3} &amp; ... &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\frac{\partial^2 f}{\partial x_3 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_3 \partial x_2} &amp; \frac{\partial^2 f}{\partial x_3^2} &amp; ... &amp; \frac{\partial^2 f}{\partial x_3 \partial x_n} \\
...\\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \frac{\partial^2 f}{\partial x_n \partial x_3} &amp; ... &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}\]</span></p>
<p>As a special case, in the context of logit:</p>
<p>Suppose that the log likelihood function is <span class="math inline">\(\ell (\theta)\)</span>. <span class="math inline">\(\theta\)</span> is a <span class="math inline">\(m\)</span> demension vector.</p>
<p><span class="math display">\[ \theta = \begin{bmatrix}\theta_1 \\
\theta_2 \\
\theta_3 \\
\theta_4 \\
...\\
\theta_m \\
\end{bmatrix}\]</span></p>
<p><span class="math display">\[Hessian=\nabla ^2(\ell) =\begin{bmatrix}
\frac{\partial^2 \ell}{\partial \theta_1^2} &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_2} &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_3} &amp; ... &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_m}\\
\frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_1} &amp; \frac{\partial^2 \ell}{\partial \theta_2^2 } &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_3} &amp; ... &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_m} \\
\frac{\partial^2 \ell}{\partial \theta_3 \partial \theta_1} &amp; \frac{\partial^2 \ell}{\partial \theta_3 \theta_2 } &amp; \frac{\partial^2 \ell}{\partial \theta_3^2} &amp; ... &amp; \frac{\partial^2 \ell}{\partial \theta_3 \partial \theta_m} \\
...\\
\frac{\partial^2 \ell}{\partial \theta_m \partial \theta_1} &amp; \frac{\partial^2 \ell}{\partial \theta_m \theta_2 } &amp; \frac{\partial^2 \ell}{\partial \theta_m \partial \theta_3} &amp; ... &amp; \frac{\partial^2 \ell}{\partial \theta_m \partial \theta_m} 
\end{bmatrix}\]</span></p>
<p>&quot;In statistics, the observed information, or observed Fisher information, is the negative of the second derivative (the Hessian matrix) of the &quot;log-likelihood&quot; (the logarithm of the likelihood function). It is a sample-based version of the Fisher information.&quot; (Direct quote from Wikipedia.)</p>
<p>Thus, the observed information matrix:</p>
<p><span class="math display">\[-Hessian=-\nabla ^2(\ell) \]</span></p>
<p>Expected (Fisher) information matrix:</p>
<p><span class="math display">\[E[-\nabla ^2(\ell)] \]</span></p>
</div>
<div id="canonical-link-function" class="section level2">
<h2><span class="header-section-number">4.4</span> Canonical link function</h2>
<p>Inspired by a Stack Exchange post, I created the following figure:</p>
<p><span class="math display">\[ \frac{Paramter}{\theta} \longrightarrow \gamma^{&#39;}(\theta) = \mu \longrightarrow \frac{Mean}{\mu} \longrightarrow g(\mu) = \eta \longrightarrow \frac{ Linear predictor}{\eta} \]</span></p>
<p>For the case of <span class="math inline">\(n\)</span> time Bernoulli (i.e., Binomial), its canonical link function is logit. Specifically,</p>
<p><span class="math display">\[ \frac{Paramter}{\theta=\beta^Tx_i}  \longrightarrow \gamma^{&#39;}(\theta)= \frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}\longrightarrow \frac{Mean}{\mu=\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}}\longrightarrow g(\mu) = log \frac{\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}}{1-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}}\longrightarrow \frac{ Linear predictor}{\eta = \beta^Tx_i}\]</span> Thus, we can see that,</p>
<p><span class="math display">\[\theta \equiv \eta \]</span> The link function <span class="math inline">\(g(\mu)\)</span> relates the linear predictor <span class="math inline">\(\eta = \beta^Tx_i\)</span> to the mean <span class="math inline">\(\mu\)</span>.</p>
<p><strong>Remarks</strong>:</p>
<ol style="list-style-type: decimal">
<li><p>Parameter is <span class="math inline">\(\theta = \beta ^T x_i\)</span> (Not <span class="math inline">\(\mu\)</span>!).</p></li>
<li><p><span class="math inline">\(\mu=p(y=1)=\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}\)</span> (Not logit!).</p></li>
<li><p>Link function (i.e., <span class="math inline">\(g(\mu)\)</span>) = logit = logarithm of odds = log <span class="math inline">\(\frac{Event - Happened }{Event - Not - Happened}\)</span>.</p></li>
<li><p><span class="math inline">\(g(\mu) = log \frac{\mu}{1-\mu}=\beta^T x_i\)</span>. Thus, link function = linear predictor = log odds!</p></li>
<li><p>Quote from the Stack Exchange post &quot;Newton Method and Fisher scoring for finding the ML estimator coincide, these links simplify the derivation of the MLE.&quot;</p></li>
</ol>
<p>(Recall, we know that <span class="math inline">\(\mu\)</span> or <span class="math inline">\(p(y=1)\)</span> is the mean function. Recall that, <span class="math inline">\(n\)</span> trails of coin flips, and get <span class="math inline">\(p\)</span> heads. Thus <span class="math inline">\(\mu = \frac{p}{n}\)</span>.)</p>
</div>
<div id="ordinary-least-squares-ols" class="section level2">
<h2><span class="header-section-number">4.5</span> Ordinary Least Squares (OLS)</h2>
<p>Suppose we have <span class="math inline">\(n\)</span> observation, and <span class="math inline">\(m\)</span> variables.</p>
<p><span class="math display">\[\begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1m}\\
x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2m} \\
...\\
x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nm}
\end{bmatrix}\]</span></p>
<p>Thus, we can write it as the following <span class="math inline">\(n\)</span> equations.</p>
<p><span class="math display">\[y_1=\beta_0+\beta_1 x_{11}+\beta_2 x_{12}+...+ \beta_m x_{1m}\]</span> <span class="math display">\[y_2=\beta_0+\beta_1 x_{21}+\beta_2 x_{22}+...+ \beta_m x_{2m}\]</span> <span class="math display">\[y_3=\beta_0+\beta_1 x_{31}+\beta_2 x_{32}+...+ \beta_m x_{3m}\]</span> <span class="math display">\[...\]</span></p>
<p><span class="math display">\[y_n=\beta_0+\beta_1 x_{n1}+\beta_2 x_{n2}+...+ \beta_m x_{nm}\]</span></p>
<p>We can combine all the <span class="math inline">\(n\)</span> equations as the following one:</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+...+ \beta_m x_{im}  (i \in [1,n])\]</span></p>
<p>We can further rewrite it as a matrix format as follows.</p>
<p><span class="math display">\[y= X \beta\]</span> Where,</p>
<p><span class="math display">\[y = \begin{bmatrix}y_1 \\
y_2 \\
y_3 \\
y_4 \\
...\\
y_n \\
\end{bmatrix}\]</span></p>
<p><span class="math display">\[X=\begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1m}\\
1 &amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2m} \\
...\\
1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nm}
\end{bmatrix}\]</span></p>
<p><span class="math display">\[\beta = \begin{bmatrix}\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3 \\
...\\
\beta_m \\
\end{bmatrix}\]</span></p>
<p>Since later we need the inverse of <span class="math inline">\(X\)</span>, we need to make it into a square matrix.</p>
<p><span class="math display">\[X^Ty=X^TX \hat{\beta} \Rightarrow \hat{\beta} = (X^TX)^{-1} X^Ty\]</span></p>
<p>We can use R to implement this calculation. As we can see, there is no need to do any iterations at all, but rather just pure matrix calculation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X&lt;-<span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1000</span>),<span class="dt">ncol=</span><span class="dv">2</span>) <span class="co"># we define a 2 column matrix, with 500 rows</span>
X&lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,X) <span class="co"># add a 1 constant</span>
beta_true&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>) <span class="co"># True regression coefficients</span>
beta_true&lt;-<span class="kw">as.matrix</span>(beta_true)
y=X<span class="op">%*%</span>beta_true<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">500</span>)

transposed_X&lt;-<span class="kw">t</span>(X)
beta_hat&lt;-<span class="kw">solve</span>(transposed_X<span class="op">%*%</span>X)<span class="op">%*%</span>transposed_X<span class="op">%*%</span>y
beta_hat</code></pre></div>
<pre><code>##          [,1]
## [1,] 2.017690
## [2,] 1.054682
## [3,] 2.037671</code></pre>
<p><strong>Side Notes</strong> The function of as.matrix will automatically make c(2,1,2) become the dimension of <span class="math inline">\(3 \times 1\)</span>, you do not need to transpose the <span class="math inline">\(\beta\)</span>.</p>
</div>
<div id="taylor-series" class="section level2">
<h2><span class="header-section-number">4.6</span> Taylor series</h2>
<p><span class="math display">\[\begin{aligned}
f(x)|_{a} &amp;=f(a)+\frac{f^{&#39;}(a)}{1!}(x-a)+\frac{f^{&#39;}(a)}{2!}(x-a)^2+\frac{f^{&#39;&#39;}(a)}{3!}(x-a)^{3}+...\\&amp;=\sum_{n=0}^{\infty} \frac{f^{n}(a)}{n!}(x-a)^n 
\end{aligned}\]</span></p>
<p>For example:</p>
<p><span class="math display">\[\begin{aligned} 
e^x |_{a=0} &amp;= e^a+ \frac{e^a}{1!}(x-a)+\frac{e^a}{2!}(x-a)^2+...+\frac{e^a}{n!}(x-a)^n \\ 
&amp;=  1+ \frac{1}{1!}x+\frac{1}{2!}x^2+...+\frac{1}{n!}x^n
\end{aligned}\]</span></p>
<p>if <span class="math inline">\(x=2\)</span></p>
<p><span class="math inline">\(e^2 = 7.389056\)</span></p>
<p><span class="math inline">\(e^2 \approx 1+\frac{1}{1!}x =1+\frac{1}{1!}2=3\)</span></p>
<p><span class="math inline">\(e^2 \approx 1+\frac{1}{1!}x+\frac{1}{2!}x^2 =1+\frac{1}{1!}2 + \frac{1}{2!}2 =5\)</span> ...</p>
<p><span class="math inline">\(e^2 \approx 1+\frac{1}{1!}x+\frac{1}{2!}x^2 +\frac{1}{3!}x^2+\frac{1}{4!}x^2+\frac{1}{5!}x^2=7.2666...\)</span></p>
</div>
<div id="fisher-scoring" class="section level2">
<h2><span class="header-section-number">4.7</span> Fisher scoring</h2>
<p>[I will come back to this later.]</p>
<p><a href="https://www2.stat.duke.edu/courses/Fall00/sta216/handouts/diagnostics.pdf" class="uri">https://www2.stat.duke.edu/courses/Fall00/sta216/handouts/diagnostics.pdf</a></p>
<p><a href="https://stats.stackexchange.com/questions/176351/implement-fisher-scoring-for-linear-regression" class="uri">https://stats.stackexchange.com/questions/176351/implement-fisher-scoring-for-linear-regression</a></p>
</div>
<div id="references-1" class="section level2">
<h2><span class="header-section-number">4.8</span> References</h2>
<ol style="list-style-type: decimal">
<li>Steffen Lauritzen's slides:</li>
</ol>
<p><a href="http://www.stats.ox.ac.uk/~steffen/teaching/bs2HT9/scoring.pdf" class="uri">http://www.stats.ox.ac.uk/~steffen/teaching/bs2HT9/scoring.pdf</a></p>
<ol start="2" style="list-style-type: decimal">
<li>The Stack Exchange post:</li>
</ol>
<p><a href="https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function" class="uri">https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function</a></p>
<ol start="3" style="list-style-type: decimal">
<li>Wilipedia for OLS</li>
</ol>
<p><a href="https://en.wikipedia.org/wiki/Ordinary_least_squares" class="uri">https://en.wikipedia.org/wiki/Ordinary_least_squares</a></p>
<ol start="4" style="list-style-type: decimal">
<li>Gradient and Jacobian</li>
</ol>
<p><a href="https://math.stackexchange.com/questions/1519367/difference-between-gradient-and-jacobian" class="uri">https://math.stackexchange.com/questions/1519367/difference-between-gradient-and-jacobian</a></p>
<p><a href="https://www.youtube.com/watch?v=3xVMVT-2_t4" class="uri">https://www.youtube.com/watch?v=3xVMVT-2_t4</a></p>
<p><a href="https://math.stackexchange.com/questions/661195/what-is-the-difference-between-the-gradient-and-the-directional-derivative" class="uri">https://math.stackexchange.com/questions/661195/what-is-the-difference-between-the-gradient-and-the-directional-derivative</a></p>
<ol start="5" style="list-style-type: decimal">
<li>Hessian</li>
</ol>
<p><a href="https://en.wikipedia.org/wiki/Hessian_matrix" class="uri">https://en.wikipedia.org/wiki/Hessian_matrix</a></p>
<ol start="6" style="list-style-type: decimal">
<li>Observed information</li>
</ol>
<p><a href="https://en.wikipedia.org/wiki/Observed_information" class="uri">https://en.wikipedia.org/wiki/Observed_information</a></p>
<ol start="7" style="list-style-type: decimal">
<li>Fisher information</li>
</ol>
<p><a href="https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf">https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf</a></p>
<ol start="8" style="list-style-type: decimal">
<li>Link function</li>
</ol>
<p><a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function" class="uri">https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function</a></p>
<p><a href="https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function" class="uri">https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function</a></p>
<!--chapter:end:081-BasicStat.Rmd-->
</div>
</div>
<div id="basic-r" class="section level1">
<h1><span class="header-section-number">5</span> Basic R</h1>
<p>This section is about R coding.</p>
<div id="apply-lapply-sapply" class="section level2">
<h2><span class="header-section-number">5.1</span> apply, lapply, sapply</h2>
<div id="apply" class="section level3">
<h3><span class="header-section-number">5.1.1</span> apply</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_trying &lt;-<span class="st"> </span><span class="kw">matrix</span>(C&lt;-(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>),<span class="dt">nrow=</span><span class="dv">2</span>, <span class="dt">ncol=</span><span class="dv">5</span>)
m_trying</code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    3    5    7    9
## [2,]    2    4    6    8   10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Operating on the columns
<span class="kw">apply</span>(m_trying, <span class="dv">2</span>, sum)</code></pre></div>
<pre><code>## [1]  3  7 11 15 19</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Operating on the rows
<span class="kw">apply</span>(m_trying, <span class="dv">1</span>, sum)</code></pre></div>
<pre><code>## [1] 25 30</code></pre>
</div>
<div id="lapply" class="section level3">
<h3><span class="header-section-number">5.1.2</span> lapply</h3>
<p>&quot;lapply returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.&quot;</p>
<p>lapply operates on lists. Thus, as we can see below, even if m_trying is not a list, each cell becomes a list.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results1&lt;-<span class="kw">lapply</span>(m_trying,sum)
<span class="kw">str</span>(results1)</code></pre></div>
<pre><code>## List of 10
##  $ : int 1
##  $ : int 2
##  $ : int 3
##  $ : int 4
##  $ : int 5
##  $ : int 6
##  $ : int 7
##  $ : int 8
##  $ : int 9
##  $ : int 10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">is.list</span>(results1)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="sapply" class="section level3">
<h3><span class="header-section-number">5.1.3</span> sapply</h3>
<p>&quot;sapply() function takes list, vector or data frame as input and gives output in vector or matrix.&quot;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results2&lt;-<span class="kw">sapply</span>(m_trying, sum)
<span class="kw">str</span>(results2)</code></pre></div>
<pre><code>##  int [1:10] 1 2 3 4 5 6 7 8 9 10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">is.list</span>(results2)</code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">is.matrix</span>(results2)</code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">is.data.frame</span>(results2)</code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">is.vector</span>(results2)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
</div>
<div id="c" class="section level2">
<h2><span class="header-section-number">5.2</span> C</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mydata1&lt;-<span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dv">4</span><span class="op">*</span><span class="dv">2</span>),<span class="dv">4</span>,<span class="dv">2</span>)
mydata1</code></pre></div>
<pre><code>##           [,1]      [,2]
## [1,] 0.7767640 0.3839558
## [2,] 0.8404593 0.9506320
## [3,] 0.8705815 0.7041046
## [4,] 0.9530419 0.4219814</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(mydata1)</code></pre></div>
<pre><code>##  num [1:4, 1:2] 0.777 0.84 0.871 0.953 0.384 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mydata2&lt;-<span class="kw">c</span>(mydata1)
mydata2</code></pre></div>
<pre><code>## [1] 0.7767640 0.8404593 0.8705815 0.9530419 0.3839558 0.9506320 0.7041046
## [8] 0.4219814</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(mydata2)</code></pre></div>
<pre><code>##  num [1:8] 0.777 0.84 0.871 0.953 0.384 ...</code></pre>
<!--chapter:end:082-BasicR.Rmd-->
</div>
</div>
<div id="computing-techniques" class="section level1">
<h1><span class="header-section-number">6</span> Computing Techniques</h1>
<p>Since GLMM can use EM algorithm in its maximum likelihood calculation (see McCulloch, 1994), it is practically useful to rehearse EM and other computing techniques.</p>
<div id="monte-carlo-approximation" class="section level2">
<h2><span class="header-section-number">6.1</span> Monte carlo approximation</h2>
<p>Example: calculate the integral of <span class="math inline">\(p(z&gt;2)\)</span> when <span class="math inline">\(z \sim N(0,1)\)</span>. To use Monte Carlo approximation, we can have an indicator function, which will determine whether the sample from <span class="math inline">\(N(0,1)\)</span> will be included into the calculation of the integral.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Nsim=<span class="dv">10</span><span class="op">^</span><span class="dv">4</span>

indicator=<span class="cf">function</span>(x){
y=<span class="kw">ifelse</span>((x<span class="op">&gt;</span><span class="dv">2</span>),<span class="dv">1</span>,<span class="dv">0</span>)
<span class="kw">return</span>(y)}

newdata&lt;-<span class="kw">rnorm</span>(Nsim, <span class="dv">0</span>,<span class="dv">1</span> )

mc=<span class="kw">c</span>(); v=<span class="kw">c</span>(); upper=<span class="kw">c</span>(); lower=<span class="kw">c</span>()

<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>Nsim)
{
mc[j]=<span class="kw">mean</span>(<span class="kw">indicator</span>(newdata[<span class="dv">1</span><span class="op">:</span>j]))
v[j]=(j<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>})<span class="op">*</span><span class="kw">var</span>(<span class="kw">indicator</span>(newdata[<span class="dv">1</span><span class="op">:</span>j]))
upper[j]=mc[j]<span class="op">+</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sqrt</span>(v[j])
lower[j]=mc[j]<span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sqrt</span>(v[j])
}

<span class="kw">library</span>(ggplot2)
values=<span class="kw">c</span>(mc,upper,lower)
type=<span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;mc&quot;</span>,Nsim),<span class="kw">rep</span>(<span class="st">&quot;upper&quot;</span>,Nsim),<span class="kw">rep</span>(<span class="st">&quot;lower&quot;</span>,Nsim))
iter=<span class="kw">rep</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span>Nsim),<span class="dv">3</span>)
data=<span class="kw">data.frame</span>(<span class="dt">val=</span>values, <span class="dt">tp=</span>type, <span class="dt">itr=</span>iter)
Rcode&lt;-<span class="kw">ggplot</span>(data,<span class="kw">aes</span>(itr,val,<span class="dt">col=</span>tp))<span class="op">+</span><span class="kw">geom_line</span>(<span class="dt">size=</span><span class="fl">0.5</span>)
Rcode<span class="op">+</span><span class="kw">geom_hline</span>(<span class="dt">yintercept=</span><span class="dv">1</span><span class="op">-</span><span class="kw">pnorm</span>(<span class="dv">2</span>),<span class="dt">color=</span><span class="st">&quot;green&quot;</span>,<span class="dt">size=</span><span class="fl">0.5</span>)</code></pre></div>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-21-1.png" /><!-- --></p>
</div>
<div id="importance-sampling" class="section level2">
<h2><span class="header-section-number">6.2</span> Importance sampling</h2>
<p>Importance sampling has samples generated from a different distribution than the distribution of interest. Specifically, assume that we want to calculate the expected value of <span class="math inline">\(h(x)\)</span>, and <span class="math inline">\(x \sim f(x)\)</span>.</p>
<p><span class="math display">\[E(h(x))=\int h(x) f(x) dx = \int h(x) \frac{f(x)}{g(x)} g(x) dx \]</span> We can sample <span class="math inline">\(x_i\)</span> from <span class="math inline">\(g(x)\)</span> and then calculate the mean of <span class="math inline">\(h(x_i) \frac{f(x_i)}{g(x_i)}\)</span>.</p>
<p>Using the same explane above, we can use a shifted exponential distribution to help calculate the intergral for normal distribution. Specifically,</p>
<p><span class="math display">\[\int_2^{\infty} \frac{1}{2 \pi} e^{-\frac{1}{2}x^2}dx = \int_2^{\infty} \frac{\frac{1}{2 \pi} e^{-\frac{1}{2}x^2}}{e^{-(x-2)}} e^{-(x-2)}dx \]</span> The idea is that, we can generate <span class="math inline">\(x_i\)</span> from exponential distribution of <span class="math inline">\(e^{-(x-2)}\)</span>, and then insert them into the targeted &quot;expected (value) function&quot; of <span class="math inline">\(\frac{\frac{1}{2 \pi} e^{-\frac{1}{2}x^2}}{e^{-(x-2)}}\)</span>. Thus, as you can see, importance sampling is based on the law of large numbers (i.e., If the same experiment or study is repeated independently a large number of times, the average of the results of the trials must be close to the expected value). We can use it to calculate integral based on link of the definition of expected value.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Nsim=<span class="dv">10</span><span class="op">^</span><span class="dv">4</span>
normal_density=<span class="cf">function</span>(x)
{y=(<span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span><span class="op">*</span>pi))<span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>(x<span class="op">^</span><span class="dv">2</span>))
<span class="kw">return</span>(y)}
x=<span class="dv">2</span><span class="op">-</span><span class="kw">log</span>(<span class="kw">runif</span>(Nsim))
ImpS=<span class="kw">c</span>(); v=<span class="kw">c</span>(); upper=<span class="kw">c</span>(); lower=<span class="kw">c</span>()
<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>Nsim)
{
ImpS[j]=<span class="kw">mean</span>(<span class="kw">normal_density</span>(x[<span class="dv">1</span><span class="op">:</span>j])<span class="op">/</span><span class="kw">exp</span>(<span class="op">-</span>(x[<span class="dv">1</span><span class="op">:</span>j]<span class="op">-</span><span class="dv">2</span>)))
v[j]=(j<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>})<span class="op">*</span><span class="kw">var</span>(<span class="kw">normal_density</span>(x[<span class="dv">1</span><span class="op">:</span>j])<span class="op">/</span><span class="kw">exp</span>(<span class="op">-</span>(x[<span class="dv">1</span><span class="op">:</span>j]<span class="op">-</span><span class="dv">2</span>)))
upper[j]=ImpS[j]<span class="op">+</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sqrt</span>(v[j])
lower[j]=ImpS[j]<span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sqrt</span>(v[j])
}

<span class="kw">library</span>(ggplot2)
values=<span class="kw">c</span>(ImpS,upper,lower)
type=<span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;mc&quot;</span>,Nsim),<span class="kw">rep</span>(<span class="st">&quot;upper&quot;</span>,Nsim),<span class="kw">rep</span>(<span class="st">&quot;lower&quot;</span>,Nsim))
iter=<span class="kw">rep</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span>Nsim),<span class="dv">3</span>)
data=<span class="kw">data.frame</span>(<span class="dt">val=</span>values, <span class="dt">tp=</span>type, <span class="dt">itr=</span>iter)
<span class="kw">ggplot</span>(data,<span class="kw">aes</span>(itr,val,<span class="dt">col=</span>tp))<span class="op">+</span><span class="kw">geom_line</span>(<span class="dt">size=</span><span class="fl">0.5</span>)<span class="op">+</span>
<span class="kw">geom_hline</span>(<span class="dt">yintercept=</span><span class="dv">1</span><span class="op">-</span><span class="kw">pnorm</span>(<span class="dv">2</span>),<span class="dt">color=</span><span class="st">&quot;green&quot;</span>,<span class="dt">size=</span><span class="fl">0.5</span>)</code></pre></div>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-22-1.png" /><!-- --></p>
</div>
<div id="newton-raphson-algorithm" class="section level2">
<h2><span class="header-section-number">6.3</span> Newton Raphson algorithm</h2>
<p>The main purpose of Newton Raphson algorithm is to calculate the root of a function (e.g., <span class="math inline">\(x^2-3=0\)</span>). We know that in order to maximize the MLE, we need to calculate the first derivatice of the function and then set it to zero <span class="math inline">\(\ell^{&#39;}(x)=0\)</span>. Thus, we can use the same Newton Raphson method to help calculate the MLE maximization as well.</p>
<p>There are different ways to understand Newton Raphson method, but I found the method fo geometric the most easy way to explain.</p>
<div class="figure">
<img src="Newton.jpg" alt="Credit of this figure: https://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf" />
<p class="caption">Credit of this figure: <a href="https://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf" class="uri">https://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf</a></p>
</div>
<p>Specifically, suppose that you want to calculate the root of a function <span class="math inline">\(f(x)=0\)</span>. We assume the root is <span class="math inline">\(r\)</span>. However, we do not that, and we randomly guess a point of <span class="math inline">\(a\)</span>. Thus, we can get a tangent line with slope of <span class="math inline">\(f^{&#39;}(a)\)</span> and a point of <span class="math inline">\((a,f(a))\)</span>. Since we know the slope and one of its points, we can write the function for this tangent line.</p>
<p><span class="math display">\[y-f(a)=f^{&#39;}(a)(x-a)\]</span> To calculate the <span class="math inline">\(x-intercept\)</span>, namely <span class="math inline">\(b\)</span> in the figure, we can set <span class="math inline">\(y=0\)</span>, and get the following:</p>
<p><span class="math display">\[-f(a)=f^{&#39;}(a)(x-a) \Rightarrow x (or, b)= a-\frac{f(a)}{f^{&#39;}(a)}\]</span> If there is significant difference of <span class="math inline">\(|a-b|\)</span>, we know that our orginal guess of <span class="math inline">\(a\)</span> is not good. We better use <span class="math inline">\(b\)</span> as the next guess, and calculate its tangent line again. To generalize, we can write it as follows. <span class="math display">\[x_{t+1}=x_{t}-\frac{f(x_t)}{f^{&#39;}(x_t)}\]</span></p>
<p>Okay, this method above is to calculate the root. For MLE, we can also use this method to calculate the root for the <span class="math inline">\(\ell ^{&#39;}=0\)</span>. We can write it as follows.</p>
<p><span class="math display">\[x_{t+1}=x_{t}-\frac{\ell^{&#39;}(x_t)}{\ell^{&#39;&#39;}(x_t)}\]</span> Often, <span class="math inline">\(x\)</span> is not just a single unknow parameter, but a vector. For this case, we can write it as follows.</p>
<p><span class="math display">\[\beta_{t+1}=\beta_{t}-\frac{\ell^{&#39;}(\beta_t)}{\ell^{&#39;&#39;}(\beta_t)}\]</span></p>
<div id="calculate-the-root" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Calculate the root</h3>
<p><span class="math inline">\(x^3-5=0\)</span></p>
<p>Note that, this is obviously not a maximization problem. In contrast, it involves a function with zero. As we can see, we can think it as the first order of Taylor approximation. That is, <span class="math inline">\(f^{&#39;}(x)=x^3-5=0\)</span>. As we can see the following plot, it converts very quickly.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f_firstorder=<span class="cf">function</span>(x){x<span class="op">^</span><span class="dv">3</span><span class="op">-</span><span class="dv">5</span>}
f_secondorder=<span class="cf">function</span>(x){<span class="dv">3</span><span class="op">*</span>x}
x_old=<span class="dv">1</span>;tolerance=<span class="fl">1e-3</span>
max_its=<span class="dv">2000</span>;iteration=<span class="dv">1</span>;difference=<span class="dv">2</span>
c_iteration&lt;-<span class="kw">c</span>() ## to collect numbers generated in the iteration process 
<span class="cf">while</span>(difference<span class="op">&gt;</span>tolerance <span class="op">&amp;</span><span class="st"> </span>iteration<span class="op">&lt;</span>max_its){
  x_updated=x_old<span class="op">-</span>(<span class="kw">f_firstorder</span>(x_old)<span class="op">/</span><span class="kw">f_secondorder</span>(x_old))
  difference=<span class="kw">abs</span>(x_updated<span class="op">-</span>x_old);
  iteration=iteration<span class="op">+</span><span class="dv">1</span>;
  x_old=x_updated
  c_iteration&lt;-<span class="kw">c</span>(c_iteration,x_updated)}

<span class="kw">plot</span>(c_iteration,<span class="dt">type=</span><span class="st">&quot;b&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-23-1.png" /><!-- --></p>
</div>
<div id="logistic-regression" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Logistic regression</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> observation, and <span class="math inline">\(m\)</span> variables.</p>
<p><span class="math display">\[\begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1m}\\
x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2m} \\
...\\
x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nm}
\end{bmatrix}\]</span></p>
<p>Typically, we add a vector of <span class="math inline">\(1\)</span> being used to estimate the constant.</p>
<p><span class="math display">\[\begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1m}\\
1 &amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2m} \\
...\\
1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nm}
\end{bmatrix}\]</span></p>
<p>And, we have observe a vector of <span class="math inline">\(n\)</span> <span class="math inline">\(y_i\)</span> as well, which is a binary variable:</p>
<p><span class="math display">\[Y = \begin{bmatrix}1 \\
0 \\
1 \\
0 \\
0 \\
0 \\
...\\
1 \\
\end{bmatrix}\]</span></p>
<p>Using the content from the MLE chapter, we can get:</p>
<p><span class="math display">\[\mathbf{L}=\prod_{i=1}^{n} p_i^{ y_i}(1-p_i)^{(1-y_i)}\]</span></p>
<p>Further, we can get a log-transformed format.</p>
<p><span class="math display">\[log (\mathbf{L})=\sum_{i=1}^{n}[y_i log (p_i) + (1-y_i) log(1-p_i)]\]</span> Given that <span class="math inline">\(p_i=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}=\frac{e^{\beta^Tx}}{1+e^{\beta^Tx}}\)</span>, we can rewrite it as follows:</p>
<p><span class="math display">\[log (\mathbf{L})=\ell=\sum_{i=1}^{n}[y_i log (\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}) + (1-y_i) log(1-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}})]\]</span> Before doing the derivative, we set. <span class="math display">\[\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}} = p(\beta ^T x_i)\]</span></p>
<p><span class="math display">\[log (\mathbf{L})=\ell=\sum_{i=1}^{n}[y_i log (p(\beta ^T x_i)) + (1-y_i) log(1-p(\beta ^T x_i))]\]</span></p>
<p>Note that, <span class="math inline">\(\frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)} = p(\beta ^T x_i)(1-p(\beta ^T x_i))\)</span>. We will use it later.</p>
<p><span class="math display">\[\begin{aligned}
\nabla \ell &amp;= \sum_{i=1}^{n} [y_i \frac{1}{p(\beta ^T x_i)} \frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)}\frac{\partial (\beta ^T x_i)}{\partial \beta}+(1-y_i) \frac{1}{1-p(\beta ^T x_i)}(-1)\frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)}\frac{\partial (\beta ^T x_i)}{\partial \beta}] \\
&amp;= \sum_{i=1}^{n} x_i^T[y_i \frac{1}{p(\beta ^T x_i)} p(\beta ^T x_i)(1-p(\beta ^T x_i))+(1-y_i) \frac{1}{1-p(\beta ^T x_i)}(-1)p(\beta ^T x_i)(1-p(\beta ^T x_i))] \\
&amp;= \sum_{i=1}^{n} x_i^T[y_i \frac{1}{p(\beta ^T x_i)} p(\beta ^T x_i)(1-p(\beta ^T x_i))-(1-y_i) \frac{1}{1-p(\beta ^T x_i)}p(\beta ^T x_i)(1-p(\beta ^T x_i))] \\
&amp;= \sum_{i=1}^{n} x_i^T[y_i (1-p(\beta ^T x_i))-(1-y_i) p(\beta ^T x_i)] \\
&amp;=\sum_{i=1}^{n} x_i^T[y_i-y_ip(\beta ^T x_i)-p(\beta ^T x_i)+y_i p(\beta ^T x_i)] \\
&amp;=\sum_{i=1}^{n} x_i^T[y_i-p(\beta ^T x_i)] \\
&amp;= \sum_{i=1}^{n} x_i^T[y_i-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}]
\end{aligned}\]</span></p>
<p>As noted, the Newton Raphson algorithm needs the second order.</p>
<p><span class="math display">\[\begin{aligned}
\nabla^2 \ell &amp;=\frac{\partial \sum_{i=1}^{n} x_i^T[y_i-p(\beta ^T x_i)]}{\partial \beta} \\
&amp;=-\sum_{i=1}^{n} x_i^T\frac{\partial p(\beta ^T x_i) }{\partial \beta}\\
&amp;=-\sum_{i=1}^{n} x_i^T\frac{\partial p(\beta ^T x_i) }{\partial (\beta^Tx_i)} \frac{\partial (\beta^Tx_i)}{\partial \beta}\\
&amp;=-\sum_{i=1}^{n} x_i^T p(\beta ^T x_i)(1-p(\beta ^T x_i))x_i
\end{aligned}\]</span></p>
<p>The following are the data simulation (3 IVs and 1 DV) and Newton Raphson analysis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Data generation</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
n=<span class="dv">500</span>
x1_norm&lt;-<span class="kw">rnorm</span>(n)
x2_norm&lt;-<span class="kw">rnorm</span>(n,<span class="dv">3</span>,<span class="dv">4</span>)
x3_norm&lt;-<span class="kw">rnorm</span>(n,<span class="dv">4</span>,<span class="dv">6</span>)
x_combined&lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,x1_norm,x2_norm,x3_norm) <span class="co"># dimension: n*4</span>
coefficients_new&lt;-<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)  <span class="co">#true regression coefficient</span>
inv_logit&lt;-<span class="cf">function</span>(x,b){<span class="kw">exp</span>(x<span class="op">%*%</span>b)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x<span class="op">%*%</span>b))}
prob_generated&lt;-<span class="kw">inv_logit</span>(x_combined,coefficients_new)
y&lt;-<span class="kw">c</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {y[i]&lt;-<span class="kw">rbinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,prob_generated[i])}

<span class="co"># Newton Raphson</span>

<span class="co">#We need to set random starting values.</span>
beta_old&lt;-<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)
tolerance=<span class="fl">1e-3</span>
max_its=<span class="dv">2000</span>;iteration=<span class="dv">1</span>;difference=<span class="dv">2</span>
W&lt;-<span class="kw">matrix</span>(<span class="dv">0</span>,n,n)

<span class="cf">while</span>(difference<span class="op">&gt;</span>tolerance <span class="op">&amp;</span><span class="st"> </span>iteration<span class="op">&lt;</span>max_its)
  {
  <span class="co"># The first order</span>
  f_firstorder&lt;-<span class="kw">t</span>(x_combined)<span class="op">%*%</span>(y<span class="op">-</span><span class="kw">inv_logit</span>(x_combined,beta_old))
  <span class="co"># The second order</span>
  <span class="kw">diag</span>(W) =<span class="st"> </span><span class="kw">inv_logit</span>(x_combined,beta_old)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">inv_logit</span>(x_combined,beta_old))
  f_secondorder&lt;-<span class="op">-</span><span class="kw">t</span>(x_combined)<span class="op">%*%</span>W<span class="op">%*%</span>x_combined
  <span class="co"># Calculate the beta_updated</span>
  beta_updated=beta_old<span class="op">-</span>(<span class="kw">solve</span>(f_secondorder)<span class="op">%*%</span>f_firstorder)
  difference=<span class="kw">max</span>(<span class="kw">abs</span>(beta_updated<span class="op">-</span>beta_old));
  iteration=iteration<span class="op">+</span><span class="dv">1</span>;
  beta_old=beta_updated}

beta_old</code></pre></div>
<pre><code>##              [,1]
##         0.9590207
## x1_norm 1.7974165
## x2_norm 3.0072303
## x3_norm 3.9578107</code></pre>
<p><span class="math display">\[\frac{\partial \ell} {\partial \beta} = \sum_{i=1}^{n} [y_i \frac{1}{p(\beta ^T x_i)} \frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)}\frac{\partial (\beta ^T x_i)}{\partial \beta}+(1-y_i) \frac{1}{1-p(\beta ^T x_i)}(-1)\frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)}\frac{\partial (\beta ^T x_i)}{\partial \beta}] \]</span> <span class="math display">\[=\sum_{i=1}^{n} [y_i \frac{1}{p(\beta ^T x_i)} \phi (\beta ^T x_i)-(1-y_i) \frac{1}{1-p(\beta ^T x_i)}\phi (\beta ^T x_i)]x_i\]</span></p>
<p><span class="math display">\[\Phi(\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3)= p(y=1)\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Data generation</span>
n=<span class="dv">500</span>
x1_norm&lt;-<span class="kw">rnorm</span>(n)
x2_norm&lt;-<span class="kw">rnorm</span>(n)
x3_norm&lt;-<span class="kw">rnorm</span>(n)
x_combined&lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,x1_norm,x2_norm,x3_norm)
coefficients_new&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>)  <span class="co">#true regression coefficient</span>
inv_norm&lt;-<span class="cf">function</span>(x,b){<span class="kw">pnorm</span>(x<span class="op">%*%</span>b)}
prob_generated&lt;-<span class="kw">inv_norm</span>(x_combined,coefficients_new)
y&lt;-<span class="kw">c</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {y[i]&lt;-<span class="kw">rbinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,prob_generated[i])}

<span class="co"># Newton Raphson</span>

<span class="co">#We need to set random starting values.</span>
x_old&lt;-<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)
tolerance=<span class="fl">1e-3</span>
max_its=<span class="dv">2000</span>;iteration=<span class="dv">1</span>;difference=<span class="dv">2</span>

<span class="cf">while</span>(difference<span class="op">&gt;</span>tolerance <span class="op">&amp;</span><span class="st"> </span>iteration<span class="op">&lt;</span>max_its){
  x_updated=x_old<span class="op">-</span>(<span class="kw">f_firstorder</span>(x_old)<span class="op">/</span><span class="kw">f_secondorder</span>(x_old))
  difference=<span class="kw">abs</span>(x_updated<span class="op">-</span>x_old);
  iteration=iteration<span class="op">+</span><span class="dv">1</span>;
  x_old=x_updated
  c_iteration&lt;-<span class="kw">c</span>(c_iteration,x_updated)}

<span class="kw">plot</span>(c_iteration,<span class="dt">type=</span><span class="st">&quot;b&quot;</span>)</code></pre></div>
</div>
</div>
<div id="metropolis-hastings" class="section level2">
<h2><span class="header-section-number">6.4</span> Metropolis Hastings</h2>
<p>Metropolis–Hastings is a MCMC method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. By using the samples, we can plot the distribution (through histgram), or we can calculate the integral (e.g., you need to calculate the expected value).</p>
<p>(Side note: does this remind you the importance sampling? Very similiar!)</p>
<p>Basic logic (my own summary):</p>
<ol style="list-style-type: decimal">
<li><p>Set up a random starting value of <span class="math inline">\(x_0\)</span>.</p></li>
<li><p>Sample a <span class="math inline">\(y_0\)</span> from the instrumental function of <span class="math inline">\(q(x)\)</span>.</p></li>
<li><p>Calculate the following:</p></li>
</ol>
<p><span class="math inline">\(p =\frac{f(y_0)}{f(x_0)}\frac{q(x_0)}{q(y_0)}\)</span></p>
<ol start="4" style="list-style-type: decimal">
<li><p><span class="math inline">\(\rho=min(p, 1)\)</span></p></li>
<li><p><span class="math inline">\(x_{1}=\begin{cases} y_0 &amp; p \\ x_0 &amp; 1-p \end{cases}\)</span></p></li>
<li><p>Repeat <span class="math inline">\(n\)</span> times (<span class="math inline">\(n\)</span> is set subjectively.)</p></li>
</ol>
<p>Use normal pdf to sample gamma distribution</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha=<span class="fl">2.7</span>; beta=<span class="fl">6.3</span> <span class="co"># I randomly chose alpha and beta values for the target gamma function</span>

Nsim=<span class="dv">5000</span>  ## define the number of iteration 

X=<span class="kw">c</span>(<span class="kw">rgamma</span>(<span class="dv">1</span>,<span class="dv">1</span>)) <span class="co"># initialize the chain from random starting numbers</span>
mygamma&lt;-<span class="cf">function</span>(Nsim,alpha,beta){
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>Nsim){
  Y=<span class="kw">rnorm</span>(<span class="dv">1</span>)
  rho=<span class="kw">dgamma</span>(Y,alpha,beta)<span class="op">*</span><span class="kw">dnorm</span>(X[i<span class="op">-</span><span class="dv">1</span>])<span class="op">/</span>(<span class="kw">dgamma</span>(X[i<span class="op">-</span><span class="dv">1</span>],alpha,beta)<span class="op">*</span><span class="kw">dnorm</span>(Y))
  X[i]=X[i<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>(Y<span class="op">-</span>X[i<span class="op">-</span><span class="dv">1</span>])<span class="op">*</span>(<span class="kw">runif</span>(<span class="dv">1</span>)<span class="op">&lt;</span>rho)
}
X
}

<span class="kw">hist</span>(<span class="kw">mygamma</span>(Nsim,alpha,beta), <span class="dt">breaks =</span> <span class="dv">100</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-26-1.png" /><!-- --></p>
</div>
<div id="em" class="section level2">
<h2><span class="header-section-number">6.5</span> EM</h2>
<p>EM algorithm is an iterative method to find ML or maximum a posteriori (MAP) estimates of parameters.</p>
<p>Direct Ref: <a href="http://www.di.fc.ul.pt/~jpn/r/EM/EM.html" class="uri">http://www.di.fc.ul.pt/~jpn/r/EM/EM.html</a></p>
<p>Suppose that we only observe <span class="math inline">\(X\)</span>, and do not know <span class="math inline">\(Z\)</span>. We thus need to construct the posterior <span class="math inline">\(p(Z|X,\theta)\)</span>. Given <span class="math inline">\(p(Z|X,\theta)\)</span>, we can compute the likelihood of the complete dataset:</p>
<p><span class="math display">\[p(X, Z|\theta)=p(Z|X,\theta)p(X|\theta)\]</span> The EM algorithm:</p>
<ol start="0" style="list-style-type: decimal">
<li><p>We got <span class="math inline">\(X\)</span> and <span class="math inline">\(p(Z|X,\theta)\)</span></p></li>
<li><p>Random assign a <span class="math inline">\(\theta_0\)</span>, since we do not know any of them.</p></li>
<li><p>E-step: <span class="math inline">\(Q_{\theta_i} = E_{Z|X,\theta_i}[log p(X,Z|\theta)]\)</span></p></li>
<li><p>M-step: compute <span class="math inline">\(\theta_{i+1} \leftarrow argmax Q_{\theta_i}\)</span></p></li>
<li><p>If <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(\theta_{i+1}\)</span> are not close enough, <span class="math inline">\(\theta_i \leftarrow \theta_{i+1}\)</span>. Goto step 2.</p></li>
</ol>
<p>For examples, you can refer to the following link: <a href="http://www.di.fc.ul.pt/~jpn/r/EM/EM.html" class="uri">http://www.di.fc.ul.pt/~jpn/r/EM/EM.html</a></p>
<p>(It is em_R.r in R_codes folder. Personally, I can also refer to Quiz 2 in 536.)</p>
</div>
<div id="references-2" class="section level2">
<h2><span class="header-section-number">6.6</span> References</h2>
<ol style="list-style-type: decimal">
<li>The UBC PDF about Newton</li>
</ol>
<p><a href="https://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf" class="uri">https://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf</a></p>
<ol start="2" style="list-style-type: decimal">
<li>Some other pages about Newton and logistic regression</li>
</ol>
<p><a href="http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/" class="uri">http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/</a></p>
<p><a href="https://stats.stackexchange.com/questions/344309/why-using-newtons-method-for-logistic-regression-optimization-is-called-iterati" class="uri">https://stats.stackexchange.com/questions/344309/why-using-newtons-method-for-logistic-regression-optimization-is-called-iterati</a></p>
<p><a href="https://tomroth.com.au/logistic/" class="uri">https://tomroth.com.au/logistic/</a></p>
<p><a href="https://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf" class="uri">https://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf</a></p>
<p><a href="https://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf" class="uri">https://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf</a></p>
<p><a href="http://hua-zhou.github.io/teaching/biostatm280-2017spring/slides/18-newton/newton.html" class="uri">http://hua-zhou.github.io/teaching/biostatm280-2017spring/slides/18-newton/newton.html</a></p>
<ol start="3" style="list-style-type: decimal">
<li>MH</li>
</ol>
<p><a href="https://www.youtube.com/watch?v=VGRVRjr0vyw" class="uri">https://www.youtube.com/watch?v=VGRVRjr0vyw</a></p>
<!--chapter:end:09-computingtechnique.Rmd-->
</div>
</div>
<div id="generalized-linear-mixed-models" class="section level1">
<h1><span class="header-section-number">7</span> Generalized Linear Mixed Models</h1>
<div id="basics-of-glmm" class="section level2">
<h2><span class="header-section-number">7.1</span> Basics of GLMM</h2>
<p>Recall the formula in the probit model:</p>
<p><span class="math display">\[Y^*=X\beta+\epsilon, \epsilon \sim N(0,\sigma^2)=N(0,I)\]</span> Similar to LMM, binary model with random effect can be written as follows.</p>
<p><span class="math display">\[Y^*=X\beta+ Z u+\epsilon\]</span> where,</p>
<p><span class="math display">\[\epsilon \sim N(0,I)\]</span> <span class="math display">\[u \sim N(0, D)\]</span></p>
<p>We also assume <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(u\)</span> are independent.Thus, we know that <span class="math inline">\(D\)</span> represents the virances of the random effects. If we make <span class="math inline">\(u =1\)</span>, the model becomes the usual probit model. McCulloch (1994) states that there are a few advantages to use probit, rather than logit models. (Note that, however, probit is not canonical link function, but logit is!)</p>
<p>The following is the note from Charle E. McCulloch's &quot;Maximum likelihood algorithems for Generalized Linear Mixed Models&quot;</p>
</div>
<div id="some-references" class="section level2">
<h2><span class="header-section-number">7.2</span> Some References</h2>
<p><a href="http://www.biostat.umn.edu/~baolin/teaching/linmods/glmm.html" class="uri">http://www.biostat.umn.edu/~baolin/teaching/linmods/glmm.html</a></p>
<p><a href="http://www.biostat.umn.edu/~baolin/teaching/probmods/GLMM_mcmc.html" class="uri">http://www.biostat.umn.edu/~baolin/teaching/probmods/GLMM_mcmc.html</a></p>
<p><a href="https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html" class="uri">https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html</a></p>
<!--chapter:end:10-glmm.rmd-->
</div>
</div>
<div id="twitter-example" class="section level1">
<h1><span class="header-section-number">8</span> Twitter Example</h1>
<p>The following is part of my course project for Stat 536. It aims to replicate part of the findings from Barbera (2015) Birds of the Same Feather Tweet Together: Bayesian Ideal Point Estimation Using Twitter Data. Political Analysis 23 (1). Note that, the following model is much simpler than that in the original paper.</p>
<div id="model" class="section level2">
<h2><span class="header-section-number">8.1</span> Model</h2>
<p>Suppose that a Twitter user is presented with a choice between following or not following another target <span class="math inline">\(j \in \{ 1, ..., m\}\)</span>. Let <span class="math inline">\(y_{j}=1\)</span> if the user decides to follow <span class="math inline">\(j\)</span>, and <span class="math inline">\(y_{j}=0\)</span> otherwise.</p>
<p><span class="math display">\[y_{j}=\begin{cases} 1 &amp; Following \\ 0 &amp; Not Following \end{cases}\]</span></p>
<p><span class="math display">\[p(y_{j}=1|\theta) = \frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)}\]</span> We additionally know the priors of <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[\theta_i \sim N(0,10^2) (i = 0, 1)\]</span></p>
<p>The likelihood function is as follows.</p>
<p><span class="math display">\[L(Y|\theta)=\prod_{j=1}^{m} (\frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)})^{y_j}(1-\frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)})^{(1-y_j)}\]</span> Thus, the posterior is as follows.</p>
<p><span class="math display">\[L(Y|\theta) \cdot N(\theta_0|0,10) \cdot N(\theta_1|0,10)\]</span> <span class="math display">\[\propto \prod_{j=1}^{m} (\frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)})^{y_j}(1-\frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)})^{(1-y_j)}\cdot exp(-\frac{1}{2}(\frac{\theta_0}{10})^2)\cdot exp(-\frac{1}{2}(\frac{\theta_1}{10})^2)\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Establish the function for logistic regression</span>
Expit&lt;-<span class="cf">function</span>(x){<span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x))}

<span class="co">#Construct the posterior - in a log-format</span>
<span class="co">#To make sure that the estimate of theta_1 is stable, </span>
<span class="co">#the following code wants to make sure that theta_0 is always greater than zero.</span>

log_post&lt;-<span class="cf">function</span>(Y, X, theta)
  {
  <span class="cf">if</span>(theta[<span class="dv">1</span>]<span class="op">&lt;=</span><span class="dv">0</span>){post=<span class="op">-</span><span class="ot">Inf</span>}
  <span class="cf">if</span>(theta[<span class="dv">1</span>]<span class="op">&gt;</span><span class="dv">0</span>){
  prob1&lt;-<span class="kw">Expit</span>(<span class="op">-</span>theta[<span class="dv">1</span>]<span class="op">*</span>((theta[<span class="dv">2</span>]<span class="op">-</span>X)<span class="op">^</span><span class="dv">2</span>))
  likelihood&lt;-<span class="kw">sum</span>(<span class="kw">dbinom</span>(Y,<span class="dv">1</span>,prob1,<span class="dt">log =</span> <span class="ot">TRUE</span>))
  priors&lt;-<span class="kw">sum</span>(<span class="kw">dnorm</span>(theta,<span class="dv">0</span>,<span class="dv">10</span>,<span class="dt">log=</span><span class="ot">TRUE</span>))
  post=likelihood<span class="op">+</span>priors}
  <span class="kw">return</span>(post)
   }

Bayes_logit&lt;-<span class="cf">function</span> (Y,X,<span class="dt">n_samples=</span><span class="dv">2000</span>)
{
<span class="co">#Initial values</span>
  theta&lt;-<span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>)
<span class="co">#store data</span>
  keep.theta&lt;-<span class="kw">matrix</span>(<span class="dv">0</span>,n_samples,<span class="dv">2</span>)
  keep.theta[<span class="dv">1</span>,]&lt;-theta
  
<span class="co">#acceptance and rejection  </span>
  acc&lt;-att&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">2</span>)
<span class="co">#current log posterior</span>
  current_lp&lt;-<span class="kw">log_post</span>(Y,X,theta)

  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>n_samples)  
  {
    
    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)
    {
      <span class="co">#attempt + 1</span>
      att[j]&lt;-att[j]<span class="op">+</span><span class="dv">1</span>
      can_theta&lt;-theta
      can_theta[j]&lt;-<span class="kw">rnorm</span>(<span class="dv">1</span>,theta[j],<span class="fl">0.5</span>)
      <span class="co">#candidate of log posterior</span>
      candidate_lp&lt;-<span class="kw">log_post</span>(Y,X,can_theta)
      Rho&lt;-<span class="kw">min</span>(<span class="kw">exp</span>(candidate_lp<span class="op">-</span>current_lp),<span class="dv">1</span>)
      Random_probability&lt;-<span class="kw">runif</span>(<span class="dv">1</span>)
      <span class="cf">if</span> (Random_probability<span class="op">&lt;</span>Rho)
      {
        theta&lt;-can_theta
        current_lp&lt;-candidate_lp
        <span class="co">#acceptance + 1, as long as Random_probability&lt;Rho</span>
        acc[j]&lt;-acc[j]<span class="op">+</span><span class="dv">1</span>
      }
    }
    <span class="co">#save theta</span>
    keep.theta[i,]&lt;-theta
  }
<span class="co">#Return: including theta and acceptance rate</span>
  <span class="kw">list</span>(<span class="dt">theta=</span>keep.theta,<span class="dt">acceptance_rate=</span>acc<span class="op">/</span>att)
}</code></pre></div>
</div>
<div id="simulating-data-of-senators-on-twitter" class="section level2">
<h2><span class="header-section-number">8.2</span> Simulating Data of Senators on Twitter</h2>
<p>Assume that we have 100 senators, 50 Democrats and 50 Republicans, who we know their ideology. Assume that Democrats have negative ideology scores to indicate that they are more liberal, whereas Republicans have positive scores to indicate that they are more conservative. The following is data simulation for senators.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Republicans are more conservative, and they have positive numbers.</span>
Republicans&lt;-<span class="kw">c</span>()
Republicans&lt;-<span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dv">1</span>,<span class="fl">0.5</span>)
No_Republicans&lt;-<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>,<span class="dv">1</span>)
Part_<span class="dv">1</span>&lt;-<span class="kw">cbind</span>(No_Republicans,Republicans)

<span class="co"># Democrats are more liberal, and they have negative numbers.</span>
Democrats&lt;-<span class="kw">c</span>()
Democrats&lt;-<span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="op">-</span><span class="dv">1</span>,<span class="fl">0.5</span>)
No_Democrats&lt;-<span class="kw">rep</span>(<span class="dv">51</span><span class="op">:</span><span class="dv">100</span>,<span class="dv">1</span>)
Part_<span class="dv">2</span>&lt;-<span class="kw">cbind</span>(No_Democrats,Democrats)
Data_Elites&lt;-<span class="kw">rbind</span>(Part_<span class="dv">1</span>,Part_<span class="dv">2</span>)
Data_Elites&lt;-<span class="kw">as.data.frame</span>(Data_Elites)
<span class="kw">colnames</span>(Data_Elites) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Elite_No&quot;</span>,<span class="st">&quot;Elite_ideology&quot;</span>)

<span class="kw">head</span>(Data_Elites)</code></pre></div>
<pre><code>##   Elite_No Elite_ideology
## 1        1      1.0541992
## 2        2      0.3805544
## 3        3      1.3568577
## 4        4      0.9922547
## 5        5      1.0089966
## 6        6      0.8878271</code></pre>
</div>
<div id="simulating-data-of-conservative-users-on-twitter-and-model-testing" class="section level2">
<h2><span class="header-section-number">8.3</span> Simulating Data of Conservative Users on Twitter and Model Testing</h2>
<p>Assume that we observe one Twitter user, who is more conservative. To simulate Twitter following data for this user, I assign this user to follow more Republican senators. Thus, if the Metropolis Hastings algorithm works as intended, we would expect to see a positive estimated value for their ideology. Importantly, as we can see in the histogram below, the estimated value indeed is positive, providing preliminary evidence for the statistical model and the algorithm. In addition, for the acceptance rate, we can see that the constant has a lower number than ideology, since we only accept a constant when it is positive.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#This user approximately follows 45 Republican Senators and 10 Democrat Senators. </span>
Data_user&lt;-<span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">50</span>)<span class="op">&lt;</span>.<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">50</span>)<span class="op">&lt;</span>.<span class="dv">8</span>,<span class="dv">0</span>,<span class="dv">1</span>))), <span class="dv">100</span>, <span class="dv">1</span>)
<span class="kw">colnames</span>(Data_user)&lt;-<span class="kw">c</span>(<span class="st">&quot;R_User&quot;</span>)
Data_combined&lt;-<span class="kw">cbind</span>(Data_Elites,Data_user)

X_data&lt;-Data_combined<span class="op">$</span>Elite_ideology
Y_data&lt;-Data_combined<span class="op">$</span>R_User

fit_C&lt;-<span class="kw">Bayes_logit</span>(Y_data,X_data)
fit_C<span class="op">$</span>acceptance_rate</code></pre></div>
<pre><code>## [1] 0.1320660 0.5557779</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit_C<span class="op">$</span>theta[,<span class="dv">1</span>],<span class="dt">main=</span><span class="st">&quot;Constant (Conservative Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Iteration Process&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Estimated Scores&quot;</span>,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-29-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit_C<span class="op">$</span>theta[,<span class="dv">2</span>],<span class="dt">main=</span><span class="st">&quot;Estimated Ideology Scores (Conservative Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Iteration Process&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Ideology Scores&quot;</span>,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-29-2.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(fit_C<span class="op">$</span>theta[,<span class="dv">2</span>],<span class="dt">main=</span><span class="st">&quot;Estimated Ideology Scores (Conservative Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Ideology Scores&quot;</span>,<span class="dt">breaks =</span> <span class="dv">100</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-29-3.png" /><!-- --></p>
</div>
<div id="simulating-data-of-liberal-users-on-twitter-and-model-testing" class="section level2">
<h2><span class="header-section-number">8.4</span> Simulating Data of Liberal Users on Twitter and Model Testing</h2>
<p>To further verify the Metropolis Hastings algorithm, I plan to test the opposite estimate. Specifically, assume that we observe another user, who is more liberal. To simulate Twitter following data for this user, I assign this user to follow more Democrat senators. In this case, we would expect to see a negative value for their estimated ideology. As we can see in the histogram shown below, as expected, the estimated value is negative, providing convergent evidence for the model and the algorithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#This user approximately follows 10 Republican Senators and 45 Democrat Senators. </span>
Data_user&lt;-<span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">50</span>)<span class="op">&lt;</span>.<span class="dv">8</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">50</span>)<span class="op">&lt;</span>.<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>))), <span class="dv">100</span>, <span class="dv">1</span>)
<span class="kw">colnames</span>(Data_user)&lt;-<span class="kw">c</span>(<span class="st">&quot;L_User&quot;</span>)
Data_combined&lt;-<span class="kw">cbind</span>(Data_Elites,Data_user)

X_data&lt;-Data_combined<span class="op">$</span>Elite_ideology
Y_data&lt;-Data_combined<span class="op">$</span>L_User


fit_L&lt;-<span class="kw">Bayes_logit</span>(Y_data,X_data)
fit_L<span class="op">$</span>acceptance_rate</code></pre></div>
<pre><code>## [1] 0.1585793 0.5092546</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit_L<span class="op">$</span>theta[,<span class="dv">1</span>],<span class="dt">main=</span><span class="st">&quot;Constant (Liberal Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Iteration Process&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Estimated Scores&quot;</span>,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-30-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit_L<span class="op">$</span>theta[,<span class="dv">2</span>],<span class="dt">main=</span><span class="st">&quot;Estimated Ideology Scores (Liberal Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Iteration Process&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Ideology Scores&quot;</span>,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-30-2.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(fit_L<span class="op">$</span>theta[,<span class="dv">2</span>],<span class="dt">main=</span><span class="st">&quot;Estimated Ideology Scores (Liberal Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Ideology Scores&quot;</span>,<span class="dt">breaks =</span> <span class="dv">100</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-30-3.png" /><!-- --></p>
<!--chapter:end:20-mh-algorithm-and-data.Rmd-->
</div>
</div>
<div id="practice-learning-on-the-battle-field" class="section level1">
<h1><span class="header-section-number">9</span> Practice: Learning on the Battle Field</h1>
<div id="r-code" class="section level2">
<h2><span class="header-section-number">9.1</span> R code</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#https://fivethirtyeight.com/contributors/josh-hermsmeyer/</span>
<span class="co"># https://github.com/ryurko/nflscrapR-data/blob/master/legacy_data/README.md</span>

<span class="co">#mydata1 = read.csv(&#39;plays.txt&#39;)</span>
<span class="co">#unique(mydata1$gameId)</span>

<span class="co">#unique(mydata1$PassLength)</span>
<span class="co">#table(mydata1$PassLength)</span>
<span class="co">#table(mydata1$PassResult)</span>
<span class="co">#table(mydata1$numberOfPassRushers)</span>


##mydata3 = read.csv(url(&#39;https://raw.githubusercontent.com/ryurko/nflscrapR-data/master/legacy_data/season_play_by_play/pbp_2017.csv&#39;))
##write.csv(mydata3,&#39;2017playbyplay.csv&#39;)

mydata3&lt;-<span class="kw">read.csv</span>(<span class="st">&#39;2017playbyplay.csv&#39;</span>)
<span class="kw">nrow</span>(mydata3)
<span class="kw">table</span>(mydata3<span class="op">$</span>Passer)
<span class="kw">table</span>(mydata3<span class="op">$</span>PlayType)

<span class="co">#mydata5&lt;-mydata3[!duplicated(mydata3[,c(&#39;GameID&#39;,&#39;Passer&#39;)]),]</span>
<span class="co">#unique(mydata3$GameID)</span>
mydata6&lt;-<span class="kw">subset</span>(mydata3,down<span class="op">==</span><span class="dv">1</span>)


mydata7&lt;-<span class="kw">subset</span>(mydata6,PlayType<span class="op">==</span><span class="st">&#39;Pass&#39;</span><span class="op">|</span>PlayType<span class="op">==</span><span class="st">&#39;Run&#39;</span>)
<span class="co">#table(mydata7$PlayType)</span>
<span class="co">#table(droplevels(mydata7$PlayType))</span>

mydata7<span class="op">$</span>PlayType&lt;-<span class="kw">droplevels</span>(mydata7<span class="op">$</span>PlayType)
<span class="kw">table</span>(mydata7<span class="op">$</span>PlayType)

<span class="co">#http://rstudio-pubs-static.s3.amazonaws.com/6975_c4943349b6174f448104a5513fed59a9.html</span>
<span class="kw">source</span>(<span class="st">&quot;http://pcwww.liv.ac.uk/~william/R/crosstab.r&quot;</span>)
mydata8&lt;-mydata7[,<span class="kw">c</span>(<span class="st">&#39;Passer&#39;</span>,<span class="st">&#39;PlayType&#39;</span>,<span class="st">&#39;GameID&#39;</span>,<span class="st">&#39;posteam&#39;</span>,<span class="st">&#39;DefensiveTeam&#39;</span>,<span class="st">&#39;Yards.Gained&#39;</span>,<span class="st">&#39;FirstDown&#39;</span>,<span class="st">&#39;TimeSecs&#39;</span>)]
<span class="co">#results&lt;-crosstab(mydata8, row.vars = &quot;GameID&quot;, col.vars = &quot;PlayType&quot;, type = &quot;r&quot;)</span>
<span class="co">#p1&lt;-results$crosstab</span>
<span class="co">#hist(p1[,1],20)</span>



<span class="kw">library</span>(plyr)
count_vector&lt;-<span class="kw">count</span>(mydata8, <span class="st">&quot;GameID&quot;</span>)

l_new&lt;-<span class="kw">length</span>(count_vector<span class="op">$</span>freq)
time&lt;-<span class="kw">c</span>()
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>l_new)
{time&lt;-<span class="kw">append</span>(time,<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>count_vector<span class="op">$</span>freq[i]))}
<span class="kw">nrow</span>(time)
mydata8<span class="op">$</span>time&lt;-time
mydata8<span class="op">$</span>play_new&lt;-<span class="kw">ifelse</span>(mydata8<span class="op">$</span>PlayType<span class="op">==</span><span class="st">&#39;Pass&#39;</span>,<span class="dv">1</span>,<span class="dv">0</span>)

n_counting&lt;-<span class="dv">0</span>  <span class="co"># help counting the number of pairs</span>

## The following code collects all the rows of each pair. However, it is difficult to analyze data
<span class="co"># in such a format. </span>

<span class="co">#empty_df = mydata8[FALSE,]</span>
<span class="co">#for (i in 1:l_new) # level of different game</span>
<span class="co">#{</span>
<span class="co">#   for(j in 1:((count_vector$freq[i])-1)) # within the same game</span>
<span class="co">#   {</span>
<span class="co">#      if(i==1)</span>
<span class="co">#      {row_id&lt;-j}</span>
<span class="co">#      else {row_id&lt;-sum(count_vector$freq[1:(i-1)])+j}</span>
<span class="co">#</span>
<span class="co">#      #print(row_id)</span>
<span class="co">#      if(as.character(mydata8[row_id,]$posteam)!=as.character(mydata8[row_id+1,]$posteam))</span>
<span class="co">#      {</span>
<span class="co">#        print(&quot;not same team&quot;)</span>
<span class="co">#        if (nrow(empty_df)==0)</span>
<span class="co">#           {empty_df&lt;-mydata8[row_id:(row_id+1),]}</span>
<span class="co">#        else</span>
<span class="co">#           {</span>
<span class="co">#             if(row.names(mydata8[row_id,])!=row.names(tail(empty_df,1)))</span>
<span class="co">#               {empty_df&lt;-rbind(empty_df,mydata8[row_id,])}</span>
<span class="co">#             empty_df&lt;-rbind(empty_df,mydata8[row_id+1,])</span>
<span class="co">#           }</span>
<span class="co">#       n_counting&lt;-n_counting+1</span>
<span class="co">#      }</span>
<span class="co">#   }</span>
<span class="co">#}</span>


<span class="co"># The following code only collects the second row of the pair, but adds data of </span>
### PT_L: type of play in the last first down from the other team
### TG_L: Yards.Gained in the last play
### FirstDown: did they get first down or not. Note that, if yes, it means it was a fumble.

PT_L=<span class="st">&quot;Pass&quot;</span>
TG_L=<span class="dv">0</span>
FD_L=<span class="dv">0</span>

pari_data=<span class="st"> </span>mydata8[<span class="dv">1</span>,]
pari_data&lt;-<span class="kw">cbind</span>(pari_data,PT_L,TG_L,FD_L)
pari_data&lt;-pari_data[<span class="ot">FALSE</span>,]

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>l_new) <span class="co"># level of different game</span>
{
  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>((count_vector<span class="op">$</span>freq[i])<span class="op">-</span><span class="dv">1</span>)) <span class="co"># within the same game</span>
  {

    <span class="cf">if</span>(i<span class="op">==</span><span class="dv">1</span>)
    {row_id&lt;-j}
    <span class="cf">else</span> {row_id&lt;-<span class="kw">sum</span>(count_vector<span class="op">$</span>freq[<span class="dv">1</span><span class="op">:</span>(i<span class="op">-</span><span class="dv">1</span>)])<span class="op">+</span>j}

    <span class="kw">print</span>(row_id)
    <span class="cf">if</span>(<span class="kw">as.character</span>(mydata8[row_id,]<span class="op">$</span>posteam)<span class="op">!=</span><span class="kw">as.character</span>(mydata8[row_id<span class="op">+</span><span class="dv">1</span>,]<span class="op">$</span>posteam))
    {
      <span class="kw">print</span>(<span class="st">&quot;not same team&quot;</span>)
      PT_L&lt;-<span class="kw">as.character</span>(mydata8[row_id,]<span class="op">$</span>PlayType)
      TG_L&lt;-mydata8[row_id,]<span class="op">$</span>Yards.Gained
      FD_L&lt;-mydata8[row_id,]<span class="op">$</span>FirstDown

      new_row&lt;-<span class="kw">cbind</span>(mydata8[(row_id<span class="op">+</span><span class="dv">1</span>),],PT_L,TG_L,FD_L)
      pari_data&lt;-<span class="kw">rbind</span>(pari_data,new_row)
     }

      n_counting&lt;-n_counting<span class="op">+</span><span class="dv">1</span>
  }
}

pari_data<span class="op">$</span>same&lt;-<span class="kw">ifelse</span>(pari_data<span class="op">$</span>PlayType<span class="op">==</span>pari_data<span class="op">$</span>PT_L,<span class="dv">1</span>,<span class="dv">0</span>)

<span class="co">#write.csv(pari_data,&#39;pari_data.csv&#39;)</span>

<span class="kw">write.table</span>(pari_data, <span class="dt">file =</span> <span class="st">&quot;pari_data.csv&quot;</span>,<span class="dt">row.names=</span><span class="ot">FALSE</span>,<span class="dt">na =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>)</code></pre></div>
<p><strong>Remarks</strong></p>
<ol style="list-style-type: decimal">
<li><p>mylogit1: in general, a team has a different play in their first down, compared to the other team in the last first down.</p></li>
<li><p>mylogit2: If the defence team passed in the last first down, the offence team is less likely to use pass. If the defence team gained more yards, the offence team is more likely to pass in the next first down. If the defence team fumbled, it will reduce the chance the offence team to do the pass.</p></li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pari_data2&lt;-<span class="kw">read.csv</span>(<span class="st">&#39;pari_data.csv&#39;</span>)

mylogit1 =<span class="st"> </span><span class="kw">glm</span>(same<span class="op">~</span><span class="dv">1</span>, <span class="dt">family=</span>binomial, <span class="dt">data=</span>pari_data2)
<span class="kw">summary</span>(mylogit1)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = same ~ 1, family = binomial, data = pari_data2)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.117  -1.117  -1.117   1.239   1.239  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.14395    0.02809  -5.124    3e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7035.5  on 5093  degrees of freedom
## Residual deviance: 7035.5  on 5093  degrees of freedom
## AIC: 7037.5
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mylogit2 =<span class="st"> </span><span class="kw">glm</span>(play_new<span class="op">~</span>same<span class="op">+</span>TG_L<span class="op">+</span>FD_L, <span class="dt">family=</span>binomial, <span class="dt">data=</span>pari_data2)
<span class="kw">summary</span>(mylogit2)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = play_new ~ same + TG_L + FD_L, family = binomial, 
##     data = pari_data2)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6114  -0.9783  -0.9382   1.0995   1.5672  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.175629   0.040712   4.314  1.6e-05 ***
## same        -0.757822   0.057618 -13.152  &lt; 2e-16 ***
## TG_L         0.010439   0.003873   2.695  0.00704 ** 
## FD_L        -0.268115   0.148835  -1.801  0.07164 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7034.3  on 5093  degrees of freedom
## Residual deviance: 6850.1  on 5090  degrees of freedom
## AIC: 6858.1
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lme4)
mylogit3 =<span class="st"> </span><span class="kw">glmer</span>(same<span class="op">~</span>play_new<span class="op">+</span>TG_L<span class="op">+</span>FD_L<span class="op">+</span>(<span class="dv">1</span><span class="op">|</span>GameID), <span class="dt">family=</span> <span class="kw">binomial</span>(<span class="st">&quot;logit&quot;</span>), <span class="dt">data=</span>pari_data2)</code></pre></div>
<pre><code>## boundary (singular) fit: see ?isSingular</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mylogit3)</code></pre></div>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: same ~ play_new + TG_L + FD_L + (1 | GameID)
##    Data: pari_data2
## 
##      AIC      BIC   logLik deviance df.resid 
##   6862.4   6895.1  -3426.2   6852.4     5089 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -1.3918 -0.7763 -0.7532  0.9061  1.6255 
## 
## Random effects:
##  Groups Name        Variance  Std.Dev. 
##  GameID (Intercept) 1.562e-15 3.953e-08
## Number of obs: 5094, groups:  GameID, 256
## 
## Fixed effects:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.197140   0.040513   4.866 1.14e-06 ***
## play_new    -0.757838   0.057619 -13.153  &lt; 2e-16 ***
## TG_L         0.006027   0.003824   1.576  0.11502    
## FD_L        -0.392792   0.150715  -2.606  0.00916 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##          (Intr) ply_nw TG_L  
## play_new -0.627              
## TG_L     -0.270 -0.043       
## FD_L     -0.147  0.031 -0.041
## convergence code: 0
## boundary (singular) fit: see ?isSingular</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Bill_1&lt;- bild(play_new ~ TG_L+FD_L, data = mydata8, id=&quot;GameID&quot;,start = NULL, dependence = &quot;MC1R&quot;)</span>
<span class="co">#summary(Bill_1)</span>

<span class="co">#locust2 &lt;- bild(as.factor(PlayType) ~ time + I(time^2), data = mydata8,id=&quot;GameID&quot;,start = NULL, dependence = &quot;MC2&quot;)</span></code></pre></div>
</div>
<div id="references-3" class="section level2">
<h2><span class="header-section-number">9.2</span> References</h2>
<p><a href="https://arxiv.org/pdf/1403.7993.pdf" class="uri">https://arxiv.org/pdf/1403.7993.pdf</a></p>
<p><a href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf" class="uri">http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf</a></p>
<p><a href="https://rpubs.com/JanpuHou/326048" class="uri">https://rpubs.com/JanpuHou/326048</a></p>
<!--chapter:end:21-Football.Rmd-->
</div>
</div>
<div id="project-draft" class="section level1">
<h1><span class="header-section-number">10</span> Project Draft</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mydata3&lt;-<span class="kw">read.csv</span>(<span class="st">&#39;Schnibbe 1502 Binary Data.csv&#39;</span>)
<span class="kw">head</span>(mydata3)</code></pre></div>
<pre><code>##   X0
## 1  0
## 2  1
## 3  0
## 4  0
## 5  1
## 6  0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">NO_new&lt;-<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">222</span>)
mydata4&lt;-<span class="kw">cbind</span>(mydata3,NO_new)
<span class="kw">head</span>(mydata4)</code></pre></div>
<pre><code>##   X0 NO_new
## 1  0      1
## 2  1      2
## 3  0      3
## 4  0      4
## 5  1      5
## 6  0      6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a1 =<span class="st"> </span><span class="kw">glmer</span>(X0 <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>NO_new), <span class="dt">data =</span> mydata4,<span class="dt">family=</span>binomial)
<span class="kw">summary</span>(a1)</code></pre></div>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: X0 ~ 1 + (1 | NO_new)
##    Data: mydata4
## 
##      AIC      BIC   logLik deviance df.resid 
##    243.3    250.1   -119.6    239.3      220 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -0.5461 -0.5461 -0.5461 -0.5461  1.8311 
## 
## Random effects:
##  Groups Name        Variance  Std.Dev.
##  NO_new (Intercept) 1.246e-07 0.000353
## Number of obs: 222, groups:  NO_new, 222
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -1.2098     0.1603  -7.549 4.38e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a2 =<span class="st"> </span><span class="kw">glm</span>(X0 <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> mydata4,<span class="dt">family=</span>binomial)
<span class="kw">summary</span>(a2)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = X0 ~ 1, family = binomial, data = mydata4)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7225  -0.7225  -0.7225  -0.7225   1.7151  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -1.2098     0.1595  -7.583 3.38e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 239.29  on 221  degrees of freedom
## Residual deviance: 239.29  on 221  degrees of freedom
## AIC: 241.29
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div id="background" class="section level2">
<h2><span class="header-section-number">10.1</span> Background</h2>
<p>The following code is from this website: <a href="http://www.biostat.umn.edu/~baolin/teaching/probmods/GLMM_mcmc.html" class="uri">http://www.biostat.umn.edu/~baolin/teaching/probmods/GLMM_mcmc.html</a>. I will remove it on this page after I complete my practice and learning.</p>
<p>In this example, it simulates a longitudinal data with 4 variables for each of 1000 separate individuals. Specifically, there are three continuous covariates (varying over time) and one ordinal covariate (constant over time). We will consider a random intercept model (mean zero and variance 100), and fit the data with glmer() from lme4 R package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">1000</span>; p =<span class="st"> </span><span class="dv">3</span>; K =<span class="st"> </span><span class="dv">4</span>; sig =<span class="st"> </span><span class="dv">10</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

## time varying covariates
Xl =<span class="st"> </span><span class="kw">vector</span>(<span class="st">&#39;list&#39;</span>, K)
<span class="co"># 4 list, each 1000 individuals</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) Xl[[i]] =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>p), n,p)

## constant covariate
Z =<span class="st"> </span><span class="kw">rbinom</span>(n, <span class="dv">2</span>,<span class="fl">0.2</span>)

## random effects
<span class="co">#just 1000 random numubers?</span>
U =<span class="st"> </span><span class="kw">rnorm</span>(n)<span class="op">*</span>sig

## fixed effects
<span class="co"># It ends a 1000*4 matrix</span>
etaX =<span class="st"> </span><span class="kw">sapply</span>(Xl, rowSums)

## random errors
eps =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>K), n,K)

## logit model
eta =<span class="st"> </span>etaX <span class="op">+</span><span class="st"> </span>U <span class="op">+</span><span class="st"> </span>eps
<span class="co"># calculate probability</span>
prb =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>eta))
D =<span class="st"> </span><span class="dv">1</span><span class="op">*</span>(<span class="kw">matrix</span>(<span class="kw">runif</span>(n<span class="op">*</span>K),n,K)<span class="op">&lt;</span>prb) <span class="co"># comparing it to prb, and change to 1 and 0; 1000*4</span>
<span class="co"># Select the first list from &quot;Xl&quot;, and then add other 3 lists--&gt; 4000 * 3</span>
Xs =<span class="st"> </span>Xl[[<span class="dv">1</span>]]
<span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>K) Xs =<span class="st"> </span><span class="kw">rbind</span>(Xs, Xl[[k]])

## GLMM model
<span class="kw">library</span>(lme4)
sid =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n, K) <span class="co"># a vector of 1-1000, 4 repetitions</span>
## model fit with GLMMM (default to Laplace approximation)
<span class="co"># subjects as the random effect</span>
a1 =<span class="st"> </span><span class="kw">glmer</span>(<span class="kw">c</span>(D) <span class="op">~</span><span class="st"> </span>Xs <span class="op">+</span><span class="st"> </span>Z[sid] <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>sid), <span class="dt">family=</span>binomial)

a1</code></pre></div>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: c(D) ~ Xs + Z[sid] + (1 | sid)
##       AIC       BIC    logLik  deviance  df.resid 
##  3213.666  3251.430 -1600.833  3201.666      3994 
## Random effects:
##  Groups Name        Std.Dev.
##  sid    (Intercept) 5.816   
## Number of obs: 4000, groups:  sid, 1000
## Fixed Effects:
## (Intercept)          Xs1          Xs2          Xs3       Z[sid]  
##      0.1537       0.6650       0.6429       0.6074       0.0199</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## MH sampling of random effects | data
## logit\Pr(D_i|eta_i,U) = eta_i+U; U \sim N(0,Vu)
## proposal dist: N(Uc,Vc)

U.mh &lt;-<span class="st"> </span><span class="cf">function</span>(Di,eta, Vu, Uc,Vc, <span class="dt">B=</span><span class="dv">100</span>){
  ub =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, B)
  ub[<span class="dv">1</span>] =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>)<span class="op">*</span><span class="kw">sqrt</span>(Vc)<span class="op">+</span>Uc <span class="co"># random starting value</span>
  prb =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>eta<span class="op">-</span>ub[<span class="dv">1</span>]))
  llk0 =<span class="st"> </span><span class="kw">dnorm</span>(ub[<span class="dv">1</span>],<span class="dt">sd=</span><span class="kw">sqrt</span>(Vu), <span class="dt">log=</span><span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(Di<span class="op">*</span>prb<span class="op">+</span>(<span class="dv">1</span><span class="op">-</span>Di)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>prb))) <span class="op">-</span><span class="st"> </span><span class="kw">dnorm</span>(ub[<span class="dv">1</span>],Uc,<span class="kw">sqrt</span>(Vc), <span class="dt">log=</span><span class="ot">TRUE</span>) <span class="co"># likelihood function? </span>
  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>B){
    ub[k] =<span class="st"> </span>ub[k<span class="op">-</span><span class="dv">1</span>]
    uk =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>)<span class="op">*</span><span class="kw">sqrt</span>(Vc)<span class="op">+</span>Uc
    prb =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>eta<span class="op">-</span>uk))
    llk1 =<span class="st"> </span><span class="kw">dnorm</span>(uk,<span class="dt">sd=</span><span class="kw">sqrt</span>(Vu), <span class="dt">log=</span><span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(Di<span class="op">*</span>prb<span class="op">+</span>(<span class="dv">1</span><span class="op">-</span>Di)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>prb))) <span class="op">-</span><span class="st"> </span><span class="kw">dnorm</span>(uk,Uc,<span class="kw">sqrt</span>(Vc), <span class="dt">log=</span><span class="ot">TRUE</span>)
    alpha =<span class="st"> </span><span class="kw">exp</span>( llk1 <span class="op">-</span><span class="st"> </span>llk0  )
    <span class="cf">if</span>(alpha<span class="op">&gt;=</span><span class="dv">1</span>){
      ub[k] =<span class="st"> </span>uk
      llk0 =<span class="st"> </span>llk1
    } <span class="cf">else</span>{
      aa =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
      <span class="cf">if</span>(aa<span class="op">&lt;</span>alpha){
        ub[k] =<span class="st"> </span>uk
        llk0 =<span class="st"> </span>llk1
      }
    }
  }
  <span class="kw">return</span>(ub)
}

<span class="kw">library</span>(numDeriv)
UV.est &lt;-<span class="st"> </span><span class="cf">function</span>(Di,eta,Vu,Uc){
  llk0 =<span class="st"> </span><span class="cf">function</span>(xpar){
    Uc =<span class="st"> </span>xpar
    prb =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>eta<span class="op">-</span>Uc))
    res =<span class="st"> </span><span class="kw">dnorm</span>(Uc,<span class="dt">sd=</span><span class="kw">sqrt</span>(Vu), <span class="dt">log=</span><span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(Di<span class="op">*</span>prb<span class="op">+</span>(<span class="dv">1</span><span class="op">-</span>Di)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>prb)))
    <span class="op">-</span>res
  }
  tmp =<span class="st"> </span><span class="kw">try</span>(<span class="kw">optim</span>(Uc, llk0, <span class="dt">method=</span><span class="st">&#39;Brent&#39;</span>, <span class="dt">lower=</span>Uc<span class="op">-</span><span class="dv">10</span>,<span class="dt">upper=</span>Uc<span class="op">+</span><span class="dv">10</span>) )
  <span class="cf">if</span>(<span class="kw">class</span>(tmp)<span class="op">==</span><span class="st">&#39;try-error&#39;</span>) tmp =<span class="st"> </span><span class="kw">optim</span>(Uc, llk0)
  Uc =<span class="st"> </span>tmp<span class="op">$</span>par
  Vc =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="kw">hessian</span>(llk0, Uc)
  <span class="kw">c</span>(Uc,Vc)
}
UV.mh &lt;-<span class="st"> </span><span class="cf">function</span>(Vu,beta,Uc, D,X,subj){
  ## Cov matrix
  sid =<span class="st"> </span><span class="kw">unique</span>(subj);  n =<span class="st"> </span><span class="kw">length</span>(sid)
  Uc =<span class="st"> </span>Vc =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,n)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n){
    ij =<span class="st"> </span><span class="kw">which</span>(subj<span class="op">==</span>sid[i]);  ni =<span class="st"> </span><span class="kw">length</span>(ij)
    Xi =<span class="st"> </span>X[ij,,drop=<span class="ot">FALSE</span>]
    eta =<span class="st"> </span>Xi<span class="op">%*%</span>beta
    zi =<span class="st"> </span><span class="kw">UV.est</span>(D[ij],eta,Vu,Uc[i])
    Uc[i] =<span class="st"> </span>zi[<span class="dv">1</span>]; Vc[i] =<span class="st"> </span>zi[<span class="dv">2</span>]
  }
  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">Uc=</span>Uc,<span class="dt">Vc=</span>Vc) )
}

<span class="co">#Newton Raphson update</span>
<span class="co"># Compute first/second derives of complete data log likelihood</span>
## score and fisher information
SF.mh &lt;-<span class="st"> </span><span class="cf">function</span>(Vu,beta,Uc,Vc, D,X,subj){
  ## S/hessian matrix
  sid =<span class="st"> </span><span class="kw">unique</span>(subj);  n =<span class="st"> </span><span class="kw">length</span>(sid)
  p =<span class="st"> </span><span class="kw">dim</span>(X)[<span class="dv">2</span>]
  S =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, p)
  FI =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, p,p)
  sig2 =<span class="st"> </span><span class="dv">0</span>
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n)
    {
    ij =<span class="st"> </span><span class="kw">which</span>(subj<span class="op">==</span>sid[i]);  ni =<span class="st"> </span><span class="kw">length</span>(ij)
    Xi =<span class="st"> </span>X[ij,,drop=<span class="ot">FALSE</span>]
    eta =<span class="st"> </span>Xi<span class="op">%*%</span>beta
    zi =<span class="st"> </span><span class="kw">U.mh</span>(D[ij],eta,Vu,Uc[i],Vc[i], <span class="dt">B=</span><span class="fl">5e3</span>)[<span class="op">-</span>(<span class="dv">1</span><span class="op">:</span><span class="fl">1e3</span>)]
    theta =<span class="st"> </span><span class="kw">sapply</span>(eta, <span class="cf">function</span>(b0)  <span class="kw">mean</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>b0<span class="op">-</span>zi))) )
    theta2 =<span class="st"> </span><span class="kw">sapply</span>(eta, <span class="cf">function</span>(b0) <span class="kw">mean</span>(<span class="kw">exp</span>(b0<span class="op">+</span>zi)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(b0<span class="op">+</span>zi))<span class="op">^</span><span class="dv">2</span>) )
    FI =<span class="st"> </span>FI <span class="op">+</span><span class="st"> </span><span class="kw">t</span>(Xi)<span class="op">%*%</span>(theta2<span class="op">*</span>Xi)
    S =<span class="st"> </span>S<span class="op">+</span><span class="kw">colSums</span>((D[ij]<span class="op">-</span>theta)<span class="op">*</span>Xi)
    sig2 =<span class="st"> </span>sig2 <span class="op">+</span><span class="st"> </span><span class="kw">mean</span>(zi<span class="op">^</span><span class="dv">2</span>)
    }
  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">S=</span>S, <span class="dt">FI=</span>FI, <span class="dt">sig2=</span>sig2<span class="op">/</span>n) )
}

<span class="kw">library</span>(lme4)
sid =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n, K)
a1 =<span class="st"> </span><span class="kw">glmer</span>(<span class="kw">c</span>(D) <span class="op">~</span><span class="st"> </span>Xs <span class="op">+</span><span class="st"> </span>Z[sid] <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>sid), <span class="dt">family=</span>binomial)
## extract variance and fixed effects parameters; + mode/variance of (random effects|data)
Vu =<span class="st"> </span>(<span class="kw">getME</span>(a1,<span class="st">&#39;theta&#39;</span>))<span class="op">^</span><span class="dv">2</span>; beta =<span class="st"> </span><span class="kw">fixef</span>(a1); Um =<span class="st"> </span><span class="kw">ranef</span>(a1,<span class="dt">condVar=</span><span class="ot">TRUE</span>)
D =<span class="st"> </span><span class="kw">c</span>(D); X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>,Xs,Z[sid]); subj =<span class="st"> </span>sid
Uc =<span class="st"> </span><span class="kw">unlist</span>(Um[[<span class="dv">1</span>]]); Vc =<span class="st"> </span><span class="kw">c</span>( <span class="kw">attr</span>(Um[[<span class="dv">1</span>]], <span class="st">&#39;postVar&#39;</span>) )
<span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>){
  ## NR updates with MH sampling
  obj =<span class="st"> </span><span class="kw">SF.mh</span>(Vu,beta,Uc,Vc, D,X,subj)
  Vu =<span class="st"> </span>obj<span class="op">$</span>sig2
  tmp =<span class="st"> </span><span class="kw">solve</span>(obj<span class="op">$</span>FI,obj<span class="op">$</span>S)
  beta =<span class="st"> </span>beta <span class="op">+</span><span class="st"> </span>tmp
  ## Proposal dist update
  tmp1 =<span class="st"> </span><span class="kw">UV.mh</span>(Vu,beta,Uc, D,X,subj)
  Uc =<span class="st"> </span>tmp1<span class="op">$</span>Uc; Vc =<span class="st"> </span>tmp1<span class="op">$</span>Vc
  <span class="kw">cat</span>(b, <span class="st">&#39;:&#39;</span>, tmp, <span class="st">&#39;;&#39;</span>, obj<span class="op">$</span>S<span class="op">/</span>n, <span class="st">&#39;</span><span class="ch">\n\t</span><span class="st">&#39;</span>, <span class="kw">sqrt</span>(Vu), beta, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)
}</code></pre></div>
</div>
<div id="important-examples-with-r-code" class="section level2">
<h2><span class="header-section-number">10.2</span> Important Examples with R code</h2>
<ol style="list-style-type: decimal">
<li>Fitting mixed models with (temporal) correlations in R</li>
</ol>
<p><a href="https://bbolker.github.io/mixedmodels-misc/notes/corr_braindump.html" class="uri">https://bbolker.github.io/mixedmodels-misc/notes/corr_braindump.html</a></p>
<ol start="2" style="list-style-type: decimal">
<li>Mixed effects logistic regression</li>
</ol>
<p><a href="https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/" class="uri">https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/</a></p>
</div>
<div id="references-4" class="section level2">
<h2><span class="header-section-number">10.3</span> References</h2>
<ol style="list-style-type: decimal">
<li>Data</li>
</ol>
<p><a href="http://www.michelecoscia.com/?page_id=379" class="uri">http://www.michelecoscia.com/?page_id=379</a></p>
<!--chapter:end:30-projectdraft.Rmd-->
</div>
</div>
