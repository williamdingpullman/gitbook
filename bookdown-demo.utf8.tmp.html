<div id="basics" class="section level1">
<h1><span class="header-section-number">1</span> Basics</h1>
<div id="logit" class="section level2">
<h2><span class="header-section-number">1.1</span> Logit</h2>
<p><span class="math display">\[f(x)=log(\frac{p(y=1)}{1-p(y=1)})\]</span> The basic idea of logistic regression: <span class="math display">\[p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span> Thus, <span class="math inline">\(e^{\beta_0+\beta_1x_1+...+\beta_nx_n}\)</span> can be from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>, and <span class="math inline">\(p(y=1)\)</span> will be always within the range of <span class="math inline">\((0,1)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f&lt;-<span class="cf">function</span>(x){<span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x))}
data&lt;-<span class="kw">seq</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(data,<span class="kw">f</span>(data),<span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-1-1.png" /><!-- --></p>
<p>We can also write the function into another format as follows: <span class="math display">\[log \frac{p(y=1)}{1-p(y=1)}= \beta_0+\beta_1x_1+...+\beta_nx_n\]</span> Thus, we know that the regression coeficients of <span class="math inline">\(\beta_i\)</span> actually change the &quot;log-odds&quot; of the event. Of course, note that the magnitude of <span class="math inline">\(\beta_i\)</span> is dependent upon the units of <span class="math inline">\(x_i\)</span>.</p>
<p>The following is an example testing whether that home teams are more likely to win in NFL games. The results show that the odd of winning is the same for both home and away teams.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mydata =<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&#39;https://raw.githubusercontent.com/nfl-football-ops/Big-Data-Bowl/master/Data/games.csv&#39;</span>))
mydata<span class="op">$</span>result_new&lt;-<span class="kw">ifelse</span>(mydata<span class="op">$</span>HomeScore<span class="op">&gt;</span>mydata<span class="op">$</span>VisitorScore,<span class="dv">1</span>,<span class="dv">0</span>)
<span class="kw">summary</span>(mydata<span class="op">$</span>result_new)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.4945  1.0000  1.0000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mylogit1 =<span class="st"> </span><span class="kw">glm</span>(result_new<span class="op">~</span><span class="dv">1</span>, <span class="dt">family=</span>binomial, <span class="dt">data=</span>mydata)
<span class="kw">summary</span>(mylogit1)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = result_new ~ 1, family = binomial, data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.168  -1.168  -1.168   1.187   1.187  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -0.02198    0.20967  -0.105    0.917
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 126.14  on 90  degrees of freedom
## Residual deviance: 126.14  on 90  degrees of freedom
## AIC: 128.14
## 
## Number of Fisher Scoring iterations: 3</code></pre>
</div>
<div id="probit" class="section level2">
<h2><span class="header-section-number">1.2</span> Probit</h2>
<p>As noted above, logit <span class="math inline">\(f(x)=log(\frac{p(y=1)}{1-p(y=1)})\)</span> provides the resulting range of <span class="math inline">\((0,1)\)</span>. Another way to provide the same rage is through the cdf of normal distribution.The following R code is used to illusrate this process.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data2&lt;-<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(data2,<span class="kw">pnorm</span>(data2),<span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-3-1.png" /><!-- --> Thus, the cdf of normal distribution can be used to indicate the probability of <span class="math inline">\(p(y=1)\)</span>.</p>
<p><span class="math display">\[\Phi(\beta_0+\beta_1x_1+...+\beta_nx_n )= p(y=1)\]</span></p>
<p>Similar to logit model, we can also write the inverse function of the cdf to get the function that can be from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>.</p>
<p><span class="math display">\[\beta_0+\beta_1x_1+...+\beta_nx_n =\Phi^{-1}(p(y=1))\]</span></p>
<p>Thus, for example, if <span class="math inline">\(X\beta\)</span> = -2, based on <span class="math inline">\(\Phi(\beta_0+\beta_1x_1+...+\beta_nx_n )= p(y=1)\)</span> we can get that the <span class="math inline">\(p(y=1)=0.023\)</span>.</p>
<p>In contrast, if <span class="math inline">\(X\beta\)</span> = 3, the <span class="math inline">\(p(y=1)=0.999\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="op">-</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.02275013</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>## [1] 0.9986501</code></pre>
<p>Let's assume that there is a latent variable called <span class="math inline">\(Y^*\)</span> such that</p>
<p><span class="math display">\[Y^*=X\beta+\epsilon, \epsilon \sim N(0,\sigma^2)\]</span> You could think of <span class="math inline">\(Y^*\)</span> as a kind of &quot;proxy&quot; between <span class="math inline">\(X\beta+\epsilon\)</span> and the observed <span class="math inline">\(Y (1 or 0)\)</span>. Thus, we can get the following. Note that, it does not have to be zero, and can be any constant.</p>
<p><span class="math display">\[
Y^*=\begin{cases} 0 \;\;\: if \;  y_i^* \leq 0 \\ 1 \;\;\: if \;  y_i^* &gt; 0 \end{cases}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[y_i^* &gt; 0 \Rightarrow \beta^{&#39;}X_i + \epsilon_i &gt;0 \Rightarrow \epsilon_i &gt; -\beta^{&#39;}X_i\]</span></p>
<p>Thus, we can write it as follows. Note that <span class="math inline">\(\frac{ \epsilon_i}{\sigma} \sim N(0,1)\)</span></p>
<p><span class="math display">\[p(y=1|x_i)= p(y_i^* &gt;0|x_i)=p(\epsilon_i &gt; -\beta^{&#39;}X_i)= p(\frac{ \epsilon_i}{\sigma}&gt;\frac{-\beta^{&#39;}X_i}{\sigma})=\Phi(\frac{\beta^{&#39;}X_i}{\sigma}) \]</span> We thus can get:</p>
<p><span class="math display">\[p(y=0|x_i)=1-\Phi(\frac{\beta^{&#39;}X_i}{\sigma})\]</span></p>
<p>For <span class="math inline">\(p(y=1|x_i)=\Phi(\frac{\beta^{&#39;}X_i}{\sigma})\)</span>, we can not really estimate both <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> as they are in a ratio. We can assume <span class="math inline">\(\sigma =1\)</span>, then <span class="math inline">\(\epsilon \sim N(0,1)\)</span>. We know <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> since we observe them. Thus, we can write it as follows.</p>
<p><span class="math display">\[p(y=1|x_i)=\Phi(\beta^{&#39;}X_i)\]</span></p>
<!--chapter:end:index.Rmd-->
</div>
</div>
<div id="intro" class="section level1">
<h1><span class="header-section-number">2</span> MLE</h1>
<div id="basic-idea-of-mle" class="section level2">
<h2><span class="header-section-number">2.1</span> Basic idea of MLE</h2>
<p>Suppose that we flip a coin, <span class="math inline">\(y_i=0\)</span> for tails and <span class="math inline">\(y_i=1\)</span> for heads. If we get <span class="math inline">\(p\)</span> heads from <span class="math inline">\(n\)</span> trials, we can get the proportion of heads is <span class="math inline">\(p/n\)</span>, which is the sample mean. If we do not do any further calculation, this is our best guess.</p>
<p>Suppose that the true proablity is <span class="math inline">\(\rho\)</span>, then we can get:</p>
<p><span class="math display">\[
\mathbf{L}(y_i)=\begin{cases} \rho \;\;\:   y_i = 1 \\ 1-\rho \;\;\:  y_i = 0 \end{cases}
\]</span> Thus, we can also write it as follows. <span class="math display">\[\mathbf{L}(y_i) = \rho^{y_i}(1-\rho)^{1-y_i}\]</span></p>
<p>Thus, we can get:</p>
<p><span class="math display">\[\prod \mathbf{L}(y_i|\rho)=\rho^{\sum y_i}(1-\rho)^{\sum(1-y_i)}\]</span> Further, we can get a log-transformed format.</p>
<p><span class="math display">\[log (\prod \mathbf{L}(y_i|\rho))=\sum y_i log \rho + \sum(1-y_i) log(1-\rho)\]</span></p>
<p>To maximize the log-function above, we can calculate the derivative with respect to <span class="math inline">\(\rho\)</span>. <span class="math display">\[\frac{\partial log (\prod \mathbf{L}(y_i|\rho)) }{\partial \rho}=\sum y_i \frac{1}{\rho}-\sum(1-y_i) \frac{1}{1-\rho}\]</span> Set the derivative to zero and solve for <span class="math inline">\(\rho\)</span>, we can get</p>
<p><span class="math display">\[\sum y_i \frac{1}{\rho}-\sum(1-y_i) \frac{1}{1-\rho}=0\]</span> <span class="math display">\[\Rightarrow (1-\rho)\sum y_i - \rho \sum(1-y_i) =0\]</span> <span class="math display">\[\Rightarrow \sum y_i-\rho\sum y_i - n\rho +\rho\sum y_i =0\]</span> <span class="math display">\[\Rightarrow \sum y_i - n\rho  =0\]</span> <span class="math display">\[\Rightarrow \rho  = \frac{\sum y_i}{n}=\frac{p}{n}\]</span> Thus, we can see that the <span class="math inline">\(\rho\)</span> maximizing the likelihood function is equal to the sample mean.</p>
</div>
<div id="coin-flip-example-probit-and-logit" class="section level2">
<h2><span class="header-section-number">2.2</span> Coin flip example, probit, and logit</h2>
<p>In the example above, we are not really trying to estimate a lot of regression coefficients. What we are doing actually is to calculate the sample mean, or intercept in the regresion sense. What does it mean? Let's use some data to explain it.</p>
<p>Suppose that we flip a coin 20 times and observe 8 heads. We can use the R's glm function to esimate the <span class="math inline">\(\rho\)</span>. If the result is consistent with what we did above, we should observe that the <span class="math inline">\(cdf\)</span> of the esimate of <span class="math inline">\(\beta_0\)</span> (i.e., intercept) should be equal to <span class="math inline">\(8/20=0.4\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coins&lt;-<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dt">times=</span><span class="dv">8</span>),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dt">times=</span><span class="dv">12</span>))
<span class="kw">table</span>(coins)</code></pre></div>
<pre><code>## coins
##  0  1 
## 12  8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coins&lt;-<span class="kw">as.data.frame</span>(coins)</code></pre></div>
<div id="probit-1" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Probit</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">probitresults &lt;-<span class="st"> </span><span class="kw">glm</span>(coins <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), <span class="dt">data =</span> coins)
probitresults</code></pre></div>
<pre><code>## 
## Call:  glm(formula = coins ~ 1, family = binomial(link = &quot;probit&quot;), 
##     data = coins)
## 
## Coefficients:
## (Intercept)  
##     -0.2533  
## 
## Degrees of Freedom: 19 Total (i.e. Null);  19 Residual
## Null Deviance:       26.92 
## Residual Deviance: 26.92     AIC: 28.92</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(probitresults<span class="op">$</span>coefficients)</code></pre></div>
<pre><code>## (Intercept) 
##         0.4</code></pre>
<p>As we can see the intercept is <span class="math inline">\(-0.2533\)</span>, and thus <span class="math inline">\(\Phi(-0.2533471)=0.4\)</span></p>
</div>
<div id="logit-1" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Logit</h3>
<p>We can also use logit link to calculate the intercept as well. Recall that</p>
<p><span class="math display">\[p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span> Thus,</p>
<p><span class="math display">\[p(y=1)=\frac{e^{\beta_0}}{1+e^{\beta_0}}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logitresults &lt;-<span class="st"> </span><span class="kw">glm</span>(coins <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>), <span class="dt">data =</span> coins)
logitresults<span class="op">$</span>coefficients</code></pre></div>
<pre><code>## (Intercept) 
##  -0.4054651</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(logitresults<span class="op">$</span>coefficients)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(logitresults<span class="op">$</span>coefficients))</code></pre></div>
<pre><code>## (Intercept) 
##         0.4</code></pre>
<p>Note that, the defaul link for the binomial in the glm function in logit.</p>
</div>
</div>
<div id="further-on-logit" class="section level2">
<h2><span class="header-section-number">2.3</span> Further on logit</h2>
<p>The probablity of <span class="math inline">\(y=1\)</span> is as follows:</p>
<p><span class="math display">\[p=p(y=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span></p>
<p>Thus, the likelihood function is as follows:</p>
<p><span class="math display">\[L=\prod p^{y_i}(1-p)^{1-y_i}=\prod (\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}})^{y_i}(\frac{1}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}})^{1-y_i}\]</span></p>
<p><span class="math display">\[=\prod (1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)})^{-y_i}(1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n})^{-(1-y_i)}\]</span></p>
<p>Thus, the log-likelihood is as follows: <span class="math display">\[logL=\sum (-y_i \cdot log(1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)})-(1-y_i)\cdot log(1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}))\]</span></p>
<p>Typically, optimisers minimize a function, so we use negative log-likelihood as minimising that is equivalent to maximising the log-likelihood or the likelihood itself.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Source of R code: https://www.r-bloggers.com/logistic-regression/</span>

mle.logreg =<span class="st"> </span><span class="cf">function</span>(fmla, data)
{
  <span class="co"># Define the negative log likelihood function</span>
  logl &lt;-<span class="st"> </span><span class="cf">function</span>(theta,x,y){
    y &lt;-<span class="st"> </span>y
    x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(x)
    beta &lt;-<span class="st"> </span>theta[<span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x)]
    
    <span class="co"># Use the log-likelihood of the Bernouilli distribution, where p is</span>
    <span class="co"># defined as the logistic transformation of a linear combination</span>
    <span class="co"># of predictors, according to logit(p)=(x%*%beta)</span>
    loglik &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">-</span>y<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>(x<span class="op">%*%</span>beta))) <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x<span class="op">%*%</span>beta)))
    <span class="kw">return</span>(<span class="op">-</span>loglik)
  }
  
  <span class="co"># Prepare the data</span>
  outcome =<span class="st"> </span><span class="kw">rownames</span>(<span class="kw">attr</span>(<span class="kw">terms</span>(fmla),<span class="st">&quot;factors&quot;</span>))[<span class="dv">1</span>]
  dfrTmp =<span class="st"> </span><span class="kw">model.frame</span>(data)
  x =<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">model.matrix</span>(fmla, <span class="dt">data=</span>dfrTmp))
  y =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.matrix</span>(data[,<span class="kw">match</span>(outcome,<span class="kw">colnames</span>(data))]))
  
  <span class="co"># Define initial values for the parameters</span>
  theta.start =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,(<span class="kw">dim</span>(x)[<span class="dv">2</span>]))
  <span class="kw">names</span>(theta.start) =<span class="st"> </span><span class="kw">colnames</span>(x)
  
  <span class="co"># Calculate the maximum likelihood</span>
  mle =<span class="st"> </span><span class="kw">optim</span>(theta.start,logl,<span class="dt">x=</span>x,<span class="dt">y=</span>y, <span class="dt">method =</span> <span class="st">&#39;BFGS&#39;</span>, <span class="dt">hessian=</span>T)
  out =<span class="st"> </span><span class="kw">list</span>(<span class="dt">beta=</span>mle<span class="op">$</span>par,<span class="dt">vcov=</span><span class="kw">solve</span>(mle<span class="op">$</span>hessian),<span class="dt">ll=</span><span class="dv">2</span><span class="op">*</span>mle<span class="op">$</span>value)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mydata =<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(<span class="st">&#39;https://stats.idre.ucla.edu/stat/data/binary.csv&#39;</span>))
mylogit1 =<span class="st"> </span><span class="kw">glm</span>(admit<span class="op">~</span>gre<span class="op">+</span>gpa<span class="op">+</span><span class="kw">as.factor</span>(rank), <span class="dt">family=</span>binomial, <span class="dt">data=</span>mydata)

mydata<span class="op">$</span>rank =<span class="st"> </span><span class="kw">factor</span>(mydata<span class="op">$</span>rank) <span class="co">#Treat rank as a categorical variable</span>
fmla =<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;admit~gre+gpa+rank&quot;</span>) <span class="co">#Create model formula</span>
mylogit2 =<span class="st"> </span><span class="kw">mle.logreg</span>(fmla, mydata) <span class="co">#Estimate coefficients</span>


 <span class="kw">print</span>(<span class="kw">cbind</span>(<span class="kw">coef</span>(mylogit1), mylogit2<span class="op">$</span>beta))</code></pre></div>
<pre><code>##                          [,1]         [,2]
## (Intercept)      -3.989979073 -3.772676422
## gre               0.002264426  0.001375522
## gpa               0.804037549  0.898201239
## as.factor(rank)2 -0.675442928 -0.675543009
## as.factor(rank)3 -1.340203916 -1.356554831
## as.factor(rank)4 -1.551463677 -1.563396035</code></pre>
</div>
<div id="fisher-information" class="section level2">
<h2><span class="header-section-number">2.4</span> Fisher information</h2>
<p><a href="https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf">https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf</a></p>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">2.5</span> References</h2>
<p><a href="http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf" class="uri">http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf</a></p>
<!--chapter:end:01-MLE.Rmd-->
</div>
</div>
<div id="linear-mixed-models" class="section level1">
<h1><span class="header-section-number">3</span> Linear Mixed Models</h1>
<p>The following is a shortened version of Jonathan Rosenblatt's LMM tutorial. <a href="http://www.john-ros.com/Rcourse/lme.html" class="uri">http://www.john-ros.com/Rcourse/lme.html</a>.</p>
<p>In addition, another reference is from Douglas Bates's R package document. <a href="https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf?fbclid=IwAR1nmmRP9A0BrhKdgBibNjM5acR_spTpXV8QlQGdmTWyQz3ZtV3LYn6kCbQ" class="uri">https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf?fbclid=IwAR1nmmRP9A0BrhKdgBibNjM5acR_spTpXV8QlQGdmTWyQz3ZtV3LYn6kCbQ</a></p>
<p>Assume that <span class="math inline">\(y\)</span> is a function of <span class="math inline">\(x\)</span> and <span class="math inline">\(u\)</span>, where <span class="math inline">\(x\)</span> is the fixed effect and <span class="math inline">\(u\)</span> is the random effect. Thus, we can get,</p>
<p><span class="math display">\[y|x, u = x&#39;\beta+z&#39;u+\epsilon\]</span></p>
<p>For random effect, one example can be that you want to test the treatment effect, and sample 8 observations from 4 groups. You measure before and after the treatment. In this case, <span class="math inline">\(x\)</span> represents the treatment effect, whereas <span class="math inline">\(z\)</span> represents the group effect (i.e., random effect). Note that, in this case, it reminds the paired t-test. Remember in SPSS, why do we do paired t-test? Typically, it is the case when we measure a subject (or, participant) twice. In this case, we can consider each participant as an unit of random effect (rather than as group in the last example.)</p>
<div id="calculate-mean" class="section level2">
<h2><span class="header-section-number">3.1</span> Calculate mean</h2>
<p>The following code generates 4 numbers (<span class="math inline">\(N(0,10)\)</span>) for 4 groups. Then, replicate it within each group.That is, in the end, there are 8 observations.</p>
<p>Note that, in the following code, there are no &quot;independent variables&quot;. Both the linear model and mixed model are actually just trying to calculate the mean. Note that lmer(y~1+1|groups) and lmer(y~1|groups) will generate the same results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
n.groups &lt;-<span class="st"> </span><span class="dv">4</span> <span class="co"># number of groups</span>
n.repeats &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># samples per group</span>
<span class="co">#Generating index for observations belong to the same group</span>
groups &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n.groups, <span class="dt">each=</span>n.repeats))
n &lt;-<span class="st"> </span><span class="kw">length</span>(groups)
<span class="co">#Generating 4 random numbers, assuming normal distribution</span>
z0 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n.groups, <span class="dv">0</span>, <span class="dv">10</span>) 
z &lt;-<span class="st"> </span>z0[<span class="kw">as.numeric</span>(groups)] <span class="co"># generate and inspect random group effects</span>
z</code></pre></div>
<pre><code>## [1] -5.6047565 -5.6047565 -2.3017749 -2.3017749 15.5870831 15.5870831  0.7050839
## [8]  0.7050839</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>) <span class="co"># generate measurement error</span>
beta0 &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># this is the actual parameter of interest! The global mean.</span>
y &lt;-<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>z <span class="op">+</span><span class="st"> </span>epsilon <span class="co"># sample from an LMM</span>

<span class="co"># fit a linear model assuming independence</span>
<span class="co"># i.e., assume that there is no &quot;group things&quot;.</span>
lm.<span class="dv">5</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span><span class="dv">1</span>)

<span class="co"># fit a mixed-model that deals with the group dependence</span>
<span class="co">#install.packages(&quot;lme4&quot;)</span>
<span class="kw">library</span>(lme4)
lme.<span class="fl">5.</span>a &lt;-<span class="st"> </span><span class="kw">lmer</span>(y<span class="op">~</span><span class="dv">1</span><span class="op">+</span><span class="dv">1</span><span class="op">|</span>groups) 
lme.<span class="fl">5.</span>b &lt;-<span class="st"> </span><span class="kw">lmer</span>(y<span class="op">~</span><span class="dv">1</span><span class="op">|</span>groups) 
lm.<span class="dv">5</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ 1)
## 
## Coefficients:
## (Intercept)  
##       4.283</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lme.<span class="fl">5.</span>a </code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: y ~ 1 + 1 | groups
## REML criterion at convergence: 36.1666
## Random effects:
##  Groups   Name        Std.Dev.
##  groups   (Intercept) 8.8521  
##  Residual             0.8873  
## Number of obs: 8, groups:  groups, 4
## Fixed Effects:
## (Intercept)  
##       4.283</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lme.<span class="fl">5.</span>b </code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: y ~ 1 | groups
## REML criterion at convergence: 36.1666
## Random effects:
##  Groups   Name        Std.Dev.
##  groups   (Intercept) 8.8521  
##  Residual             0.8873  
## Number of obs: 8, groups:  groups, 4
## Fixed Effects:
## (Intercept)  
##       4.283</code></pre>
</div>
<div id="test-the-treatment-effect" class="section level2">
<h2><span class="header-section-number">3.2</span> Test the treatment effect</h2>
<p>As we can see that, LLM and paired t-test generate the same t-value.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">times&lt;-<span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="dv">4</span>) <span class="co"># first time and second time</span>
times</code></pre></div>
<pre><code>## [1] 1 2 1 2 1 2 1 2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data_combined&lt;-<span class="kw">cbind</span>(y,groups,times)
data_combined</code></pre></div>
<pre><code>##               y groups times
## [1,] -3.4754687      1     1
## [2,] -1.8896915      1     2
## [3,]  0.1591413      2     1
## [4,] -1.5668361      2     2
## [5,] 16.9002303      3     1
## [6,] 17.1414212      3     2
## [7,]  3.9291657      4     1
## [8,]  3.0648977      4     2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lme_diff_times&lt;-<span class="st"> </span><span class="kw">lmer</span>(y<span class="op">~</span>times<span class="op">+</span>(<span class="dv">1</span><span class="op">|</span>groups)) 


t_results&lt;-<span class="kw">t.test</span>(y<span class="op">~</span>times, <span class="dt">paired=</span><span class="ot">TRUE</span>)

lme_diff_times</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: y ~ times + (1 | groups)
## REML criterion at convergence: 35.0539
## Random effects:
##  Groups   Name        Std.Dev.
##  groups   (Intercept) 8.845   
##  Residual             1.013   
## Number of obs: 8, groups:  groups, 4
## Fixed Effects:
## (Intercept)        times  
##      4.5691      -0.1908</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="st">&quot;The following results are from paired t-test&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;The following results are from paired t-test&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t_results<span class="op">$</span>statistic</code></pre></div>
<pre><code>##         t 
## 0.2664793</code></pre>
</div>
<div id="another-example" class="section level2">
<h2><span class="header-section-number">3.3</span> Another example</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Dyestuff, <span class="dt">package=</span><span class="st">&#39;lme4&#39;</span>)
<span class="kw">attach</span>(Dyestuff)</code></pre></div>
<pre><code>## The following objects are masked from Dyestuff (pos = 4):
## 
##     Batch, Yield</code></pre>
<pre><code>## The following objects are masked from Dyestuff (pos = 8):
## 
##     Batch, Yield</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Dyestuff</code></pre></div>
<pre><code>##    Batch Yield
## 1      A  1545
## 2      A  1440
## 3      A  1440
## 4      A  1520
## 5      A  1580
## 6      B  1540
## 7      B  1555
## 8      B  1490
## 9      B  1560
## 10     B  1495
## 11     C  1595
## 12     C  1550
## 13     C  1605
## 14     C  1510
## 15     C  1560
## 16     D  1445
## 17     D  1440
## 18     D  1595
## 19     D  1465
## 20     D  1545
## 21     E  1595
## 22     E  1630
## 23     E  1515
## 24     E  1635
## 25     E  1625
## 26     F  1520
## 27     F  1455
## 28     F  1450
## 29     F  1480
## 30     F  1445</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lme_batch&lt;-<span class="st"> </span><span class="kw">lmer</span>( Yield <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>Batch)  , Dyestuff )
<span class="kw">summary</span>(lme_batch)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Yield ~ 1 + (1 | Batch)
##    Data: Dyestuff
## 
## REML criterion at convergence: 319.7
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -1.4117 -0.7634  0.1418  0.7792  1.8296 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Batch    (Intercept) 1764     42.00   
##  Residual             2451     49.51   
## Number of obs: 30, groups:  Batch, 6
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  1527.50      19.38    78.8</code></pre>
</div>
<div id="full-lmm-model" class="section level2">
<h2><span class="header-section-number">3.4</span> Full LMM model</h2>
<p>In the following, I used the data from the package of lme4. For Days + (1 | Subject), it only has random intercept; in contrast, Days + ( Days| Subject ) has both random intercept and random slope for Days. Note that, random effects do not generate specific slopes for each level of Days, but rather just a variance of all the slopes.</p>
<p>Therefore, we can see that &quot;Days + ( Days| Subject )&quot; and &quot;Days + ( 1+Days| Subject )&quot; generate the same results. For more discussion, you can refer to the following link: <a href="https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r" class="uri">https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(sleepstudy, <span class="dt">package=</span><span class="st">&#39;lme4&#39;</span>)
<span class="kw">attach</span>(sleepstudy)</code></pre></div>
<pre><code>## The following objects are masked from sleepstudy (pos = 4):
## 
##     Days, Reaction, Subject</code></pre>
<pre><code>## The following objects are masked from sleepstudy (pos = 8):
## 
##     Days, Reaction, Subject</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm1 &lt;-<span class="st"> </span><span class="kw">lmer</span>(Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Subject), sleepstudy)
<span class="kw">summary</span>(fm1)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Reaction ~ Days + (1 | Subject)
##    Data: sleepstudy
## 
## REML criterion at convergence: 1786.5
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.2257 -0.5529  0.0109  0.5188  4.2506 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Subject  (Intercept) 1378.2   37.12   
##  Residual              960.5   30.99   
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 251.4051     9.7467   25.79
## Days         10.4673     0.8042   13.02
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.371</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm2&lt;-<span class="kw">lmer</span> ( Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>( Days<span class="op">|</span><span class="st"> </span>Subject ) , <span class="dt">data=</span> sleepstudy )
<span class="kw">summary</span>(fm2)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Reaction ~ Days + (Days | Subject)
##    Data: sleepstudy
## 
## REML criterion at convergence: 1743.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.9536 -0.4634  0.0231  0.4633  5.1793 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  Subject  (Intercept) 611.90   24.737       
##           Days         35.08    5.923   0.07
##  Residual             654.94   25.592       
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  251.405      6.824  36.843
## Days          10.467      1.546   6.771
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.138</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm3&lt;-<span class="kw">lmer</span> ( Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">+</span>Days<span class="op">|</span><span class="st"> </span>Subject ) , <span class="dt">data=</span> sleepstudy )
<span class="kw">summary</span>(fm3)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Reaction ~ Days + (1 + Days | Subject)
##    Data: sleepstudy
## 
## REML criterion at convergence: 1743.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.9536 -0.4634  0.0231  0.4633  5.1793 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  Subject  (Intercept) 611.90   24.737       
##           Days         35.08    5.923   0.07
##  Residual             654.94   25.592       
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  251.405      6.824  36.843
## Days          10.467      1.546   6.771
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.138</code></pre>
</div>
<div id="serial-correlations-in-time-and-space" class="section level2">
<h2><span class="header-section-number">3.5</span> Serial correlations in time and space</h2>
<p>The hierarchical model of <span class="math inline">\(y|x, u = x&#39;\beta+z&#39;u+\epsilon\)</span> can work well for correlations within blocks, but not for correlations in time as the correlations decay in time. The following uses nlme package to calculate time serial data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nlme)
<span class="kw">head</span>(nlme<span class="op">::</span>Ovary,<span class="dt">n=</span><span class="dv">50</span>)</code></pre></div>
<pre><code>## Grouped Data: follicles ~ Time | Mare
##    Mare        Time follicles
## 1     1 -0.13636360        20
## 2     1 -0.09090910        15
## 3     1 -0.04545455        19
## 4     1  0.00000000        16
## 5     1  0.04545455        13
## 6     1  0.09090910        10
## 7     1  0.13636360        12
## 8     1  0.18181820        14
## 9     1  0.22727270        13
## 10    1  0.27272730        20
## 11    1  0.31818180        22
## 12    1  0.36363640        15
## 13    1  0.40909090        18
## 14    1  0.45454550        17
## 15    1  0.50000000        14
## 16    1  0.54545450        18
## 17    1  0.59090910        14
## 18    1  0.63636360        16
## 19    1  0.68181820        17
## 20    1  0.72727270        18
## 21    1  0.77272730        18
## 22    1  0.81818180        17
## 23    1  0.86363640        14
## 24    1  0.90909090        12
## 25    1  0.95454550        12
## 26    1  1.00000000        14
## 27    1  1.04545500        10
## 28    1  1.09090900        11
## 29    1  1.13636400        16
## 30    2 -0.15000000         6
## 31    2 -0.10000000         6
## 32    2 -0.05000000         8
## 33    2  0.00000000         7
## 34    2  0.05000000        16
## 35    2  0.10000000        10
## 36    2  0.15000000        13
## 37    2  0.20000000         9
## 38    2  0.25000000         7
## 39    2  0.30000000         6
## 40    2  0.35000000         8
## 41    2  0.40000000         8
## 42    2  0.45000000         6
## 43    2  0.50000000         8
## 44    2  0.55000000         7
## 45    2  0.60000000         9
## 46    2  0.65000000         6
## 47    2  0.70000000         4
## 48    2  0.75000000         5
## 49    2  0.80000000         8
## 50    2  0.85000000        11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm1Ovar.lme &lt;-<span class="st"> </span>nlme<span class="op">::</span><span class="kw">lme</span>(<span class="dt">fixed=</span>follicles <span class="op">~</span><span class="st"> </span><span class="kw">sin</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>Time) <span class="op">+</span><span class="st"> </span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>Time), 
                   <span class="dt">data =</span> Ovary, 
                   <span class="dt">random =</span> <span class="kw">pdDiag</span>(<span class="op">~</span><span class="kw">sin</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>Time)), 
                   <span class="dt">correlation=</span><span class="kw">corAR1</span>() )
<span class="kw">summary</span>(fm1Ovar.lme)</code></pre></div>
<pre><code>## Linear mixed-effects model fit by REML
##  Data: Ovary 
##        AIC     BIC   logLik
##   1563.448 1589.49 -774.724
## 
## Random effects:
##  Formula: ~sin(2 * pi * Time) | Mare
##  Structure: Diagonal
##         (Intercept) sin(2 * pi * Time) Residual
## StdDev:    2.858385           1.257977 3.507053
## 
## Correlation Structure: AR(1)
##  Formula: ~1 | Mare 
##  Parameter estimate(s):
##       Phi 
## 0.5721866 
## Fixed effects: follicles ~ sin(2 * pi * Time) + cos(2 * pi * Time) 
##                        Value Std.Error  DF   t-value p-value
## (Intercept)        12.188089 0.9436602 295 12.915760  0.0000
## sin(2 * pi * Time) -2.985297 0.6055968 295 -4.929513  0.0000
## cos(2 * pi * Time) -0.877762 0.4777821 295 -1.837159  0.0672
##  Correlation: 
##                    (Intr) s(*p*T
## sin(2 * pi * Time)  0.000       
## cos(2 * pi * Time) -0.123  0.000
## 
## Standardized Within-Group Residuals:
##         Min          Q1         Med          Q3         Max 
## -2.34910093 -0.58969626 -0.04577893  0.52931186  3.37167486 
## 
## Number of Observations: 308
## Number of Groups: 11</code></pre>
<!--chapter:end:07-lmm.rmd-->
</div>
</div>
<div id="basic-concepts" class="section level1">
<h1><span class="header-section-number">4</span> Basic Concepts</h1>
<div id="score" class="section level2">
<h2><span class="header-section-number">4.1</span> Score</h2>
<p>The score is the gradient (the vector of partial derivatives) of <span class="math inline">\(log L(\theta)\)</span>, with respect to an m-dimensional parameter vector <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[S(\theta) = \frac{\partial\ell}{\partial \theta}\]</span></p>
<p>Such differentiation will generate a <span class="math inline">\(m\times 1\)</span> row vector, which indicates the sensitivity of the likelihood.</p>
<p>Quote from Steffen Lauritzen's slides: &quot;Generally the solution to this equation must be calculated by iterative methods. One of the most common methods is the Newton–Raphson method and this is based on successive approximations to the solution, using Taylor’s theorem to approximate the equation.&quot;</p>
<p>For instance, using logit link, we can get the first derivative of log likelihood logistic regression as follows. We can not really find <span class="math inline">\(\beta\)</span> easily to make the equation to be 0.</p>
<p><span class="math display">\[\begin{aligned}
\frac{\partial \ell} {\partial \beta} 
&amp;= \sum_{i=1}^{n}x_i^T[y_i-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}] \\
&amp;=\sum_{i=1}^{n} x_i^T[y_i-\hat{y_i}]
\end{aligned}\]</span></p>
</div>
<div id="canonical-link-function" class="section level2">
<h2><span class="header-section-number">4.2</span> Canonical link function</h2>
<p>Inspired by a Stack Exchange post, I created the following figure:</p>
<p><span class="math display">\[ \frac{Paramter}{\theta} \longrightarrow \gamma^{&#39;}(\theta) = \mu \longrightarrow \frac{Mean}{\mu} \longrightarrow g(\mu) = \eta \longrightarrow \frac{ Linear predictor}{\eta} \]</span></p>
<p>For the case of <span class="math inline">\(n\)</span> time Bernoulli (i.e., Binomial), its canonical link function is logit. Specifically,</p>
<p><span class="math display">\[ \frac{Paramter}{\theta=\beta^TX} \longrightarrow \gamma^{&#39;}(\theta)= \frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}\longrightarrow \frac{Mean}{\mu=\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}}\longrightarrow g(\mu) = log \frac{\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}}{1-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}}\longrightarrow \frac{ Linear predictor}{\eta = \beta^TX}\]</span></p>
<p>(Side note: logit = log odds = log <span class="math inline">\(\frac{Event - Happened }{Event - Not - Happened}\)</span>)</p>
<p>Where,</p>
<p><span class="math display">\[\mu=p(y=1)=\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}\]</span> Thus, we know that <span class="math inline">\(\mu\)</span> or <span class="math inline">\(p(y=1)\)</span> is the mean function. Recall that, <span class="math inline">\(n\)</span> trails of coin flips, and get <span class="math inline">\(p\)</span> heads. Thus <span class="math inline">\(\mu = \frac{p}{n}\)</span>.</p>
<p>References: 1. Steffen Lauritzen's slides: <a href="http://www.stats.ox.ac.uk/~steffen/teaching/bs2HT9/scoring.pdf" class="uri">http://www.stats.ox.ac.uk/~steffen/teaching/bs2HT9/scoring.pdf</a> 2. The Stack Exchange post: <a href="https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function" class="uri">https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function</a></p>
<!--chapter:end:08-Basic_concepts.Rmd-->
</div>
</div>
<div id="generalized-linear-mixed-models" class="section level1">
<h1><span class="header-section-number">5</span> Generalized Linear Mixed Models</h1>
<div id="computing-techniques" class="section level2">
<h2><span class="header-section-number">5.1</span> Computing techniques</h2>
<p>Since GLMM can use EM algorithm in its maximum likelihood calculation (see McCulloch, 1994), it is practically useful to rehearse EM and other computing techniques.</p>
<div id="monte-carlo-approximation" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Monte carlo approximation</h3>
<p>Example: calculate the integral of <span class="math inline">\(p(z&gt;2)\)</span> when <span class="math inline">\(z \sim N(0,1)\)</span>. To use Monte Carlo approximation, we can have an indicator function, which will determine whether the sample from <span class="math inline">\(N(0,1)\)</span> will be included into the calculation of the integral.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Nsim=<span class="dv">10</span><span class="op">^</span><span class="dv">4</span>

indicator=<span class="cf">function</span>(x){
y=<span class="kw">ifelse</span>((x<span class="op">&gt;</span><span class="dv">2</span>),<span class="dv">1</span>,<span class="dv">0</span>)
<span class="kw">return</span>(y)}

newdata&lt;-<span class="kw">rnorm</span>(Nsim, <span class="dv">0</span>,<span class="dv">1</span> )

mc=<span class="kw">c</span>(); v=<span class="kw">c</span>(); upper=<span class="kw">c</span>(); lower=<span class="kw">c</span>()

<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>Nsim)
{
mc[j]=<span class="kw">mean</span>(<span class="kw">indicator</span>(newdata[<span class="dv">1</span><span class="op">:</span>j]))
v[j]=(j<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>})<span class="op">*</span><span class="kw">var</span>(<span class="kw">indicator</span>(newdata[<span class="dv">1</span><span class="op">:</span>j]))
upper[j]=mc[j]<span class="op">+</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sqrt</span>(v[j])
lower[j]=mc[j]<span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sqrt</span>(v[j])
}

<span class="kw">library</span>(ggplot2)
values=<span class="kw">c</span>(mc,upper,lower)
type=<span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;mc&quot;</span>,Nsim),<span class="kw">rep</span>(<span class="st">&quot;upper&quot;</span>,Nsim),<span class="kw">rep</span>(<span class="st">&quot;lower&quot;</span>,Nsim))
iter=<span class="kw">rep</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span>Nsim),<span class="dv">3</span>)
data=<span class="kw">data.frame</span>(<span class="dt">val=</span>values, <span class="dt">tp=</span>type, <span class="dt">itr=</span>iter)
Rcode&lt;-<span class="kw">ggplot</span>(data,<span class="kw">aes</span>(itr,val,<span class="dt">col=</span>tp))<span class="op">+</span><span class="kw">geom_line</span>(<span class="dt">size=</span><span class="fl">0.5</span>)
Rcode<span class="op">+</span><span class="kw">geom_hline</span>(<span class="dt">yintercept=</span><span class="dv">1</span><span class="op">-</span><span class="kw">pnorm</span>(<span class="dv">2</span>),<span class="dt">color=</span><span class="st">&quot;green&quot;</span>,<span class="dt">size=</span><span class="fl">0.5</span>)</code></pre></div>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-16-1.png" /><!-- --></p>
</div>
<div id="importance-sampling" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Importance sampling</h3>
<p>Importance sampling has samples generated from a different distribution than the distribution of interest. Specifically, assume that we want to calculate the expected value of <span class="math inline">\(h(x)\)</span>, and <span class="math inline">\(x \sim f(x)\)</span>.</p>
<p><span class="math display">\[E(h(x))=\int h(x) f(x) dx = \int h(x) \frac{f(x)}{g(x)} g(x) dx \]</span> We can sample <span class="math inline">\(x_i\)</span> from <span class="math inline">\(g(x)\)</span> and then calculate the mean of <span class="math inline">\(h(x_i) \frac{f(x_i)}{g(x_i)}\)</span>.</p>
<p>Using the same explane above, we can use a shifted exponential distribution to help calculate the intergral for normal distribution. Specifically,</p>
<p><span class="math display">\[\int_2^{\infty} \frac{1}{2 \pi} e^{-\frac{1}{2}x^2}dx = \int_2^{\infty} \frac{\frac{1}{2 \pi} e^{-\frac{1}{2}x^2}}{e^{-(x-2)}} e^{-(x-2)}dx \]</span> The idea is that, we can generate <span class="math inline">\(x_i\)</span> from exponential distribution of <span class="math inline">\(e^{-(x-2)}\)</span>, and then insert them into the targeted &quot;expected (value) function&quot; of <span class="math inline">\(\frac{\frac{1}{2 \pi} e^{-\frac{1}{2}x^2}}{e^{-(x-2)}}\)</span>. Thus, as you can see, importance sampling is based on the law of large numbers (i.e., If the same experiment or study is repeated independently a large number of times, the average of the results of the trials must be close to the expected value). We can use it to calculate integral based on link of the definition of expected value.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Nsim=<span class="dv">10</span><span class="op">^</span><span class="dv">4</span>
normal_density=<span class="cf">function</span>(x)
{y=(<span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span><span class="op">*</span>pi))<span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>(x<span class="op">^</span><span class="dv">2</span>))
<span class="kw">return</span>(y)}
x=<span class="dv">2</span><span class="op">-</span><span class="kw">log</span>(<span class="kw">runif</span>(Nsim))
ImpS=<span class="kw">c</span>(); v=<span class="kw">c</span>(); upper=<span class="kw">c</span>(); lower=<span class="kw">c</span>()
<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>Nsim)
{
ImpS[j]=<span class="kw">mean</span>(<span class="kw">normal_density</span>(x[<span class="dv">1</span><span class="op">:</span>j])<span class="op">/</span><span class="kw">exp</span>(<span class="op">-</span>(x[<span class="dv">1</span><span class="op">:</span>j]<span class="op">-</span><span class="dv">2</span>)))
v[j]=(j<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>})<span class="op">*</span><span class="kw">var</span>(<span class="kw">normal_density</span>(x[<span class="dv">1</span><span class="op">:</span>j])<span class="op">/</span><span class="kw">exp</span>(<span class="op">-</span>(x[<span class="dv">1</span><span class="op">:</span>j]<span class="op">-</span><span class="dv">2</span>)))
upper[j]=ImpS[j]<span class="op">+</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sqrt</span>(v[j])
lower[j]=ImpS[j]<span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sqrt</span>(v[j])
}

<span class="kw">library</span>(ggplot2)
values=<span class="kw">c</span>(ImpS,upper,lower)
type=<span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;mc&quot;</span>,Nsim),<span class="kw">rep</span>(<span class="st">&quot;upper&quot;</span>,Nsim),<span class="kw">rep</span>(<span class="st">&quot;lower&quot;</span>,Nsim))
iter=<span class="kw">rep</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span>Nsim),<span class="dv">3</span>)
data=<span class="kw">data.frame</span>(<span class="dt">val=</span>values, <span class="dt">tp=</span>type, <span class="dt">itr=</span>iter)
<span class="kw">ggplot</span>(data,<span class="kw">aes</span>(itr,val,<span class="dt">col=</span>tp))<span class="op">+</span><span class="kw">geom_line</span>(<span class="dt">size=</span><span class="fl">0.5</span>)<span class="op">+</span>
<span class="kw">geom_hline</span>(<span class="dt">yintercept=</span><span class="dv">1</span><span class="op">-</span><span class="kw">pnorm</span>(<span class="dv">2</span>),<span class="dt">color=</span><span class="st">&quot;green&quot;</span>,<span class="dt">size=</span><span class="fl">0.5</span>)</code></pre></div>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-17-1.png" /><!-- --></p>
</div>
<div id="newton-raphson-algorithm" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Newton Raphson algorithm</h3>
<p>For nonlinear functions, it is sometimes difficult to calculate MLEs. Newton Raphson algorithm is an iterative procedure to calculate MLEs.</p>
<p>The basic idea of Newton Raphson is to find a approximate function that can be easily maximized analytically. We need some theoretical background from Taylor's Theorem.</p>
<p>If <span class="math inline">\(f\)</span> has <span class="math inline">\(k+1\)</span> times differentiable on an open interval <span class="math inline">\(I\)</span>. For any <span class="math inline">\(x\)</span> and <span class="math inline">\(x+h\)</span> in <span class="math inline">\(I\)</span>, there is a point of <span class="math inline">\(w\)</span> between <span class="math inline">\(x\)</span> and <span class="math inline">\(x+h\)</span> where we can get the following:</p>
<p><span class="math display">\[f(x+h)=f(x)+f^{&#39;}h+\frac{1}{2}f^{&#39;&#39;}h^2+...+\frac{1}{k!}f^{[k]}(x)h^k+\frac{1}{(k+1)!}f^{[k+1]}(w)h^{k+1}\]</span> If <span class="math inline">\(h\)</span> goes to be close to <span class="math inline">\(0\)</span>, the higher order terms will go to <span class="math inline">\(0\)</span> as well. Thus, we can get:</p>
<p><span class="math display">\[f(x+h) \approx f(x)+f^{&#39;}(x)h \]</span> This is the first order Taylor approximation of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span>. In a similar vein, we also have the second order Taylor approximation of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span> as follows.</p>
<p><span class="math display">\[f(x+h)=f(x)+f^{&#39;}h+\frac{1}{2}f^{&#39;&#39;}h^2\]</span> For the first order, we can rewrite it as follows.</p>
<p><span class="math display">\[f(x+h) \approx f(x)+f^{&#39;}(x)h = a+bh\]</span> where,</p>
<p><span class="math display">\[ a = f(x), b=f^{&#39;}(x)\]</span> Similarly,</p>
<p><span class="math display">\[f(x+h)\approx f(x)+f^{&#39;}(x)h+\frac{1}{2}f^{&#39;&#39;}(x)h^2=a+bh+\frac{1}{2}ch^2\]</span> We can calculate the derivative with respect to <span class="math inline">\(h\)</span>, we can get:</p>
<p><span class="math display">\[f^{&#39;}(x+h) \approx b+ch\]</span> We can then set it to zero, and get:</p>
<p><span class="math display">\[0=b+c \hat{h}\]</span> Thus, we can get,</p>
<p><span class="math display">\[\hat{h} = -\frac{b}{c}=-\frac{f^{&#39;}(x)}{f^{&#39;&#39;}(x)}\]</span> Thus, we can get that the following can maximize <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[x+\hat{h}=x-\frac{f^{&#39;}(x)}{f^{&#39;&#39;}(x)}\]</span> Thus, the basic idea of Newton Raphson algorithm is as follows. - set a tolerance (typically a very small number) - Check if <span class="math inline">\(|f^{&#39;}(x)|&lt; the tolerance\)</span>. If not, <span class="math inline">\(i \leftarrow i+1; x_i\leftarrow x_{i-1}-\frac{f^{&#39;}(x_{i-1})}{f^{&#39;&#39;}(x_{i-1})}\)</span></p>
<p>Practice:</p>
<ol style="list-style-type: decimal">
<li>Solve the <span class="math inline">\(x^3-5=0\)</span></li>
</ol>
<p>Note that, this is obviously not a maximization problem. In contrast, it involves a function with zero. As we can see, we can think it as the first order of Taylor approximation. That is, <span class="math inline">\(f^{&#39;}(x)=x^3-5=0\)</span>. As we can see the following plot, it converts very quickly. [I will revisit this part later to double check whether such narrative is correct. But, the code is correct, for sure.]</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f_firstorder=<span class="cf">function</span>(x){x<span class="op">^</span><span class="dv">3</span><span class="op">-</span><span class="dv">5</span>}
f_secondorder=<span class="cf">function</span>(x){<span class="dv">3</span><span class="op">*</span>x}
x_old=<span class="dv">1</span>;tolerance=<span class="fl">1e-3</span>
max_its=<span class="dv">2000</span>;iteration=<span class="dv">1</span>;difference=<span class="dv">2</span>
c_iteration&lt;-<span class="kw">c</span>() ## to collect numbers generated in the iteration process 
<span class="cf">while</span>(difference<span class="op">&gt;</span>tolerance <span class="op">&amp;</span><span class="st"> </span>iteration<span class="op">&lt;</span>max_its){
  x_updated=x_old<span class="op">-</span>(<span class="kw">f_firstorder</span>(x_old)<span class="op">/</span><span class="kw">f_secondorder</span>(x_old))
  difference=<span class="kw">abs</span>(x_updated<span class="op">-</span>x_old);
  iteration=iteration<span class="op">+</span><span class="dv">1</span>;
  x_old=x_updated
  c_iteration&lt;-<span class="kw">c</span>(c_iteration,x_updated)}

<span class="kw">plot</span>(c_iteration,<span class="dt">type=</span><span class="st">&quot;b&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-18-1.png" /><!-- --></p>
<ol start="2" style="list-style-type: decimal">
<li>Use Newton Raphson to calculate the logistic regression</li>
</ol>
<p>Suppose we have <span class="math inline">\(n\)</span> observation, and <span class="math inline">\(m\)</span> variables.</p>
<p><span class="math display">\[\begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1m}\\
x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2m} \\
...\\
x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nm}
\end{bmatrix}\]</span></p>
<p>Typically, we add a vector of <span class="math inline">\(1\)</span> being used to estimate the constant.</p>
<p><span class="math display">\[\begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1m}\\
1 &amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2m} \\
...\\
1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nm}
\end{bmatrix}\]</span></p>
<p>And, we have observe a vector of <span class="math inline">\(n\)</span> <span class="math inline">\(y_i\)</span> as well, which is a binary variable:</p>
<p><span class="math display">\[Y = \begin{bmatrix}1 \\
0 \\
1 \\
0 \\
0 \\
0 \\
...\\
1 \\
\end{bmatrix}\]</span></p>
<p>Using the content from the MLE chapter, we can get:</p>
<p><span class="math display">\[\mathbf{L}=\prod_{i=1}^{n} p_i^{ y_i}(1-p_i)^{(1-y_i)}\]</span></p>
<p>Further, we can get a log-transformed format.</p>
<p><span class="math display">\[log (\mathbf{L})=\sum_{i=1}^{n}[y_i log (p_i) + (1-y_i) log(1-p_i)]\]</span> Given that <span class="math inline">\(p_i=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}=\frac{e^{\beta^Tx}}{1+e^{\beta^Tx}}\)</span>, we can rewrite it as follows:</p>
<p><span class="math display">\[log (\mathbf{L})=\ell=\sum_{i=1}^{n}[y_i log (\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}) + (1-y_i) log(1-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}})]\]</span> Before doing the derivative, we set. <span class="math display">\[\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}} = p(\beta ^T x_i)\]</span></p>
<p><span class="math display">\[log (\mathbf{L})=\ell=\sum_{i=1}^{n}[y_i log (p(\beta ^T x_i)) + (1-y_i) log(1-p(\beta ^T x_i))]\]</span></p>
<p>Note that, <span class="math inline">\(\frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)} = p(\beta ^T x_i)(1-p(\beta ^T x_i))\)</span>. We will use it later.</p>
<p><span class="math display">\[\begin{aligned}
\frac{\partial \ell} {\partial \beta} &amp;= \sum_{i=1}^{n} [y_i \frac{1}{p(\beta ^T x_i)} \frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)}\frac{\partial (\beta ^T x_i)}{\partial \beta}+(1-y_i) \frac{1}{1-p(\beta ^T x_i)}(-1)\frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)}\frac{\partial (\beta ^T x_i)}{\partial \beta}] \\
&amp;= \sum_{i=1}^{n} x_i^T[y_i \frac{1}{p(\beta ^T x_i)} p(\beta ^T x_i)(1-p(\beta ^T x_i))+(1-y_i) \frac{1}{1-p(\beta ^T x_i)}(-1)p(\beta ^T x_i)(1-p(\beta ^T x_i))] \\
&amp;= \sum_{i=1}^{n} x_i^T[y_i \frac{1}{p(\beta ^T x_i)} p(\beta ^T x_i)(1-p(\beta ^T x_i))-(1-y_i) \frac{1}{1-p(\beta ^T x_i)}p(\beta ^T x_i)(1-p(\beta ^T x_i))] \\
&amp;= \sum_{i=1}^{n} x_i^T[y_i (1-p(\beta ^T x_i))-(1-y_i) p(\beta ^T x_i)] \\
&amp;=\sum_{i=1}^{n} x_i^T[y_i-y_ip(\beta ^T x_i)-p(\beta ^T x_i)+y_i p(\beta ^T x_i)] \\
&amp;=\sum_{i=1}^{n} x_i^T[y_i-p(\beta ^T x_i)] \\
&amp;= \sum_{i=1}^{n} x_i^T[y_i-\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}]
\end{aligned}\]</span></p>
<p>As noted, the Newton Raphson algorithm needs the second order.</p>
<p><span class="math display">\[\begin{aligned}
Second order &amp;=\frac{\partial \sum_{i=1}^{n} x_i^T[y_i-p(\beta ^T x_i)]}{\partial \beta} \\
&amp;=-\sum_{i=1}^{n} x_i^T\frac{\partial p(\beta ^T x_i) }{\partial \beta}\\
&amp;=-\sum_{i=1}^{n} x_i^T\frac{\partial p(\beta ^T x_i) }{\partial (\beta^Tx_i)} \frac{\partial (\beta^Tx_i)}{\partial \beta}\\
&amp;=-\sum_{i=1}^{n} x_i^T p(\beta ^T x_i)(1-p(\beta ^T x_i))x_i
\end{aligned}\]</span></p>
<p>The following are the data simulation (3 IVs and 1 DV) and Newton Raphson analysis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Data generation</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
n=<span class="dv">500</span>
x1_norm&lt;-<span class="kw">rnorm</span>(n)
x2_norm&lt;-<span class="kw">rnorm</span>(n,<span class="dv">3</span>,<span class="dv">4</span>)
x3_norm&lt;-<span class="kw">rnorm</span>(n,<span class="dv">4</span>,<span class="dv">6</span>)
x_combined&lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,x1_norm,x2_norm,x3_norm) <span class="co"># dimension: n*4</span>
coefficients_new&lt;-<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)  <span class="co">#true regression coefficient</span>
inv_logit&lt;-<span class="cf">function</span>(x,b){<span class="kw">exp</span>(x<span class="op">%*%</span>b)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x<span class="op">%*%</span>b))}
prob_generated&lt;-<span class="kw">inv_logit</span>(x_combined,coefficients_new)
y&lt;-<span class="kw">c</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {y[i]&lt;-<span class="kw">rbinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,prob_generated[i])}

<span class="co"># Newton Raphson</span>

<span class="co">#We need to set random starting values.</span>
beta_old&lt;-<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)
tolerance=<span class="fl">1e-3</span>
max_its=<span class="dv">2000</span>;iteration=<span class="dv">1</span>;difference=<span class="dv">2</span>
W&lt;-<span class="kw">matrix</span>(<span class="dv">0</span>,n,n)

<span class="cf">while</span>(difference<span class="op">&gt;</span>tolerance <span class="op">&amp;</span><span class="st"> </span>iteration<span class="op">&lt;</span>max_its)
  {
  <span class="co"># The first order</span>
  f_firstorder&lt;-<span class="kw">t</span>(x_combined)<span class="op">%*%</span>(y<span class="op">-</span><span class="kw">inv_logit</span>(x_combined,beta_old))
  <span class="co"># The second order</span>
  <span class="kw">diag</span>(W) =<span class="st"> </span><span class="kw">inv_logit</span>(x_combined,beta_old)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">inv_logit</span>(x_combined,beta_old))
  f_secondorder&lt;-<span class="op">-</span><span class="kw">t</span>(x_combined)<span class="op">%*%</span>W<span class="op">%*%</span>x_combined
  <span class="co"># Calculate the beta_updated</span>
  beta_updated=beta_old<span class="op">-</span>(<span class="kw">solve</span>(f_secondorder)<span class="op">%*%</span>f_firstorder)
  difference=<span class="kw">max</span>(<span class="kw">abs</span>(beta_updated<span class="op">-</span>beta_old));
  iteration=iteration<span class="op">+</span><span class="dv">1</span>;
  beta_old=beta_updated}

beta_old</code></pre></div>
<pre><code>##              [,1]
##         0.9590207
## x1_norm 1.7974165
## x2_norm 3.0072303
## x3_norm 3.9578107</code></pre>
<p><span class="math display">\[\frac{\partial \ell} {\partial \beta} = \sum_{i=1}^{n} [y_i \frac{1}{p(\beta ^T x_i)} \frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)}\frac{\partial (\beta ^T x_i)}{\partial \beta}+(1-y_i) \frac{1}{1-p(\beta ^T x_i)}(-1)\frac{\partial p(\beta ^T x_i)}{\partial (\beta ^T x_i)}\frac{\partial (\beta ^T x_i)}{\partial \beta}] \]</span> <span class="math display">\[=\sum_{i=1}^{n} [y_i \frac{1}{p(\beta ^T x_i)} \phi (\beta ^T x_i)-(1-y_i) \frac{1}{1-p(\beta ^T x_i)}\phi (\beta ^T x_i)]x_i\]</span></p>
<p><span class="math display">\[\Phi(\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3)= p(y=1)\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Data generation</span>
n=<span class="dv">500</span>
x1_norm&lt;-<span class="kw">rnorm</span>(n)
x2_norm&lt;-<span class="kw">rnorm</span>(n)
x3_norm&lt;-<span class="kw">rnorm</span>(n)
x_combined&lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,x1_norm,x2_norm,x3_norm)
coefficients_new&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>)  <span class="co">#true regression coefficient</span>
inv_norm&lt;-<span class="cf">function</span>(x,b){<span class="kw">pnorm</span>(x<span class="op">%*%</span>b)}
prob_generated&lt;-<span class="kw">inv_norm</span>(x_combined,coefficients_new)
y&lt;-<span class="kw">c</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {y[i]&lt;-<span class="kw">rbinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,prob_generated[i])}

<span class="co"># Newton Raphson</span>

<span class="co">#We need to set random starting values.</span>
x_old&lt;-<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)
tolerance=<span class="fl">1e-3</span>
max_its=<span class="dv">2000</span>;iteration=<span class="dv">1</span>;difference=<span class="dv">2</span>

<span class="cf">while</span>(difference<span class="op">&gt;</span>tolerance <span class="op">&amp;</span><span class="st"> </span>iteration<span class="op">&lt;</span>max_its){
  x_updated=x_old<span class="op">-</span>(<span class="kw">f_firstorder</span>(x_old)<span class="op">/</span><span class="kw">f_secondorder</span>(x_old))
  difference=<span class="kw">abs</span>(x_updated<span class="op">-</span>x_old);
  iteration=iteration<span class="op">+</span><span class="dv">1</span>;
  x_old=x_updated
  c_iteration&lt;-<span class="kw">c</span>(c_iteration,x_updated)}

<span class="kw">plot</span>(c_iteration,<span class="dt">type=</span><span class="st">&quot;b&quot;</span>)</code></pre></div>
<p>Some links related to this topic (canonical link function): <a href="http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/" class="uri">http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/</a> <a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function" class="uri">https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function</a> <a href="https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function" class="uri">https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function</a> <a href="https://stats.stackexchange.com/questions/344309/why-using-newtons-method-for-logistic-regression-optimization-is-called-iterati" class="uri">https://stats.stackexchange.com/questions/344309/why-using-newtons-method-for-logistic-regression-optimization-is-called-iterati</a> <a href="https://www.stat.washington.edu/adobra/classes/536/Files/week1/newtonfull.pdf" class="uri">https://www.stat.washington.edu/adobra/classes/536/Files/week1/newtonfull.pdf</a> <a href="https://tomroth.com.au/logistic/" class="uri">https://tomroth.com.au/logistic/</a> <a href="https://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf" class="uri">https://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf</a></p>
<p><a href="http://seankross.com/2019/10/27/R-as-a-First-Programming-Language.html" class="uri">http://seankross.com/2019/10/27/R-as-a-First-Programming-Language.html</a></p>
</div>
</div>
<div id="basics-of-glmm" class="section level2">
<h2><span class="header-section-number">5.2</span> Basics of GLMM</h2>
<p>Recall the formula in the probit model:</p>
<p><span class="math display">\[Y^*=X\beta+\epsilon, \epsilon \sim N(0,\sigma^2)=N(0,I)\]</span> Similar to LMM, binary model with random effect can be written as follows.</p>
<p><span class="math display">\[Y^*=X\beta+ Z u+\epsilon\]</span> where,</p>
<p><span class="math display">\[\epsilon \sim N(0,I)\]</span> <span class="math display">\[u \sim N(0, D)\]</span></p>
<p>We also assume <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(u\)</span> are independent.Thus, we know that <span class="math inline">\(D\)</span> represents the virances of the random effects. If we make <span class="math inline">\(u =1\)</span>, the model becomes the usual probit model. McCulloch (1994) states that there are a few advantages to use probit, rather than logit models.</p>
<p>The following is the note from Charle E. McCulloch's &quot;Maximum likelihood algorithems for Generalized Linear Mixed Models&quot;</p>
</div>
<div id="some-references" class="section level2">
<h2><span class="header-section-number">5.3</span> Some References</h2>
<p><a href="http://www.biostat.umn.edu/~baolin/teaching/linmods/glmm.html" class="uri">http://www.biostat.umn.edu/~baolin/teaching/linmods/glmm.html</a></p>
<p><a href="http://www.biostat.umn.edu/~baolin/teaching/probmods/GLMM_mcmc.html" class="uri">http://www.biostat.umn.edu/~baolin/teaching/probmods/GLMM_mcmc.html</a></p>
<p><a href="https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html" class="uri">https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html</a></p>
<!--chapter:end:10-glmm.rmd-->
</div>
</div>
<div id="twitter-example" class="section level1">
<h1><span class="header-section-number">6</span> Twitter Example</h1>
<p>The following is part of my course project for Stat 536. It aims to replicate part of the findings from Barbera (2015) Birds of the Same Feather Tweet Together: Bayesian Ideal Point Estimation Using Twitter Data. Political Analysis 23 (1). Note that, the following model is much simpler than that in the original paper.</p>
<div id="model" class="section level2">
<h2><span class="header-section-number">6.1</span> Model</h2>
<p>Suppose that a Twitter user is presented with a choice between following or not following another target <span class="math inline">\(j \in \{ 1, ..., m\}\)</span>. Let <span class="math inline">\(y_{j}=1\)</span> if the user decides to follow <span class="math inline">\(j\)</span>, and <span class="math inline">\(y_{j}=0\)</span> otherwise.</p>
<p><span class="math display">\[y_{j}=\begin{cases} 1 &amp; Following \\ 0 &amp; Not Following \end{cases}\]</span></p>
<p><span class="math display">\[p(y_{j}=1|\theta) = \frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)}\]</span> We additionally know the priors of <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[\theta_i \sim N(0,10^2) (i = 0, 1)\]</span></p>
<p>The likelihood function is as follows.</p>
<p><span class="math display">\[L(Y|\theta)=\prod_{j=1}^{m} (\frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)})^{y_j}(1-\frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)})^{(1-y_j)}\]</span> Thus, the posterior is as follows.</p>
<p><span class="math display">\[L(Y|\theta) \cdot N(\theta_0|0,10) \cdot N(\theta_1|0,10)\]</span> <span class="math display">\[\propto \prod_{j=1}^{m} (\frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)})^{y_j}(1-\frac{exp(- \theta_0|\theta_1 - x_j|^2)}{1+exp(- \theta_0|\theta_1 - x_j|^2)})^{(1-y_j)}\cdot exp(-\frac{1}{2}(\frac{\theta_0}{10})^2)\cdot exp(-\frac{1}{2}(\frac{\theta_1}{10})^2)\]</span></p>
</div>
<div id="simulating-data-of-senators-on-twitter" class="section level2">
<h2><span class="header-section-number">6.2</span> Simulating Data of Senators on Twitter</h2>
<p>Assume that we have 100 senators, 50 Democrats and 50 Republicans, who we know their ideology. Assume that Democrats have negative ideology scores to indicate that they are more liberal, whereas Republicans have positive scores to indicate that they are more conservative. The following is data simulation for senators.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Republicans are more conservative, and they have positive numbers.</span>
Republicans&lt;-<span class="kw">c</span>()
Republicans&lt;-<span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dv">1</span>,<span class="fl">0.5</span>)
No_Republicans&lt;-<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>,<span class="dv">1</span>)
Part_<span class="dv">1</span>&lt;-<span class="kw">cbind</span>(No_Republicans,Republicans)

<span class="co"># Democrats are more liberal, and they have negative numbers.</span>
Democrats&lt;-<span class="kw">c</span>()
Democrats&lt;-<span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="op">-</span><span class="dv">1</span>,<span class="fl">0.5</span>)
No_Democrats&lt;-<span class="kw">rep</span>(<span class="dv">51</span><span class="op">:</span><span class="dv">100</span>,<span class="dv">1</span>)
Part_<span class="dv">2</span>&lt;-<span class="kw">cbind</span>(No_Democrats,Democrats)
Data_Elites&lt;-<span class="kw">rbind</span>(Part_<span class="dv">1</span>,Part_<span class="dv">2</span>)
Data_Elites&lt;-<span class="kw">as.data.frame</span>(Data_Elites)
<span class="kw">colnames</span>(Data_Elites) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Elite_No&quot;</span>,<span class="st">&quot;Elite_ideology&quot;</span>)

<span class="kw">head</span>(Data_Elites)</code></pre></div>
<pre><code>##   Elite_No Elite_ideology
## 1        1      1.2848895
## 2        2      0.1733722
## 3        3      0.6666728
## 4        4      0.7758829
## 5        5      1.5219457
## 6        6      1.5140870</code></pre>
</div>
<div id="simulating-data-of-conservative-users-on-twitter-and-model-testing" class="section level2">
<h2><span class="header-section-number">6.3</span> Simulating Data of Conservative Users on Twitter and Model Testing</h2>
<p>Assume that we observe one Twitter user, who is more conservative. To simulate Twitter following data for this user, I assign this user to follow more Republican senators. Thus, if the Metropolis Hastings algorithm works as intended, we would expect to see a positive estimated value for their ideology. Importantly, as we can see in the histogram below, the estimated value indeed is positive, providing preliminary evidence for the statistical model and the algorithm. In addition, for the acceptance rate, we can see that the constant has a lower number than ideology, since we only accept a constant when it is positive.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#This user approximately follows 45 Republican Senators and 10 Democrat Senators. </span>
Data_user&lt;-<span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">50</span>)<span class="op">&lt;</span>.<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">50</span>)<span class="op">&lt;</span>.<span class="dv">8</span>,<span class="dv">0</span>,<span class="dv">1</span>))), <span class="dv">100</span>, <span class="dv">1</span>)
<span class="kw">colnames</span>(Data_user)&lt;-<span class="kw">c</span>(<span class="st">&quot;R_User&quot;</span>)
Data_combined&lt;-<span class="kw">cbind</span>(Data_Elites,Data_user)

X_data&lt;-Data_combined<span class="op">$</span>Elite_ideology
Y_data&lt;-Data_combined<span class="op">$</span>R_User

fit_C&lt;-<span class="kw">Bayes_logit</span>(Y_data,X_data)
fit_C<span class="op">$</span>acceptance_rate</code></pre></div>
<pre><code>## [1] 0.1590795 0.4817409</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit_C<span class="op">$</span>theta[,<span class="dv">1</span>],<span class="dt">main=</span><span class="st">&quot;Constant (Conservative Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Iteration Process&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Estimated Scores&quot;</span>,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-22-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit_C<span class="op">$</span>theta[,<span class="dv">2</span>],<span class="dt">main=</span><span class="st">&quot;Estimated Ideology Scores (Conservative Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Iteration Process&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Ideology Scores&quot;</span>,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-22-2.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(fit_C<span class="op">$</span>theta[,<span class="dv">2</span>],<span class="dt">main=</span><span class="st">&quot;Estimated Ideology Scores (Conservative Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Ideology Scores&quot;</span>,<span class="dt">breaks =</span> <span class="dv">100</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-22-3.png" /><!-- --></p>
</div>
<div id="simulating-data-of-liberal-users-on-twitter-and-model-testing" class="section level2">
<h2><span class="header-section-number">6.4</span> Simulating Data of Liberal Users on Twitter and Model Testing</h2>
<p>To further verify the Metropolis Hastings algorithm, I plan to test the opposite estimate. Specifically, assume that we observe another user, who is more liberal. To simulate Twitter following data for this user, I assign this user to follow more Democrat senators. In this case, we would expect to see a negative value for their estimated ideology. As we can see in the histogram shown below, as expected, the estimated value is negative, providing convergent evidence for the model and the algorithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#This user approximately follows 10 Republican Senators and 45 Democrat Senators. </span>
Data_user&lt;-<span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">50</span>)<span class="op">&lt;</span>.<span class="dv">8</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">50</span>)<span class="op">&lt;</span>.<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>))), <span class="dv">100</span>, <span class="dv">1</span>)
<span class="kw">colnames</span>(Data_user)&lt;-<span class="kw">c</span>(<span class="st">&quot;L_User&quot;</span>)
Data_combined&lt;-<span class="kw">cbind</span>(Data_Elites,Data_user)

X_data&lt;-Data_combined<span class="op">$</span>Elite_ideology
Y_data&lt;-Data_combined<span class="op">$</span>L_User


fit_L&lt;-<span class="kw">Bayes_logit</span>(Y_data,X_data)
fit_L<span class="op">$</span>acceptance_rate</code></pre></div>
<pre><code>## [1] 0.2081041 0.4337169</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit_L<span class="op">$</span>theta[,<span class="dv">1</span>],<span class="dt">main=</span><span class="st">&quot;Constant (Liberal Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Iteration Process&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Estimated Scores&quot;</span>,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-23-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit_L<span class="op">$</span>theta[,<span class="dv">2</span>],<span class="dt">main=</span><span class="st">&quot;Estimated Ideology Scores (Liberal Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Iteration Process&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Ideology Scores&quot;</span>,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-23-2.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(fit_L<span class="op">$</span>theta[,<span class="dv">2</span>],<span class="dt">main=</span><span class="st">&quot;Estimated Ideology Scores (Liberal Users)&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Ideology Scores&quot;</span>,<span class="dt">breaks =</span> <span class="dv">100</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-epub3/unnamed-chunk-23-3.png" /><!-- --></p>
<!--chapter:end:20-mh-algorithm-and-data.Rmd-->
</div>
</div>
