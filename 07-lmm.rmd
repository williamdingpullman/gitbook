# Linear Mixed Models

## LM and GLM

Before moving to LMM, I would like to review LM and GLM first.

### LM

$$Y|X \sim N(\mu(X),\sigma^2 I)$$

$$E(Y|X)=\mu(X)=X^T \beta$$
where, 

$$\mu(X): random component$$
$$X^T \beta: covariates$$
### GLM

Ref: https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_GLM.pdf

$$Y \sim exponential family$$
 Link function
 
 $$g(\mu(X))=X^T \beta$$
Possion regression:
$$\mu_i = \gamma e^{\delta t_i}$$
Link function is log link, and it becomes:
$$log(\mu_i) = log(\gamma) + log(\delta t_i)=\beta_0+\beta_1 t_i$$
$$\mu_i=\frac{\alpha x_i}{h+x_i}$$
Reciprocal link:
$$g(\mu_i)=\frac{1}{\mu_i}=\frac{1}{\alpha}+\frac{h}{\alpha}\frac{1}{x_i}=\beta_0+\beta_1 \frac{1}{x_i}$$
In a more general sense, for exponential family:

$$\begin{aligned} P_{\theta}(X)=P(X, \theta)&= e^{\sum \eta_i(\theta)T_i(X)} C(\theta)h(x)\\ &=e^{\sum \eta_i(\theta)T_i(X)} e^{-log(\frac{1}{c(\theta)})}h(x) \\ &= e^{\sum \eta_i(\theta)T_i(X)-log(\frac{1}{c(\theta)})} h(x)  \\&= e^{\sum \eta_i(\theta)T_i(X)-B(\theta)} h(x) \end{aligned}$$

For normal distributions, it belongs to exponential family.

$$\begin{aligned} P_{\theta}(X) &= \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}\\ &=e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}} e^{log(\frac{1}{\sigma\sqrt{2\pi}})} \\ &= e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}-log (\sigma\sqrt{2\pi})} \\ &= e^{-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2} \mu^2+\frac{x\mu}{\sigma^2}-log(\sqrt{2\pi}\sigma)}\\ &=e^{-\frac{1}{2\sigma^2}x^2+\frac{x\mu}{\sigma^2}-(\frac{1}{2\sigma^2} \mu^2+log(\sqrt{2\pi}\sigma))} \end{aligned}$$
Where,

$\eta_1 =-\frac{1}{2\sigma^2}$ and $T_1(x)=x^2$

$\eta_2 =-\frac{\mu}{\sigma^2}$ and $T_2(x)=x$

$B(\theta)=\frac{1}{2\sigma^2} \mu^2+log(\sqrt{2\pi}\sigma)$

$h(x)=1$

In the case above, $\theta=(\mu, \sigma^2)$. If $\sigma^2$ is known, $\theta=\mu$. In this case, we can rewrite the normal pdf as follows.

$$\begin{aligned} P_{\theta}(X) &=e^{-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2} \mu^2+\frac{x\mu}{\sigma^2}-log(\sqrt{2\pi}\sigma)}\\ &=e^{\frac{x\mu}{\sigma^2}-\frac{1}{2\sigma^2} \mu^2}e^{-\frac{1}{2\sigma^2}x^2-log(\sqrt{2\pi}\sigma)} \end{aligned}$$
Where,

$\eta_1 =-\frac{\mu}{\sigma^2}$ and $T_1(x)=x$

$B(\theta)=\frac{1}{2\sigma^2} \mu^2$

$\begin{aligned} h(x) &=e^{-\frac{1}{2\sigma^2}x^2-log(\sqrt{2\pi}\sigma)} \\&=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\frac{x^2}{\sigma^2}} \end{aligned}$

Thus, we can see that $h(x)$ is a normal pdf $\sim N(0, \sigma^2)$.


Another example, $x$ is descrete. For example, Bernoulli:

$$ \begin{aligned} &= p^x(1-p)^{1-x}  \\ &=e^{log(p^x(1-p)^{1-x})} \\ &= e^{xlog(p)+(1-x)log(1-p)}\\ &= e^{xlog(p)-xlog(1-p)+log(1-p)}\\ &=e^{xlog(\frac{p}{1-p})+log(1-p)} \end{aligned}$$

Where,

$\eta_1 =log(\frac{p}{1-p})$ and $T_1(x)=x$

$B(\theta)=log(\frac{1}{1-p})$

$h(x) =1$

## LMM
The following is a shortened version of Jonathan Rosenblatt's LMM tutorial. http://www.john-ros.com/Rcourse/lme.html.

In addition, another reference is from Douglas Bates's R package document.
https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf?fbclid=IwAR1nmmRP9A0BrhKdgBibNjM5acR_spTpXV8QlQGdmTWyQz3ZtV3LYn6kCbQ


Assume that $y$ is a function of $x$ and $u$, where $x$ is the fixed effect and $u$ is the random effect. Thus, we can get,

$$y|x, u = x'\beta+z'u+\epsilon$$

For random effect, one example can be that you want to test the treatment effect, and sample 8 observations from 4 groups. You measure before and after the treatment. In this case, $x$ represents the treatment effect, whereas $z$ represents the group effect (i.e., random effect). Note that, in this case, it reminds the paired t-test. Remember in SPSS, why do we do paired t-test? Typically, it is the case when we measure a subject (or, participant) twice. In this case, we can consider each participant as an unit of random effect (rather than as group in the last example.)


## Calculate mean

The following code generates 4 numbers ($N(0,10)$) for 4 groups. Then, replicate it within each group.That is, in the end, there are 8 observations. 

Note that, in the following code, there are no "independent variables". Both the linear model and mixed model are actually just trying to calculate the mean. Note that lmer(y~1+1|groups) and lmer(y~1|groups) will generate the same results.

```{R}
set.seed(123)
n.groups <- 4 # number of groups
n.repeats <- 2 # samples per group
#Generating index for observations belong to the same group
groups <- as.factor(rep(1:n.groups, each=n.repeats))
n <- length(groups)
#Generating 4 random numbers, assuming normal distribution
z0 <- rnorm(n.groups, 0, 10) 
z <- z0[as.numeric(groups)] # generate and inspect random group effects
z
```


```{R}
epsilon <- rnorm(n,0,1) # generate measurement error
beta0 <- 2 # this is the actual parameter of interest! The global mean.
y <- beta0 + z + epsilon # sample from an LMM

# fit a linear model assuming independence
# i.e., assume that there is no "group things".
lm.5 <- lm(y~1)

# fit a mixed-model that deals with the group dependence
#install.packages("lme4")
library(lme4)
lme.5.a <- lmer(y~1+1|groups) 
lme.5.b <- lmer(y~1|groups) 
lm.5
lme.5.a 
lme.5.b 

```

## Test the treatment effect 

As we can see that, LLM and paired t-test generate the same t-value. 

```{R}
times<-rep(c(1,2),4) # first time and second time
times
data_combined<-cbind(y,groups,times)
data_combined

lme_diff_times<- lmer(y~times+(1|groups)) 


t_results<-t.test(y~times, paired=TRUE)

lme_diff_times

print("The following results are from paired t-test")
t_results$statistic
```


## Another example

```{R}
data(Dyestuff, package='lme4')
attach(Dyestuff)
Dyestuff

lme_batch<- lmer( Yield ~ 1 + (1|Batch)  , Dyestuff )
summary(lme_batch)
```


## Full LMM model

In the following, I used the data from the package of lme4. For Days + (1 | Subject), it only has random intercept; in contrast, Days + ( Days| Subject ) has both random intercept and random slope for Days. Note that, random effects do not generate specific slopes for each level of Days, but rather just a variance of all the slopes. 

Therefore, we can see that "Days + ( Days| Subject )" and "Days + ( 1+Days| Subject )" generate the same results. For more discussion, you can refer to the following link: https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r

```{R}
data(sleepstudy, package='lme4')
attach(sleepstudy)

fm1 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)
summary(fm1)

fm2<-lmer ( Reaction ~ Days + ( Days| Subject ) , data= sleepstudy )
summary(fm2)

fm3<-lmer ( Reaction ~ Days + (1+Days| Subject ) , data= sleepstudy )
summary(fm3)
```

## Serial correlations in time and space

The hierarchical model of $y|x, u = x'\beta+z'u+\epsilon$ can work well for correlations within blocks, but not for correlations in time as the correlations decay in time. The following uses nlme package to calculate time serial data.

```{R}
library(nlme)
head(nlme::Ovary,n=50)

fm1Ovar.lme <- nlme::lme(fixed=follicles ~ sin(2*pi*Time) + cos(2*pi*Time), 
                   data = Ovary, 
                   random = pdDiag(~sin(2*pi*Time)), 
                   correlation=corAR1() )
summary(fm1Ovar.lme)
```

